{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 14674312,
          "sourceType": "datasetVersion",
          "datasetId": 9374868
        }
      ],
      "dockerImageVersionId": 31260,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "gpuType": "T4",
      "name": "notebookc4efb879ae",
      "provenance": []
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# ü©∫ Building Accessible AI: Nail Disease Detection with MedGemma\n\n**The Challenge**: \"How can we make expert-level dermatology accessible to everyone, everywhere?\"\n\nIn many parts of the world, access to a dermatologist is a luxury. Early detection of conditions like melanoma or signs of systemic disease in nails can save lives. This project leverages **Google's MedSigLIP (Medical SigLIP Vision-Language Model)** to build a highly accurate, efficient, and mobile-ready nail disease classifier. \n\nOur goal isn't just to build a model; it's to build a tool that can be deployed on a smartphone to help community health workers and individuals make informed decisions.\n\n---\n\n### üåü Why This Matters\n\nThis notebook represents a submission to the **MedGemma Impact Challenge**, focusing on human-centered AI. We are:\n- **Democratizing Access**: Using open-source medical models to bring specialist knowledge to the edge.\n- **Prioritizing Privacy**: Designing for efficient edge deployment so data can stay on the device.\n- **Optimizing for Real Performance**: Not just chasing accuracy, but ensuring the model is robust and fast.\n\n---\n\n### üöÄ Key Capabilities\n\n- **Seamless Integration**: Directly connects to our curated Kaggle dataset.\n- **Smart Processing**: Auto-magically handles train/test splits and image augmentation.\n- **Advanced Fine-Tuning**: We don't just retrain the top layer; we carefully unfreeze deeper layers to let the model \"learn\" the texture of nails.\n- **Safety Nets**: Built-in overfitting detection to ensure our model generalizes well to new patients.\n\n---\n\n### ü¶† The Conditions We Detect\n\nWe are training our digital assistant to recognize 7 specific categories:\n1. **Acral Lentiginous Melanoma (ALM)**: A dangerous form of skin cancer that can mimic a bruise.\n2. **Blue Finger**: Often a sign of poor oxygenation or circulation issues.\n3. **Clubbing**: A classic sign of chronic heart or lung conditions.\n4. **Onychogryphosis**: \"Ram's horn nails,\" common in elderly populations, needing specific care.\n5. **Pitting**: Often the first sign of Psoriasis or other autoimmune issues.\n6. **Psoriasis**: A chronic condition that frequently affects nails first.\n7. **Healthy Nail**: The baseline for normal.\n\nLet's build something that matters. üëá",
      "metadata": {
        "_uuid": "496eacb8-7277-4524-8de0-6e6d92618f82",
        "_cell_guid": "2ab2f2f1-892e-4d39-89c2-91a032590fb1",
        "trusted": true,
        "collapsed": false,
        "id": "nail_disease_title_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 1Ô∏è‚É£ Setting Up Access to Medical Intelligence\n\n**Ethics First**: DeepMind's MedSigLIP is a powerful tool trained on diverse medical data. To ensure responsible use, we need to authenticate with Hugging Face.\n\n1. **Get your Key**: If you haven't, grab a token from [Hugging Face Settings](https://huggingface.co/settings/tokens).\n2. **Request Access**: Ensure you've approved the terms at [google/medsiglip-448](https://huggingface.co/google/medsiglip-448).\n3. **Authenticate below**: Paste your token when prompted to unlock the model.",
      "metadata": {
        "_uuid": "8fab7aba-25fa-4ed7-8883-cc2f993ddfba",
        "_cell_guid": "83f6d7f5-b164-4b57-bdfc-6a58d619f88c",
        "trusted": true,
        "collapsed": false,
        "id": "hf_login_section",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "from huggingface_hub import notebook_login\n\nprint(\"=\"*70)\nprint(\"üîê HUGGING FACE LOGIN\")\nprint(\"=\"*70)\nprint(\"\\nYou'll be prompted to enter your Hugging Face token.\")\nprint(\"Get your token: https://huggingface.co/settings/tokens\\n\")\n\nnotebook_login()\n\nprint(\"\\n‚úÖ Login successful!\")",
      "metadata": {
        "_uuid": "5f483859-7a12-45ba-93eb-a1e8fd31379c",
        "_cell_guid": "08043a60-8459-4da7-9772-704727b5bdfb",
        "trusted": true,
        "collapsed": false,
        "id": "hf_login",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 2Ô∏è‚É£ Install Dependencies",
      "metadata": {
        "_uuid": "d00e8c1e-dcbf-4935-aee3-0991d5bb25bb",
        "_cell_guid": "b5838788-1771-41a8-af83-e9bd77b117ae",
        "trusted": true,
        "collapsed": false,
        "id": "setup_section_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "!pip install -q torch torchvision transformers datasets pillow scikit-learn matplotlib tqdm numpy pandas\n!pip install -q open-clip-torch\n!pip install -q onnx onnxruntime\n!pip install -q huggingface_hub\n!pip install -q timm\n\nprint(\"‚úÖ Medical AI Toolkit ready! All systems go.\")",
      "metadata": {
        "_uuid": "e73e082e-dab8-4419-b281-0831b67a8dd9",
        "_cell_guid": "78eb02b4-87c7-4fdb-9644-12014cbbc9ba",
        "trusted": true,
        "collapsed": false,
        "id": "install_dependencies_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3Ô∏è‚É£ Verifying Our Computational Engine",
      "metadata": {
        "_uuid": "6288c814-23f8-4c65-8d94-955fd6a954db",
        "_cell_guid": "dcd9202e-d9f9-4e91-9d7f-527557c1e8e7",
        "trusted": true,
        "collapsed": false,
        "id": "gpu_check_section_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "import torch\nimport sys\nfrom pathlib import Path\n\nprint(\"=\"*70)\nprint(\"üñ•Ô∏è ENVIRONMENT INFO\")\nprint(\"=\"*70)\nprint(f\"Python Version: {sys.version.split()[0]}\")\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n    print(f\"CUDA Version: {torch.version.cuda}\")\nelse:\n    print(\"‚ö†Ô∏è WARNING: No GPU detected. Training will be very slow.\")\nprint(\"=\"*70)",
      "metadata": {
        "_uuid": "faecd726-4777-4430-bf8f-24cb94eb8a23",
        "_cell_guid": "0813eee6-63bd-43da-b8be-77895c1456ae",
        "trusted": true,
        "collapsed": false,
        "id": "check_gpu_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 4Ô∏è‚É£ Connecting to the Patient Database",
      "metadata": {
        "_uuid": "94823f1e-9ca6-4896-8892-f8f620edd9f2",
        "_cell_guid": "c89a2610-382d-41c9-88b6-093767baa12d",
        "trusted": true,
        "collapsed": false,
        "id": "kaggle_setup_section",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "import os\nfrom pathlib import Path\n\nKAGGLE_DATASET_PATH = '/kaggle/input/nail-disease-dataset-medsiglip'\nOUTPUT_PATH = '/kaggle/working/output'\n\nos.makedirs(OUTPUT_PATH, exist_ok=True)\n\nprint(\"=\"*70)\nprint(\"üìÇ CONNECTING TO MEDICAL DATABASE\")\nprint(\"=\"*70)\n\nif not os.path.exists(KAGGLE_DATASET_PATH):\n    print(f\"\\n‚ùå ERROR: Database connection failed at {KAGGLE_DATASET_PATH}\")\n    print(\"\\nüìã SOLUTION:\")\n    print(\"   1. Add 'nail-disease-dataset' as an input to this notebook\")\n    print(\"   2. Go to notebook settings ‚Üí Add data\")\n    print(\"   3. Search for 'nail-disease-dataset' and add it\")\n    print(\"   4. Re-run this cell\")\n    raise FileNotFoundError(f\"Dataset not found at {KAGGLE_DATASET_PATH}\")\n\nprint(f\"‚úÖ Dataset path found: {KAGGLE_DATASET_PATH}\")\n\nprint(f\"\\nüìç Available Kaggle Inputs:\")\nfor item in os.listdir('/kaggle/input'):\n    print(f\"   ‚Ä¢ {item}\")\n\nprint(f\"\\nüîç Looking for train/test directories...\")\ndataset_contents = os.listdir(KAGGLE_DATASET_PATH)\nprint(f\"\\nüìÇ Dataset contents:\")\nfor item in dataset_contents:\n    item_path = os.path.join(KAGGLE_DATASET_PATH, item)\n    if os.path.isdir(item_path):\n        file_count = len([f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))])\n        dir_count = len([d for d in os.listdir(item_path) if os.path.isdir(os.path.join(item_path, d))])\n        print(f\"   üìÅ {item}/ ({dir_count} subdirs, {file_count} files)\")\n\nTRAIN_DATA_PATH = os.path.join(KAGGLE_DATASET_PATH, 'train')\nTEST_DATA_PATH = os.path.join(KAGGLE_DATASET_PATH, 'test')\n\nif not os.path.exists(TRAIN_DATA_PATH) or not os.path.exists(TEST_DATA_PATH):\n    print(f\"\\n‚ùå ERROR: train/ or test/ directories not found!\")\n    print(f\"   Expected structure:\")\n    print(f\"   /kaggle/input/nail-disease-dataset/\")\n    print(f\"   ‚îú‚îÄ‚îÄ train/ (with class folders)\")\n    print(f\"   ‚îî‚îÄ‚îÄ test/ (with class folders)\")\n    raise FileNotFoundError(\"train/ or test/ directories not found\")\n\nprint(f\"\\n‚úÖ Dataset paths configured:\")\nprint(f\"   TRAIN: {TRAIN_DATA_PATH}\")\nprint(f\"   TEST: {TEST_DATA_PATH}\")\nprint(f\"   OUTPUT: {OUTPUT_PATH}\")\nprint(\"=\"*70)",
      "metadata": {
        "_uuid": "8036c9b1-9887-4f3e-b1af-bad4df8a8fd6",
        "_cell_guid": "809183b0-6bbc-45f2-a346-82354dbbd46a",
        "trusted": true,
        "collapsed": false,
        "id": "kaggle_dataset_setup",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 5Ô∏è‚É£ Examining the Medical Imagery",
      "metadata": {
        "_uuid": "30b8f7ae-f935-46b4-9eed-3201ecd70f2c",
        "_cell_guid": "68c3f16b-9a98-45e8-a1aa-e55d0491ea16",
        "trusted": true,
        "collapsed": false,
        "id": "data_loading_section_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "from torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nIMAGE_SIZE = 448\nBATCH_SIZE = 16\nNUM_WORKERS = 2\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.3),\n    transforms.RandomRotation(30),\n    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.15),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15), ratio=(0.3, 3.3), value='random')\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nprint(\"üìÇ Loading datasets...\")\ntry:\n    train_dataset = ImageFolder(TRAIN_DATA_PATH, transform=train_transforms)\n    test_dataset = ImageFolder(TEST_DATA_PATH, transform=val_transforms)\n\n    print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n    print(f\"‚úÖ Test samples: {len(test_dataset)}\")\n    print(f\"‚úÖ Number of classes: {len(train_dataset.classes)}\")\n    print(f\"\\nüìã Class labels: {train_dataset.classes}\")\n\n    print(\"\\nüìä Class distribution (Training):\")\n    for cls_idx, cls_name in enumerate(train_dataset.classes):\n        count = sum(1 for x, y in train_dataset if y == cls_idx)\n        print(f\"   {cls_name}: {count} images\")\n\nexcept Exception as e:\n    print(f\"‚ùå Error loading data: {e}\")\n    print(f\"\\nüìç Please verify dataset structure:\")\n    print(f\"   ‚îú‚îÄ‚îÄ train/class1/, class2/, ...\")\n    print(f\"   ‚îî‚îÄ‚îÄ test/class1/, class2/, ...\")\n    raise",
      "metadata": {
        "_uuid": "c081cdeb-3237-41de-b3b9-1b4fbdddbb80",
        "_cell_guid": "ed83f6bc-0d67-423e-82f2-ab9eb40b4a1a",
        "trusted": true,
        "collapsed": false,
        "id": "data_loader_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 6Ô∏è‚É£ Create Data Loaders",
      "metadata": {
        "_uuid": "7f356438-e803-4975-af48-efe743da874a",
        "_cell_guid": "573022ab-bd88-4a20-887f-9c0a6628fa87",
        "trusted": true,
        "collapsed": false,
        "id": "dataloader_section_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "from torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nIMAGE_SIZE = 448\nBATCH_SIZE = 8  # REDUCED from 16 for multi-GPU\nNUM_WORKERS = 0  # Set to 0 for stability\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.3),\n    transforms.RandomRotation(30),\n    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.15),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15), ratio=(0.3, 3.3), value='random')\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nprint(\"üìÇ Loading datasets...\")\ntry:\n    train_dataset = ImageFolder(TRAIN_DATA_PATH, transform=train_transforms)\n    test_dataset = ImageFolder(TEST_DATA_PATH, transform=val_transforms)\n\n    print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n    print(f\"‚úÖ Test samples: {len(test_dataset)}\")\n    print(f\"‚úÖ Number of classes: {len(train_dataset.classes)}\")\n    print(f\"\\nüìã Class labels: {train_dataset.classes}\")\n\n    print(\"\\nüìä Class distribution (Training):\")\n    for cls_idx, cls_name in enumerate(train_dataset.classes):\n        count = sum(1 for x, y in train_dataset if y == cls_idx)\n        print(f\"   {cls_name}: {count} images\")\n\nexcept Exception as e:\n    print(f\"‚ùå Error loading data: {e}\")\n    raise\n\n# DataLoader with reduced batch size\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\nprint(f\"\\n‚úÖ Train DataLoader: {len(train_loader)} batches\")\nprint(f\"‚úÖ Test DataLoader: {len(test_loader)} batches\")\n\nprint(\"\\nüîç Testing batch loading...\")\nimages, labels = next(iter(train_loader))\nprint(f\"   Batch shape: {images.shape}\")\nprint(f\"   Labels: {labels[:5].tolist()}\")\nprint(\"‚úÖ Data loading successful!\")",
      "metadata": {
        "_uuid": "9e5be109-507f-4659-8428-b1d30b2bd887",
        "_cell_guid": "b6c2e931-4c97-4226-b6aa-c88ddf468fcc",
        "trusted": true,
        "collapsed": false,
        "id": "create_dataloaders_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 7Ô∏è‚É£ Initializing MedGemma (MedSigLIP)",
      "metadata": {
        "_uuid": "ad750cc7-9c2e-494e-ac89-f56af57643ad",
        "_cell_guid": "2460cf46-fa45-471d-8377-2420f1eabcbb",
        "trusted": true,
        "collapsed": false,
        "id": "model_loading_section_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "from transformers import AutoModel, AutoProcessor\nimport torch.nn as nn\nimport gc\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üñ•Ô∏è Using device: {device}\")\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\"\\nüì• Waking up the AI Assistant...\")\nmodel_id = \"google/medsiglip-448\"\n\ntry:\n    model = AutoModel.from_pretrained(\n        model_id,\n        torch_dtype=torch.float32\n    )\n    processor = AutoProcessor.from_pretrained(model_id)\n\n    print(\"‚úÖ MedSigLIP model loaded successfully!\")\n    print(f\"\\nüìä Model info:\")\n    print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nexcept Exception as e:\n    print(f\"‚ùå Error loading model: {e}\")\n    raise\n\nclass_prompts = {\n    0: \"A medical image of acral lentiginous melanoma with black lines under the nail.\",\n    1: \"A medical image showing blue discoloration of the fingernail bed.\",\n    2: \"A medical image of nail clubbing with bulging and rounded nail appearance.\",\n    3: \"A medical image of a healthy normal nail.\",\n    4: \"A medical image of onychogryphosis with thickened and curved nails.\",\n    5: \"A medical image of nail pitting with small depressions in the nail plate.\",\n    6: \"A medical image of psoriatic nails with pitting and discoloration.\"\n}\n\nprint(\"\\nüìù Generated text prompts for classes:\")\nfor class_idx, prompt in class_prompts.items():\n    print(f\"   {class_idx}. {prompt[:60]}...\")",
      "metadata": {
        "_uuid": "e2884612-2844-4656-920e-290f702d71c1",
        "_cell_guid": "b3a66a2a-9d20-4550-a470-1941f6350de9",
        "trusted": true,
        "collapsed": false,
        "id": "load_medsiglip_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 8Ô∏è‚É£ Adapting the AI for Dermatology (Specialized Fine-Tuning)",
      "metadata": {
        "_uuid": "a4e99b27-0a7a-40c8-8d98-938f5ddcaa9e",
        "_cell_guid": "ff1035cc-3d4b-4162-9b06-c43aa7dcbc83",
        "trusted": true,
        "collapsed": false,
        "id": "classifier_head_section_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "class MedSigLIPClassifier(nn.Module):\n    def __init__(self, medsiglip_model, num_classes, device0='cuda:0', device1='cuda:1'):\n        super().__init__()\n        self.medsiglip = medsiglip_model.to(device0)\n        self.device0 = device0\n        self.device1 = device1\n\n        embed_dim = 1152\n\n        self.classifier = nn.Sequential(\n            nn.Linear(embed_dim, 768),\n            nn.LayerNorm(768),\n            nn.GELU(),\n            nn.Dropout(0.4),\n            \n            nn.Linear(768, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Dropout(0.4),\n            \n            nn.Linear(512, 256),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            \n            nn.Linear(256, num_classes)\n        ).to(device1)\n\n        # FREEZE ALL MedSigLIP layers to save memory\n        for param in self.medsiglip.parameters():\n            param.requires_grad = False\n\n    def forward(self, images):\n        # Images on GPU 0\n        images = images.to(self.device0)\n        \n        # Get embeddings on GPU 0\n        with torch.no_grad():\n            outputs = self.medsiglip.vision_model(pixel_values=images)\n            embeddings = outputs.pooler_output\n        \n        # Move embeddings to GPU 1 and cast to FP32\n        embeddings = embeddings.to(self.device1).float()\n        \n        # Classifier on GPU 1\n        logits = self.classifier(embeddings)\n        return logits\n\n\nnum_classes = len(train_dataset.classes)\n\n# Check available GPUs\nnum_gpus = torch.cuda.device_count()\nif num_gpus >= 2:\n    print(f\"üöÄ Using 2 GPUs for Model Parallel!\")\n    device0 = 'cuda:0'\n    device1 = 'cuda:1'\n    classifier = MedSigLIPClassifier(\n        medsiglip_model=model,\n        num_classes=num_classes,\n        device0=device0,\n        device1=device1\n    )\nelse:\n    print(f\"‚ö†Ô∏è Only {num_gpus} GPU(s) detected. Using single GPU.\")\n    device0 = 'cuda:0'\n    classifier = MedSigLIPClassifier(\n        medsiglip_model=model,\n        num_classes=num_classes,\n        device0=device0,\n        device1=device0\n    )\n\nprint(f\"‚úÖ Classifier ready! Classes: {num_classes}\")\nprint(f\"   MedSigLIP (Feature Extractor): {device0}\")\nprint(f\"   Classification Head: {device1 if num_gpus >= 2 else device0}\")\nprint(f\"   Optimized classifier: 1152‚Üí768‚Üí512‚Üí256‚Üí{num_classes}\")",
      "metadata": {
        "_uuid": "dd962261-bf14-4f7e-a4aa-38433da8036c",
        "_cell_guid": "42c76610-beb7-4365-8a96-c62c6d8b615b",
        "trusted": true,
        "collapsed": false,
        "id": "add_classifier_head_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 9Ô∏è‚É£ Configuring the Learning Process",
      "metadata": {
        "_uuid": "8d4e1345-a0c0-4085-b25d-9f983eda5373",
        "_cell_guid": "c690ce16-4143-4969-9f73-65a3711fe04c",
        "trusted": true,
        "collapsed": false,
        "id": "training_setup_section_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "import torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom tqdm import tqdm\nimport json\n\nNUM_EPOCHS = 10\nLEARNING_RATE = 1e-3  # Increased since we're only training classifier\nWEIGHT_DECAY = 1e-4\nGRADIENT_ACCUMULATION_STEPS = 1\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n# Only optimize classifier (MedSigLIP is frozen)\nclassifier_params = list(classifier.classifier.parameters())\n\noptimizer = optim.AdamW(classifier_params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, betas=(0.9, 0.999))\n\ntotal_steps = len(train_loader) * NUM_EPOCHS // GRADIENT_ACCUMULATION_STEPS\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=LEARNING_RATE,\n    total_steps=total_steps,\n    pct_start=0.3,\n    anneal_strategy='cos',\n    div_factor=25.0,\n    final_div_factor=1000.0\n)\n\nprint(\"‚úÖ Advanced training configuration (Classifier-Only Fine-Tuning):\")\nprint(f\"   üìç Epochs: {NUM_EPOCHS}\")\nprint(f\"   üìç Learning Rate: {LEARNING_RATE}\")\nprint(f\"   üìç Batch Size: {BATCH_SIZE}\")\nprint(f\"   üìç Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"   üìç Optimizer: AdamW with OneCycleLR\")\nprint(f\"   üìç Weight Decay: {WEIGHT_DECAY}\")\nprint(f\"   üìç Label Smoothing: 0.1\")\nprint(f\"   üìç MedSigLIP: FROZEN (memory efficient)\")\nprint(f\"   üìç GPU Strategy: Model Parallel (GPU 0 + GPU 1)\")",
      "metadata": {
        "_uuid": "09c60dd0-8f18-40e1-9ddd-217ab223a6b0",
        "_cell_guid": "6bafccea-0e1e-42fb-acfe-e7df5fc41019",
        "trusted": true,
        "collapsed": false,
        "id": "training_setup_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 1Ô∏è‚É£0Ô∏è‚É£ The Learning Loop (Training)",
      "metadata": {
        "_uuid": "e0ecf9e8-1192-42b5-b1da-cf59ab9cd766",
        "_cell_guid": "3bfe8e05-49dd-4abe-9795-892e55ef311b",
        "trusted": true,
        "collapsed": false,
        "id": "training_loop_section_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport math\n\ndef train_epoch(model, train_loader, criterion, optimizer, scheduler, accumulation_steps=1):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    optimizer.zero_grad()\n\n    pbar = tqdm(train_loader, desc=\"Learning from patients\")\n    for step, (images, labels) in enumerate(pbar):\n        labels = labels.to('cuda:1' if torch.cuda.device_count() >= 2 else 'cuda:0')\n\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        if math.isnan(loss.item()):\n            print(f\"   ‚ö†Ô∏è NaN loss at step {step}, skipping\")\n            optimizer.zero_grad()\n            continue\n            \n        loss = loss / accumulation_steps\n        loss.backward()\n        \n        if (step + 1) % accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.classifier.parameters(), max_norm=1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n        total_loss += loss.item() * accumulation_steps\n        preds = outputs.argmax(dim=1)\n        all_preds.extend(preds.detach().cpu().numpy())\n        all_labels.extend(labels.detach().cpu().numpy())\n\n        pbar.set_postfix({'loss': f'{loss.item()*accumulation_steps:.4f}'})\n\n    avg_loss = total_loss / len(train_loader)\n    accuracy = accuracy_score(all_labels, all_preds) if len(all_labels) > 0 else 0\n    return avg_loss, accuracy\n\ndef evaluate(model, test_loader, criterion):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        pbar = tqdm(test_loader, desc=\"Validating performance\")\n        for images, labels in pbar:\n            labels = labels.to('cuda:1' if torch.cuda.device_count() >= 2 else 'cuda:0')\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            total_loss += loss.item()\n            preds = outputs.argmax(dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n    avg_loss = total_loss / len(test_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n\n    return avg_loss, accuracy, precision, recall, f1, all_preds, all_labels\n\nprint(\"‚úÖ Advanced training functions defined!\")",
      "metadata": {
        "_uuid": "6983bdff-7a0b-417e-8ec6-73cd581d47c3",
        "_cell_guid": "3fcc7a84-ad84-4ee5-9b88-a6fe807bbf19",
        "trusted": true,
        "collapsed": false,
        "id": "training_loop_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 1Ô∏è‚É£1Ô∏è‚É£ Run ADVANCED Training (10 Epochs)",
      "metadata": {
        "_uuid": "4e59c6b7-d86a-422a-bc00-5fd4efb11762",
        "_cell_guid": "0f3929bf-449b-41fb-85f2-8fdc2e4eedbc",
        "trusted": true,
        "collapsed": false,
        "id": "main_training_section_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "history = {\n    'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [],\n    'test_precision': [], 'test_recall': [], 'test_f1': [], 'learning_rate': []\n}\n\nbest_accuracy = 0\nbest_epoch = 0\npatience_counter = 0\nmax_patience = 5\nbest_model_path = os.path.join(OUTPUT_PATH, 'best_model.pt')\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üöÄ COMMENCING MEDICAL AI TRAINING\")\nprint(f\"üöÄ Using {torch.cuda.device_count()} GPUs (Model Parallel)\")\nprint(\"=\"*70)\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\nüìä Epoch {epoch+1}/{NUM_EPOCHS}\")\n\n    train_loss, train_acc = train_epoch(classifier, train_loader, criterion, optimizer, scheduler, GRADIENT_ACCUMULATION_STEPS)\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n\n    test_loss, test_acc, test_prec, test_rec, test_f1, preds, labels = evaluate(classifier, test_loader, criterion)\n    history['test_loss'].append(test_loss)\n    history['test_acc'].append(test_acc)\n    history['test_precision'].append(test_prec)\n    history['test_recall'].append(test_rec)\n    history['test_f1'].append(test_f1)\n\n    print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n    print(f\"   Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n    print(f\"   Precision: {test_prec:.4f} | Recall: {test_rec:.4f} | F1: {test_f1:.4f}\")\n    print(f\"   LR: {optimizer.param_groups[0]['lr']:.6f}\")\n\n    if test_acc > best_accuracy:\n        best_accuracy = test_acc\n        best_epoch = epoch + 1\n        torch.save(classifier.state_dict(), best_model_path)\n        patience_counter = 0\n        print(f\"   ‚≠ê BEST model saved! (Accuracy: {best_accuracy:.4f})\")\n    else:\n        patience_counter += 1\n\n    torch.cuda.empty_cache()\n    gc.collect()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ TRAINING COMPLETED\")\nprint(f\"   Best Accuracy: {best_accuracy:.4f} at Epoch {best_epoch}\")\nprint(\"=\"*70)\n\nhistory_path = os.path.join(OUTPUT_PATH, 'training_history.json')\nwith open(history_path, 'w') as f:\n    json.dump(history, f, indent=4)\nprint(f\"\\nüíæ Training history saved to: {history_path}\")",
      "metadata": {
        "_uuid": "a6654985-194d-45e8-857b-745ca3dd6910",
        "_cell_guid": "a8b6c247-6a59-44b8-a692-aa9bf07986ee",
        "trusted": true,
        "collapsed": false,
        "id": "main_training_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 1Ô∏è‚É£2Ô∏è‚É£ Validating Our Diagnostics (Results)",
      "metadata": {
        "_uuid": "cb9af2aa-4de6-4c9e-a40d-671e664710b4",
        "_cell_guid": "50254f7e-76b2-46bf-81d2-82c713efd17a",
        "trusted": true,
        "collapsed": false,
        "id": "results_section_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nclassifier.load_state_dict(torch.load(best_model_path))\nclassifier.eval()\n\nwith torch.no_grad():\n    all_preds = []\n    all_labels = []\n    for images, labels in test_loader:\n        images = images.to(device)\n        if torch.cuda.is_available():\n            with autocast():\n                outputs = classifier(images)\n        else:\n            outputs = classifier(images)\n        preds = outputs.argmax(dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('MedSigLIP Nail Disease Classification - Advanced Results', fontsize=16, fontweight='bold')\n\naxes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\naxes[0, 0].plot(history['test_loss'], label='Test Loss', marker='s')\naxes[0, 0].axvline(x=best_epoch-1, color='red', linestyle='--', label=f'Best Epoch {best_epoch}')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Loss over Epochs')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(history['train_acc'], label='Train Accuracy', marker='o')\naxes[0, 1].plot(history['test_acc'], label='Test Accuracy', marker='s')\naxes[0, 1].axvline(x=best_epoch-1, color='red', linestyle='--', label=f'Best Epoch {best_epoch}')\naxes[0, 1].axhline(y=0.9, color='green', linestyle=':', alpha=0.5, label='90% Target')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Accuracy')\naxes[0, 1].set_title('Accuracy over Epochs')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\naxes[1, 0].plot(history['test_precision'], label='Precision', marker='o')\naxes[1, 0].plot(history['test_recall'], label='Recall', marker='s')\naxes[1, 0].plot(history['test_f1'], label='F1 Score', marker='^')\naxes[1, 0].axhline(y=0.9, color='green', linestyle=':', alpha=0.5, label='90% Target')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Score')\naxes[1, 0].set_title('Precision, Recall, F1 Score')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\ncm = confusion_matrix(all_labels, all_preds)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1],\n            xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\naxes[1, 1].set_title('Confusion Matrix')\naxes[1, 1].set_ylabel('True Label')\naxes[1, 1].set_xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_PATH, 'training_results.png'), dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"‚úÖ Training results visualization saved!\")\nprint(f\"üìÅ Saved to: {os.path.join(OUTPUT_PATH, 'training_results.png')}\")",
      "metadata": {
        "_uuid": "f380b6bb-bcdf-4c9e-a5e7-cee6909657c2",
        "_cell_guid": "b1fb12ed-a42e-4bd6-b0c5-9f3ee277b347",
        "trusted": true,
        "collapsed": false,
        "id": "results_visualization_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 1Ô∏è‚É£2Ô∏è‚É£A - üîç Safety Check: Is the Answer Memorized? (Overfitting Analysis)",
      "metadata": {
        "_uuid": "0221b60f-bbf1-438b-8acf-988fc538afd6",
        "_cell_guid": "9e96b3cb-52e5-47c7-bd75-148e0e32a1cb",
        "trusted": true,
        "collapsed": false,
        "id": "overfitting_analysis_section",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom scipy import stats\n\ntrain_losses = np.array(history['train_loss'])\ntest_losses = np.array(history['test_loss'])\ntrain_accs = np.array(history['train_acc'])\ntest_accs = np.array(history['test_acc'])\n\nloss_gap = test_losses - train_losses\nacc_gap = train_accs - test_accs\noverfitting_coeff = acc_gap / (train_accs + 1e-6)\n\nmetrics_df = pd.DataFrame({\n    'Epoch': np.arange(1, NUM_EPOCHS + 1),\n    'Train_Loss': train_losses,\n    'Test_Loss': test_losses,\n    'Loss_Gap': loss_gap,\n    'Train_Accuracy': train_accs,\n    'Test_Accuracy': test_accs,\n    'Accuracy_Gap': acc_gap,\n    'Overfitting_Coefficient': overfitting_coeff,\n    'Test_Precision': np.array(history['test_precision']),\n    'Test_Recall': np.array(history['test_recall']),\n    'Test_F1': np.array(history['test_f1']),\n    'Learning_Rate': np.array(history['learning_rate'])\n})\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä DETAILED OVERFITTING ANALYSIS\")\nprint(\"=\"*80)\nprint(\"\\nüîç Per-Epoch Metrics:\")\nprint(metrics_df.to_string(index=False))\n\nprint(\"\\n\\nüìà OVERFITTING SUMMARY STATISTICS:\")\nprint(\"-\" * 80)\nprint(f\"\\n1Ô∏è‚É£ Loss Gap Analysis:\")\nprint(f\"   ‚Ä¢ Average Loss Gap: {loss_gap.mean():.4f}\")\nprint(f\"   ‚Ä¢ Max Loss Gap: {loss_gap.max():.4f} (Epoch {loss_gap.argmax() + 1})\")\nprint(f\"   ‚Ä¢ Min Loss Gap: {loss_gap.min():.4f} (Epoch {loss_gap.argmin() + 1})\")\nprint(f\"   ‚Ä¢ Loss Gap Trend: {'üü¢ DECREASING (Improving)' if np.polyfit(range(len(loss_gap)), loss_gap, 1)[0] < 0 else 'üî¥ INCREASING (Worsening)'}\")\n\nprint(f\"\\n2Ô∏è‚É£ Accuracy Gap Analysis:\")\nprint(f\"   ‚Ä¢ Average Acc Gap: {acc_gap.mean():.4f}\")\nprint(f\"   ‚Ä¢ Max Acc Gap: {acc_gap.max():.4f} (Epoch {acc_gap.argmax() + 1})\")\nprint(f\"   ‚Ä¢ Min Acc Gap: {acc_gap.min():.4f} (Epoch {acc_gap.argmin() + 1})\")\nprint(f\"   ‚Ä¢ Final Acc Gap: {acc_gap[-1]:.4f}\")\n\nprint(f\"\\n3Ô∏è‚É£ Overfitting Coefficient:\")\nprint(f\"   ‚Ä¢ Average Coefficient: {overfitting_coeff.mean():.4f}\")\nprint(f\"   ‚Ä¢ Max Coefficient: {overfitting_coeff.max():.4f} (Epoch {overfitting_coeff.argmax() + 1})\")\nprint(f\"   ‚Ä¢ Overfitting Level: \", end=\"\")\nif overfitting_coeff.mean() < 0.05:\n    print(\"üü¢ MINIMAL (Excellent)\")\nelif overfitting_coeff.mean() < 0.15:\n    print(\"üü° MILD (Good)\")\nelif overfitting_coeff.mean() < 0.30:\n    print(\"üü† MODERATE (Fair)\")\nelse:\n    print(\"üî¥ SEVERE (Poor)\")\n\nprint(f\"\\n4Ô∏è‚É£ Final Performance:\")\nprint(f\"   ‚Ä¢ Final Train Acc: {train_accs[-1]:.4f}\")\nprint(f\"   ‚Ä¢ Final Test Acc: {test_accs[-1]:.4f}\")\nprint(f\"   ‚Ä¢ Best Test Acc: {test_accs.max():.4f} (Epoch {test_accs.argmax() + 1})\")\nprint(f\"   ‚Ä¢ Model Status: \", end=\"\")\nif test_accs.max() >= 0.90:\n    print(\"‚úÖ‚úÖ EXCELLENT PERFORMANCE (>=90%)\")\nelif test_accs.max() >= 0.85:\n    print(\"‚úÖ VERY GOOD PERFORMANCE (>=85%)\")\nelif test_accs.max() >= 0.80:\n    print(\"‚úÖ GOOD PERFORMANCE (>=80%)\")\nelif test_accs.max() >= 0.70:\n    print(\"‚ö†Ô∏è ACCEPTABLE PERFORMANCE (>=70%)\")\nelse:\n    print(\"‚ùå POOR PERFORMANCE (<70%)\")\n\ncsv_path = os.path.join(OUTPUT_PATH, 'overfitting_metrics.csv')\nmetrics_df.to_csv(csv_path, index=False)\nprint(f\"\\nüíæ Detailed metrics saved to: {csv_path}\")\nprint(\"=\"*80)",
      "metadata": {
        "_uuid": "43bafc17-25fe-49ef-aad8-66d415afce19",
        "_cell_guid": "99da5299-f701-4bab-8fcf-74db2f40c803",
        "trusted": true,
        "collapsed": false,
        "id": "overfitting_analysis_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 1Ô∏è‚É£2Ô∏è‚É£B - üìä Visualizing the Learning Curve",
      "metadata": {
        "_uuid": "0b773055-8123-4050-a843-f95da6ee1bd8",
        "_cell_guid": "2cf52448-b909-41a4-a05d-f9544145a70d",
        "trusted": true,
        "collapsed": false,
        "id": "overfitting_viz_section",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "fig = plt.figure(figsize=(18, 12))\ngs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n\nfig.suptitle('üîç Advanced Overfitting Detection & Analysis', fontsize=18, fontweight='bold', y=0.995)\n\nax1 = fig.add_subplot(gs[0, 0])\nepochs = np.arange(1, NUM_EPOCHS + 1)\nax1.bar(epochs, loss_gap, color=['red' if gap > loss_gap.mean() else 'green' for gap in loss_gap], alpha=0.7)\nax1.axhline(y=loss_gap.mean(), color='red', linestyle='--', linewidth=2, label=f'Avg: {loss_gap.mean():.4f}')\nax1.set_xlabel('Epoch', fontweight='bold')\nax1.set_ylabel('Loss Gap (Test - Train)', fontweight='bold')\nax1.set_title('Loss Gap Per Epoch\\\\n(Larger = More Overfitting)', fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2 = fig.add_subplot(gs[0, 1])\nax2.bar(epochs, acc_gap, color='coral', alpha=0.7)\nax2.axhline(y=acc_gap.mean(), color='darkred', linestyle='--', linewidth=2, label=f'Avg: {acc_gap.mean():.4f}')\nax2.set_xlabel('Epoch', fontweight='bold')\nax2.set_ylabel('Accuracy Gap (Train - Test)', fontweight='bold')\nax2.set_title('Accuracy Gap Per Epoch\\\\n(Smaller = Better)', fontweight='bold')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nax3 = fig.add_subplot(gs[0, 2])\ncolors = ['red' if coeff > 0.15 else 'orange' if coeff > 0.05 else 'green' for coeff in overfitting_coeff]\nax3.plot(epochs, overfitting_coeff, marker='o', linewidth=2, markersize=8, color='purple')\nax3.axhline(y=0.05, color='green', linestyle=':', linewidth=2, alpha=0.5, label='Minimal (0.05)')\nax3.axhline(y=0.15, color='orange', linestyle=':', linewidth=2, alpha=0.5, label='Moderate (0.15)')\nax3.set_xlabel('Epoch', fontweight='bold')\nax3.set_ylabel('Overfitting Coefficient', fontweight='bold')\nax3.set_title('Overfitting Coefficient Trend', fontweight='bold')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\nax4 = fig.add_subplot(gs[1, 0])\nax4.plot(epochs, train_losses, marker='o', label='Train Loss', linewidth=2.5, markersize=6)\nax4.plot(epochs, test_losses, marker='s', label='Test Loss', linewidth=2.5, markersize=6)\nax4.fill_between(epochs, train_losses, test_losses, alpha=0.2, color='red', label='Overfitting Gap')\nax4.set_xlabel('Epoch', fontweight='bold')\nax4.set_ylabel('Loss', fontweight='bold')\nax4.set_title('Train vs Test Loss with Gap', fontweight='bold')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nax5 = fig.add_subplot(gs[1, 1])\nax5.plot(epochs, train_accs, marker='o', label='Train Accuracy', linewidth=2.5, markersize=6, color='green')\nax5.plot(epochs, test_accs, marker='s', label='Test Accuracy', linewidth=2.5, markersize=6, color='blue')\nax5.fill_between(epochs, train_accs, test_accs, alpha=0.2, color='red')\nax5.axhline(y=0.9, color='green', linestyle=':', alpha=0.5, label='90% Target')\nax5.set_xlabel('Epoch', fontweight='bold')\nax5.set_ylabel('Accuracy', fontweight='bold')\nax5.set_title('Train vs Test Accuracy', fontweight='bold')\nax5.legend()\nax5.grid(True, alpha=0.3)\n\nax6 = fig.add_subplot(gs[1, 2])\nax6.plot(epochs, history['learning_rate'], marker='o', linewidth=2.5, markersize=6, color='purple')\nax6.set_xlabel('Epoch', fontweight='bold')\nax6.set_ylabel('Learning Rate', fontweight='bold')\nax6.set_title('Learning Rate Schedule', fontweight='bold')\nax6.grid(True, alpha=0.3)\nax6.set_yscale('log')\n\nax7 = fig.add_subplot(gs[2, :])\nheatmap_data = np.array([\n    train_losses / train_losses.max(),\n    test_losses / test_losses.max(),\n    train_accs,\n    test_accs,\n    history['test_precision'],\n    history['test_recall'],\n    history['test_f1']\n])\nim = ax7.imshow(heatmap_data, cmap='RdYlGn', aspect='auto')\nax7.set_yticks(range(7))\nax7.set_yticklabels(['Train Loss (norm)', 'Test Loss (norm)', 'Train Acc', 'Test Acc', 'Precision', 'Recall', 'F1 Score'])\nax7.set_xticks(range(NUM_EPOCHS))\nax7.set_xticklabels(epochs)\nax7.set_xlabel('Epoch', fontweight='bold')\nax7.set_title('All Metrics Heatmap (Green=Better, Red=Worse)', fontweight='bold')\nplt.colorbar(im, ax=ax7, label='Normalized Value')\n\nplt.savefig(os.path.join(OUTPUT_PATH, 'overfitting_analysis.png'), dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"‚úÖ Overfitting analysis visualization saved!\")\nprint(f\"üìÅ Saved to: {os.path.join(OUTPUT_PATH, 'overfitting_analysis.png')}\")",
      "metadata": {
        "_uuid": "089e5b2f-38cd-4c20-a996-0c66ad364a89",
        "_cell_guid": "0f02417d-b987-413d-a52d-e4062940f877",
        "trusted": true,
        "collapsed": false,
        "id": "overfitting_viz_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 1Ô∏è‚É£3Ô∏è‚É£ üöÄ VERTEX AI DEPLOYMENT: Save Complete Model for HuggingFace Hub\n\n**Critical for Vertex AI Deployment**: We need to save the COMPLETE model (not just weights) in HuggingFace format so it can be:\n1. Uploaded to HuggingFace Hub\n2. Accessed by Vertex AI Model Garden\n3. Deployed as a prediction endpoint\n\nThis section creates a deployment-ready model package.",
      "metadata": {
        "id": "vertex_ai_save_section",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "import shutil\nfrom pathlib import Path\n\n# Create deployment directory\nDEPLOYMENT_PATH = os.path.join(OUTPUT_PATH, 'medsiglip_nail_classifier_hf')\nos.makedirs(DEPLOYMENT_PATH, exist_ok=True)\n\nprint(\"=\"*70)\nprint(\"üöÄ PREPARING MODEL FOR VERTEX AI DEPLOYMENT\")\nprint(\"=\"*70)\n\n# Step 1: Load best model weights\nprint(\"\\nüì• Step 1: Loading best trained model...\")\nclassifier.load_state_dict(torch.load(best_model_path))\nclassifier.eval()\nprint(\"‚úÖ Best model loaded\")\n\n# Step 2: Move entire model to single device for saving\nprint(\"\\nüîÑ Step 2: Consolidating model to single device...\")\nsave_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\n# Create a version with both components on same device\nclass MedSigLIPClassifierSingleDevice(nn.Module):\n    \"\"\"Unified model for deployment - all on single device\"\"\"\n    def __init__(self, medsiglip_model, classifier_head, num_classes):\n        super().__init__()\n        self.medsiglip = medsiglip_model\n        self.classifier = classifier_head\n        self.num_classes = num_classes\n        \n    def forward(self, pixel_values):\n        # Get embeddings from vision model\n        with torch.no_grad():\n            outputs = self.medsiglip.vision_model(pixel_values=pixel_values)\n            embeddings = outputs.pooler_output\n        \n        # Classification\n        logits = self.classifier(embeddings.float())\n        return logits\n\n# Consolidate model\nconsolidated_model = MedSigLIPClassifierSingleDevice(\n    medsiglip_model=classifier.medsiglip.to(save_device),\n    classifier_head=classifier.classifier.to(save_device),\n    num_classes=num_classes\n).to(save_device)\n\nprint(f\"‚úÖ Model consolidated on {save_device}\")\n\n# Step 3: Save processor (critical for inference)\nprint(\"\\nüíæ Step 3: Saving image processor...\")\nprocessor.save_pretrained(DEPLOYMENT_PATH)\nprint(f\"‚úÖ Processor saved to: {DEPLOYMENT_PATH}\")\n\n# Step 4: Save complete model using HuggingFace format\nprint(\"\\nüíæ Step 4: Saving complete model...\")\nmodel_save_path = os.path.join(DEPLOYMENT_PATH, 'pytorch_model.bin')\ntorch.save({\n    'model_state_dict': consolidated_model.state_dict(),\n    'num_classes': num_classes,\n    'class_names': train_dataset.classes,\n    'image_size': IMAGE_SIZE,\n    'best_accuracy': best_accuracy,\n    'best_epoch': best_epoch\n}, model_save_path)\nprint(f\"‚úÖ Model saved to: {model_save_path}\")\n\n# Step 5: Create config.json for HuggingFace\nprint(\"\\nüìù Step 5: Creating model configuration...\")\nconfig = {\n    \"model_type\": \"medsiglip-classifier\",\n    \"base_model\": \"google/medsiglip-448\",\n    \"num_classes\": num_classes,\n    \"class_names\": train_dataset.classes,\n    \"image_size\": IMAGE_SIZE,\n    \"embedding_dim\": 1152,\n    \"classifier_hidden_dims\": [768, 512, 256],\n    \"best_accuracy\": float(best_accuracy),\n    \"best_epoch\": int(best_epoch),\n    \"framework\": \"pytorch\",\n    \"task\": \"image-classification\"\n}\n\nconfig_path = os.path.join(DEPLOYMENT_PATH, 'config.json')\nwith open(config_path, 'w') as f:\n    json.dump(config, f, indent=2)\nprint(f\"‚úÖ Config saved to: {config_path}\")\n\n# Step 6: Create README for HuggingFace Hub\nprint(\"\\nüìÑ Step 6: Creating model card (README.md)...\")\nreadme_content = f\"\"\"---\nlicense: apache-2.0\ntags:\n- medical\n- image-classification\n- nail-disease\n- medsiglip\n- dermatology\nlibrary_name: transformers\npipeline_tag: image-classification\n---\n\n# MedSigLIP Nail Disease Classifier\n\n## Model Description\n\nThis model is fine-tuned from [google/medsiglip-448](https://huggingface.co/google/medsiglip-448) for nail disease classification.\nIt can detect 7 different nail conditions with {best_accuracy*100:.2f}% accuracy.\n\n## Detected Conditions\n\n{chr(10).join([f'{i+1}. {cls}' for i, cls in enumerate(train_dataset.classes)])}\n\n## Performance\n\n- **Accuracy**: {best_accuracy*100:.2f}%\n- **Training Epochs**: {best_epoch}\n- **Image Size**: {IMAGE_SIZE}x{IMAGE_SIZE}\n\n## Usage\n\n```python\nimport torch\nfrom transformers import AutoProcessor\nfrom PIL import Image\n\n# Load processor\nprocessor = AutoProcessor.from_pretrained(\"YOUR_HF_USERNAME/medsiglip-nail-classifier\")\n\n# Load model\nmodel = torch.load(\"pytorch_model.bin\")\nmodel.eval()\n\n# Inference\nimage = Image.open(\"nail_image.jpg\")\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = outputs.logits.argmax(dim=-1)\n    \nprint(f\"Predicted class: {{predictions.item()}}\")\n```\n\n## Deployment to Vertex AI\n\n1. Upload this model to HuggingFace Hub\n2. In Google Cloud Vertex AI, navigate to Model Garden\n3. Select \"Import\" ‚Üí \"From HuggingFace\"\n4. Enter your model repository URL\n5. Deploy to get prediction endpoint\n\n## Training Details\n\n- **Base Model**: google/medsiglip-448 (frozen)\n- **Classifier Architecture**: 1152 ‚Üí 768 ‚Üí 512 ‚Üí 256 ‚Üí {num_classes}\n- **Optimizer**: AdamW with OneCycleLR\n- **Data Augmentation**: Extensive (rotation, flip, color jitter, etc.)\n\n## Limitations\n\n- This model is for research purposes only\n- Not approved for clinical diagnosis\n- Should be used alongside professional medical evaluation\n\n## Citation\n\nIf you use this model, please cite:\n\n```bibtex\n@misc{{medsiglip-nail-classifier,\n  author = {{Your Name}},\n  title = {{MedSigLIP Nail Disease Classifier}},\n  year = {{2026}},\n  publisher = {{HuggingFace}},\n  howpublished = {{\\\\url{{https://huggingface.co/YOUR_USERNAME/medsiglip-nail-classifier}}}}\n}}\n```\n\"\"\"\n\nreadme_path = os.path.join(DEPLOYMENT_PATH, 'README.md')\nwith open(readme_path, 'w') as f:\n    f.write(readme_content)\nprint(f\"‚úÖ README saved to: {readme_path}\")\n\n# Step 7: Create inference example script\nprint(\"\\nüîß Step 7: Creating inference example script...\")\ninference_script = '''import torch\nimport torch.nn as nn\nfrom transformers import AutoModel, AutoProcessor\nfrom PIL import Image\nimport json\n\nclass MedSigLIPClassifierSingleDevice(nn.Module):\n    \"\"\"Unified model for deployment\"\"\"\n    def __init__(self, medsiglip_model, classifier_head, num_classes):\n        super().__init__()\n        self.medsiglip = medsiglip_model\n        self.classifier = classifier_head\n        self.num_classes = num_classes\n        \n    def forward(self, pixel_values):\n        with torch.no_grad():\n            outputs = self.medsiglip.vision_model(pixel_values=pixel_values)\n            embeddings = outputs.pooler_output\n        logits = self.classifier(embeddings.float())\n        return logits\n\ndef load_model(model_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    \"\"\"Load the fine-tuned model\"\"\"\n    # Load config\n    with open(f\"{model_path}/config.json\", \"r\") as f:\n        config = json.load(f)\n    \n    # Load processor\n    processor = AutoProcessor.from_pretrained(model_path)\n    \n    # Load base MedSigLIP\n    base_model = AutoModel.from_pretrained(\"google/medsiglip-448\")\n    \n    # Recreate classifier\n    classifier = nn.Sequential(\n        nn.Linear(1152, 768),\n        nn.LayerNorm(768),\n        nn.GELU(),\n        nn.Dropout(0.4),\n        nn.Linear(768, 512),\n        nn.LayerNorm(512),\n        nn.GELU(),\n        nn.Dropout(0.4),\n        nn.Linear(512, 256),\n        nn.LayerNorm(256),\n        nn.GELU(),\n        nn.Dropout(0.3),\n        nn.Linear(256, config[\"num_classes\"])\n    )\n    \n    # Create full model\n    model = MedSigLIPClassifierSingleDevice(\n        medsiglip_model=base_model,\n        classifier_head=classifier,\n        num_classes=config[\"num_classes\"]\n    )\n    \n    # Load trained weights\n    checkpoint = torch.load(f\"{model_path}/pytorch_model.bin\", map_location=device)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.to(device)\n    model.eval()\n    \n    return model, processor, config[\"class_names\"]\n\ndef predict(image_path, model, processor, class_names, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    \"\"\"Make prediction on a single image\"\"\"\n    # Load and preprocess image\n    image = Image.open(image_path).convert(\"RGB\")\n    inputs = processor(images=image, return_tensors=\"pt\")\n    pixel_values = inputs[\"pixel_values\"].to(device)\n    \n    # Inference\n    with torch.no_grad():\n        logits = model(pixel_values)\n        probs = torch.softmax(logits, dim=-1)\n        pred_idx = probs.argmax(dim=-1).item()\n        confidence = probs[0, pred_idx].item()\n    \n    return {\n        \"predicted_class\": class_names[pred_idx],\n        \"confidence\": confidence,\n        \"all_probabilities\": {class_names[i]: probs[0, i].item() for i in range(len(class_names))}\n    }\n\nif __name__ == \"__main__\":\n    # Example usage\n    MODEL_PATH = \"./medsiglip_nail_classifier_hf\"\n    IMAGE_PATH = \"test_nail_image.jpg\"\n    \n    print(\"Loading model...\")\n    model, processor, class_names = load_model(MODEL_PATH)\n    \n    print(f\"Making prediction on {IMAGE_PATH}...\")\n    result = predict(IMAGE_PATH, model, processor, class_names)\n    \n    print(f\"\\\\nPrediction: {result['predicted_class']}\")\n    print(f\"Confidence: {result['confidence']*100:.2f}%\")\n    print(\"\\\\nAll probabilities:\")\n    for cls, prob in result['all_probabilities'].items():\n        print(f\"  {cls}: {prob*100:.2f}%\")\n'''\n\ninference_script_path = os.path.join(DEPLOYMENT_PATH, 'inference.py')\nwith open(inference_script_path, 'w') as f:\n    f.write(inference_script)\nprint(f\"‚úÖ Inference script saved to: {inference_script_path}\")\n\n# Step 8: Summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ MODEL PACKAGE READY FOR VERTEX AI DEPLOYMENT\")\nprint(\"=\"*70)\n\nprint(f\"\\nüì¶ Deployment Package Contents:\")\nfor item in sorted(os.listdir(DEPLOYMENT_PATH)):\n    item_path = os.path.join(DEPLOYMENT_PATH, item)\n    if os.path.isfile(item_path):\n        size_mb = os.path.getsize(item_path) / (1024*1024)\n        print(f\"   ‚Ä¢ {item} ({size_mb:.2f} MB)\")\n    else:\n        print(f\"   ‚Ä¢ {item}/ (directory)\")\n\nprint(f\"\\nüìÅ Full package location: {DEPLOYMENT_PATH}\")\n\nprint(\"\\nüöÄ NEXT STEPS FOR VERTEX AI DEPLOYMENT:\")\nprint(\"   1. ‚úÖ Package created successfully\")\nprint(\"   2. üì§ Upload to HuggingFace Hub (see next cell)\")\nprint(\"   3. ‚òÅÔ∏è Deploy from Vertex AI Model Garden\")\nprint(\"   4. üîå Get prediction endpoint URL\")\nprint(\"   5. üì± Integrate with mobile app\")\n\nprint(\"\\nüí° TIP: The complete model is now in HuggingFace-compatible format!\")\nprint(\"=\"*70)",
      "metadata": {
        "id": "vertex_ai_save_code",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 1Ô∏è‚É£4Ô∏è‚É£ üì§ Upload Model to HuggingFace Hub\n\n**This is the critical step for Vertex AI deployment!**\n\nOnce uploaded to HuggingFace Hub, you can:\n1. Point Vertex AI Model Garden to your repository\n2. Deploy with one click\n3. Get a prediction endpoint for your mobile app\n\n**Before running**: Make sure you're logged in (cell 1) and replace `YOUR_USERNAME` with your HuggingFace username.",
      "metadata": {
        "id": "hf_upload_section",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "from huggingface_hub import HfApi, create_repo\nimport os\n\nprint(\"=\"*70)\nprint(\"üì§ UPLOADING MODEL TO HUGGINGFACE HUB\")\nprint(\"=\"*70)\n\n# Configuration - CHANGE THESE VALUES\nHF_USERNAME = \"YOUR_USERNAME\"  # ‚ö†Ô∏è CHANGE THIS to your HuggingFace username\nMODEL_NAME = \"medsiglip-nail-disease-classifier\"\nREPO_ID = f\"{HF_USERNAME}/{MODEL_NAME}\"\nPRIVATE = True  # Set to False if you want it public\n\nprint(f\"\\nüìù Repository Configuration:\")\nprint(f\"   Username: {HF_USERNAME}\")\nprint(f\"   Model Name: {MODEL_NAME}\")\nprint(f\"   Full Repo ID: {REPO_ID}\")\nprint(f\"   Private: {PRIVATE}\")\n\nif HF_USERNAME == \"YOUR_USERNAME\":\n    print(\"\\n‚ùå ERROR: Please change HF_USERNAME to your actual HuggingFace username!\")\n    print(\"\\nüìã Steps:\")\n    print(\"   1. Replace 'YOUR_USERNAME' with your HuggingFace username\")\n    print(\"   2. Optionally change MODEL_NAME\")\n    print(\"   3. Run this cell again\")\nelse:\n    try:\n        # Initialize HF API\n        api = HfApi()\n        \n        # Step 1: Create repository (if it doesn't exist)\n        print(\"\\nüîß Step 1: Creating repository on HuggingFace Hub...\")\n        try:\n            repo_url = create_repo(\n                repo_id=REPO_ID,\n                private=PRIVATE,\n                repo_type=\"model\",\n                exist_ok=True\n            )\n            print(f\"‚úÖ Repository ready: {repo_url}\")\n        except Exception as e:\n            print(f\"‚ÑπÔ∏è Repository might already exist: {e}\")\n        \n        # Step 2: Upload all files from deployment directory\n        print(\"\\nüì§ Step 2: Uploading model files...\")\n        api.upload_folder(\n            folder_path=DEPLOYMENT_PATH,\n            repo_id=REPO_ID,\n            repo_type=\"model\",\n            commit_message=f\"Upload MedSigLIP nail classifier (Accuracy: {best_accuracy*100:.2f}%)\"\n        )\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"‚úÖ MODEL SUCCESSFULLY UPLOADED TO HUGGINGFACE HUB!\")\n        print(\"=\"*70)\n        \n        print(f\"\\nüåê Model URL: https://huggingface.co/{REPO_ID}\")\n        print(f\"\\nüéØ For Vertex AI Deployment:\")\n        print(f\"   1. Go to Google Cloud Console ‚Üí Vertex AI ‚Üí Model Garden\")\n        print(f\"   2. Click 'Import' ‚Üí 'From HuggingFace'\")\n        print(f\"   3. Enter repository: {REPO_ID}\")\n        print(f\"   4. Deploy and get prediction endpoint\")\n        \n        print(f\"\\nüîó Quick Links:\")\n        print(f\"   ‚Ä¢ Model Page: https://huggingface.co/{REPO_ID}\")\n        print(f\"   ‚Ä¢ Vertex AI: https://console.cloud.google.com/vertex-ai/model-garden\")\n        \n        print(f\"\\nüì± Your model is now ready for mobile app integration!\")\n        print(\"=\"*70)\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Upload failed: {e}\")\n        print(\"\\nüîß Troubleshooting:\")\n        print(\"   1. Make sure you're logged in (ran cell 1)\")\n        print(\"   2. Check your HuggingFace token has write permissions\")\n        print(\"   3. Verify your username is correct\")\n        print(\"   4. Try running this cell again\")",
      "metadata": {
        "id": "hf_upload_code",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 1Ô∏è‚É£5Ô∏è‚É£ üéØ Vertex AI Deployment Instructions\n\n**Now that your model is on HuggingFace Hub, here's how to deploy it on Vertex AI:**\n\n### Step-by-Step Deployment Guide\n\n#### 1. Access Vertex AI Model Garden\n```\n1. Go to: https://console.cloud.google.com/vertex-ai/model-garden\n2. Make sure you're in the correct GCP project\n3. Enable Vertex AI API if not already enabled\n```\n\n#### 2. Import Your HuggingFace Model\n```\n1. Click \"Import\" or \"Deploy Model\"\n2. Select \"HuggingFace Hub\" as source\n3. Enter your model repository: YOUR_USERNAME/medsiglip-nail-disease-classifier\n4. Authentication:\n   - If private: Provide your HF token\n   - If public: No authentication needed\n```\n\n#### 3. Configure Deployment\n```\n1. Machine Type: n1-standard-4 (or GPU for faster inference)\n2. Accelerator: Optional (NVIDIA T4 recommended for production)\n3. Replica Count: Start with 1, scale as needed\n4. Model Name: nail-disease-classifier\n```\n\n#### 4. Deploy and Get Endpoint\n```\n1. Click \"Deploy\"\n2. Wait 5-10 minutes for deployment\n3. Copy the prediction endpoint URL\n4. Save the endpoint URL for mobile app\n```\n\n#### 5. Test Your Endpoint\n```python\nfrom google.cloud import aiplatform\n\n# Initialize\naiplatform.init(project='YOUR_PROJECT_ID', location='us-central1')\n\n# Get endpoint\nendpoint = aiplatform.Endpoint('projects/PROJECT_ID/locations/us-central1/endpoints/ENDPOINT_ID')\n\n# Make prediction\ninstances = [{\"image_base64\": \"BASE64_ENCODED_IMAGE\"}]\nprediction = endpoint.predict(instances=instances)\n\nprint(prediction)\n```\n\n#### 6. Integrate with Mobile App\n```\n- Endpoint URL: https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/endpoints/ENDPOINT_ID:predict\n- Authentication: Service account key or OAuth 2.0\n- Request format: JSON with base64-encoded image\n```\n\n### Pricing Estimate\n- **Model Storage**: ~$0.10/GB/month\n- **Inference (n1-standard-4)**: ~$0.14/hour + per-prediction cost\n- **GPU (if used)**: Additional ~$0.35/hour\n\n### Monitoring\n- View predictions: Vertex AI ‚Üí Endpoints ‚Üí Monitoring\n- Check latency and errors\n- Set up alerts for downtime\n\n**üéâ Your model is now production-ready!**",
      "metadata": {
        "id": "vertex_deployment_instructions",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 1Ô∏è‚É£6Ô∏è‚É£ Final Report & Next Steps",
      "metadata": {
        "_uuid": "5cb7c12a-3d8a-4fdf-9efb-d0cb78932d1c",
        "_cell_guid": "4bf08377-762f-436b-b59c-4ab6f2869e02",
        "trusted": true,
        "collapsed": false,
        "id": "summary_section_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import classification_report, accuracy_score\n\nfinal_accuracy = accuracy_score(all_labels, all_preds)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ TRAINING COMPLETE: Model Ready for Deployment\")\nprint(\"=\"*70)\n\nprint(f\"\\nüìä Final Results:\")\nprint(f\"   ‚Ä¢ Final Test Accuracy: {final_accuracy*100:.2f}%\")\nprint(f\"   ‚Ä¢ Best Accuracy: {best_accuracy*100:.2f}% (Epoch {best_epoch})\")\nprint(f\"   ‚Ä¢ Number of Classes: {num_classes}\")\nprint(f\"   ‚Ä¢ Training Epochs: {NUM_EPOCHS}\")\nprint(f\"   ‚Ä¢ Target Achieved: {'‚úÖ YES! (>=90%)' if best_accuracy >= 0.9 else '‚ö†Ô∏è CLOSE (Try longer training)' if best_accuracy >= 0.85 else '‚ùå Continue training'}\")\n\nprint(f\"\\nüìã Per-Class Performance:\")\nprint(classification_report(all_labels, all_preds,\n                          target_names=train_dataset.classes,\n                          digits=4))\n\nprint(f\"\\nüìÅ Output Files (in /kaggle/working/output/):\")\noutput_files = os.listdir(OUTPUT_PATH)\nfor file in sorted(output_files):\n    file_path = os.path.join(OUTPUT_PATH, file)\n    if os.path.isfile(file_path):\n        file_size = os.path.getsize(file_path) / (1024*1024)\n        print(f\"   ‚Ä¢ {file} ({file_size:.2f} MB)\")\n    else:\n        print(f\"   ‚Ä¢ {file}/ (directory)\")\n\nprint(f\"\\nüöÄ Deployment Checklist:\")\nprint(f\"   ‚úÖ Model trained and validated\")\nprint(f\"   ‚úÖ Complete model package created (HuggingFace format)\")\nprint(f\"   ‚úÖ Inference script included\")\nprint(f\"   ‚úÖ Ready for HuggingFace Hub upload\")\nprint(f\"   üì§ Next: Upload to HuggingFace (cell above)\")\nprint(f\"   ‚òÅÔ∏è Then: Deploy via Vertex AI Model Garden\")\nprint(f\"   üì± Finally: Integrate with mobile app\")\n\nif best_accuracy < 0.90:\n    print(f\"\\nüí° TIPS TO IMPROVE ACCURACY:\")\n    print(f\"   ‚Ä¢ Try increase epochs from {NUM_EPOCHS} to 15-20\")\n    print(f\"   ‚Ä¢ Reduce batch size to 8 for more frequent updates\")\n    print(f\"   ‚Ä¢ Check for class imbalance in your dataset\")\n    print(f\"   ‚Ä¢ Ensure high-quality training data\")\n    print(f\"   ‚Ä¢ Consider test-time augmentation\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üéâ MedGemma is ready for Vertex AI deployment!\")\nprint(\"MedGemma Impact Challenge Submission - January 2026\")\nprint(\"=\"*70)",
      "metadata": {
        "_uuid": "1336cff6-d371-4e32-a22e-b4d4e9fd0972",
        "_cell_guid": "59ada6f8-29d7-45d7-a05a-ab5f9fdd75cd",
        "trusted": true,
        "collapsed": false,
        "id": "summary_kaggle",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}