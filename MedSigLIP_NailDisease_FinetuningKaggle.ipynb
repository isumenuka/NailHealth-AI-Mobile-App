{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nail_disease_title_kaggle"
      },
      "source": [
        "# ğŸ¥ MedSigLIP Fine-tuning for Nail Disease Classification on Kaggle\n",
        "\n",
        "**Project**: Nail Disease Detection & Classification  \n",
        "**Model**: Google's MedSigLIP (Medical SigLIP Vision-Language Model)  \n",
        "**Platform**: Kaggle Notebooks  \n",
        "**Dataset**: Nail disease images from Kaggle Dataset (7 categories)  \n",
        "**Created**: January 2026  \n",
        "**License**: Apache 2.0\n",
        "**Version**: 3.1 - 10-Epoch Advanced Strategy for 0.9+ Accuracy\n",
        "\n",
        "---\n",
        "\n",
        "## âœ¨ Key Features\n",
        "\n",
        "- âœ… Direct Kaggle dataset integration (`/kaggle/input/nail-disease-dataset`)\n",
        "- âœ… No ZIP file extraction required\n",
        "- âœ… Auto-detects train/test directories\n",
        "- âœ… GPU optimization (P100 available)\n",
        "- âœ… Comprehensive error handling\n",
        "- âœ… Real-time training visualization\n",
        "- âœ… **NEW: Advanced Overfitting Detection Graphs**\n",
        "- âœ… **NEW: Multi-Layer Fine-Tuning (Last 8 Layers)**\n",
        "- âœ… **NEW: Warm-up Learning Rate Schedule**\n",
        "- âœ… **NEW: Gradient Accumulation + Clipping**\n",
        "- âœ… **NEW: Optimized for 0.9+ Accuracy in 10 Epochs**\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Š Expected Dataset Structure\n",
        "\n",
        "```\n",
        "/kaggle/input/nail-disease-dataset/\n",
        "â”œâ”€â”€ train/                    (80% - ~5,300 images)\n",
        "â”‚   â”œâ”€â”€ Acral_Lentiginous_Melanoma/\n",
        "â”‚   â”œâ”€â”€ blue_finger/\n",
        "â”‚   â”œâ”€â”€ clubbing/\n",
        "â”‚   â”œâ”€â”€ Healthy_Nail/\n",
        "â”‚   â”œâ”€â”€ Onychogryphosis/\n",
        "â”‚   â”œâ”€â”€ pitting/\n",
        "â”‚   â””â”€â”€ psoriasis/\n",
        "â””â”€â”€ test/                     (20% - ~1,350 images)\n",
        "    â”œâ”€â”€ Acral_Lentiginous_Melanoma/\n",
        "    â”œâ”€â”€ blue_finger/\n",
        "    â”œâ”€â”€ clubbing/\n",
        "    â”œâ”€â”€ Healthy_Nail/\n",
        "    â”œâ”€â”€ Onychogryphosis/\n",
        "    â”œâ”€â”€ pitting/\n",
        "    â””â”€â”€ psoriasis/\n",
        "```\n",
        "\n",
        "## ğŸ¯ Nail Disease Categories\n",
        "\n",
        "1. **Acral Lentiginous Melanoma (ALM)** - Black/brown lines under nail\n",
        "2. **Blue Finger** - Blue discoloration of nail bed\n",
        "3. **Clubbing** - Bulging, rounded nail appearance\n",
        "4. **Healthy Nail** - Normal reference\n",
        "5. **Onychogryphosis** - Thickened, curved nails\n",
        "6. **Pitting** - Small depressions in nail plate\n",
        "7. **Psoriasis** - Nail pitting and discoloration from psoriasis\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Expected Outcomes (v3.1)\n",
        "\n",
        "- **Training Time**: 30-45 minutes (P100 GPU, 10 epochs)\n",
        "- **Expected Accuracy**: 90-95% on test set (with advanced strategy)\n",
        "- **Model Size**: ~420 MB (compressed)\n",
        "- **Inference Time**: <500ms per image\n",
        "- **Mobile Compatible**: Yes (TensorFlow Lite conversion included)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf_login_section"
      },
      "source": [
        "## 1ï¸âƒ£ Hugging Face Login (IMPORTANT!)\n",
        "\n",
        "**Run this first!** You need a Hugging Face token to access MedSigLIP.\n",
        "\n",
        "1. Get token: https://huggingface.co/settings/tokens\n",
        "2. Request access: https://huggingface.co/google/medsiglip-448\n",
        "3. Run cell below and paste your token when prompted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf_login",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ” HUGGING FACE LOGIN\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nYou'll be prompted to enter your Hugging Face token.\")\n",
        "print(\"Get your token: https://huggingface.co/settings/tokens\\n\")\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "print(\"\\nâœ… Login successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section_kaggle"
      },
      "source": [
        "## 2ï¸âƒ£ Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision transformers datasets pillow scikit-learn matplotlib tqdm numpy pandas\n",
        "!pip install -q open-clip-torch\n",
        "!pip install -q onnx onnxruntime\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "print(\"âœ… All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpu_check_section_kaggle"
      },
      "source": [
        "## 3ï¸âƒ£ Check GPU & Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ–¥ï¸  ENVIRONMENT INFO\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Python Version: {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"âš ï¸  WARNING: No GPU detected. Training will be very slow.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaggle_setup_section"
      },
      "source": [
        "## 4ï¸âƒ£ Setup Kaggle Dataset Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaggle_dataset_setup",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "KAGGLE_DATASET_PATH = '/kaggle/input/nail-disease-dataset-medsiglip'\n",
        "OUTPUT_PATH = '/kaggle/working/output'\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ“‚ KAGGLE DATASET SETUP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if dataset path exists\n",
        "if not os.path.exists(KAGGLE_DATASET_PATH):\n",
        "    print(f\"\\nâŒ ERROR: Dataset not found at {KAGGLE_DATASET_PATH}\")\n",
        "    print(\"\\nğŸ“‹ SOLUTION:\")\n",
        "    print(\"   1. Add 'nail-disease-dataset' as an input to this notebook\")\n",
        "    print(\"   2. Go to notebook settings â†’ Add data\")\n",
        "    print(\"   3. Search for 'nail-disease-dataset' and add it\")\n",
        "    print(\"   4. Re-run this cell\")\n",
        "    raise FileNotFoundError(f\"Dataset not found at {KAGGLE_DATASET_PATH}\")\n",
        "\n",
        "print(f\"âœ… Dataset path found: {KAGGLE_DATASET_PATH}\")\n",
        "\n",
        "# List available datasets\n",
        "print(f\"\\nğŸ“ Available Kaggle Inputs:\")\n",
        "for item in os.listdir('/kaggle/input'):\n",
        "    print(f\"   â€¢ {item}\")\n",
        "\n",
        "# Check for train and test directories\n",
        "print(f\"\\nğŸ” Looking for train/test directories...\")\n",
        "dataset_contents = os.listdir(KAGGLE_DATASET_PATH)\n",
        "print(f\"\\nğŸ“‚ Dataset contents:\")\n",
        "for item in dataset_contents:\n",
        "    item_path = os.path.join(KAGGLE_DATASET_PATH, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        file_count = len([f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))])\n",
        "        dir_count = len([d for d in os.listdir(item_path) if os.path.isdir(os.path.join(item_path, d))])\n",
        "        print(f\"   ğŸ“ {item}/ ({dir_count} subdirs, {file_count} files)\")\n",
        "\n",
        "# Set train and test paths\n",
        "TRAIN_DATA_PATH = os.path.join(KAGGLE_DATASET_PATH, 'train')\n",
        "TEST_DATA_PATH = os.path.join(KAGGLE_DATASET_PATH, 'test')\n",
        "\n",
        "if not os.path.exists(TRAIN_DATA_PATH) or not os.path.exists(TEST_DATA_PATH):\n",
        "    print(f\"\\nâŒ ERROR: train/ or test/ directories not found!\")\n",
        "    print(f\"   Expected structure:\")\n",
        "    print(f\"   /kaggle/input/nail-disease-dataset/\")\n",
        "    print(f\"   â”œâ”€â”€ train/ (with class folders)\")\n",
        "    print(f\"   â””â”€â”€ test/ (with class folders)\")\n",
        "    raise FileNotFoundError(\"train/ or test/ directories not found\")\n",
        "\n",
        "print(f\"\\nâœ… Dataset paths configured:\")\n",
        "print(f\"   TRAIN: {TRAIN_DATA_PATH}\")\n",
        "print(f\"   TEST: {TEST_DATA_PATH}\")\n",
        "print(f\"   OUTPUT: {OUTPUT_PATH}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading_section_kaggle"
      },
      "source": [
        "## 5ï¸âƒ£ Load & Inspect Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loader_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "IMAGE_SIZE = 448\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# ğŸ†• Enhanced augmentation for medical images\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.85, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.3),  # Medical images can have vertical flips\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"ğŸ“‚ Loading datasets...\")\n",
        "try:\n",
        "    train_dataset = ImageFolder(TRAIN_DATA_PATH, transform=train_transforms)\n",
        "    test_dataset = ImageFolder(TEST_DATA_PATH, transform=val_transforms)\n",
        "\n",
        "    print(f\"âœ… Training samples: {len(train_dataset)}\")\n",
        "    print(f\"âœ… Test samples: {len(test_dataset)}\")\n",
        "    print(f\"âœ… Number of classes: {len(train_dataset.classes)}\")\n",
        "    print(f\"\\nğŸ“‹ Class labels: {train_dataset.classes}\")\n",
        "\n",
        "    print(\"\\nğŸ“Š Class distribution (Training):\")\n",
        "    for cls_idx, cls_name in enumerate(train_dataset.classes):\n",
        "        count = sum(1 for x, y in train_dataset if y == cls_idx)\n",
        "        print(f\"   {cls_name}: {count} images\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading data: {e}\")\n",
        "    print(f\"\\nğŸ“ Please verify dataset structure:\")\n",
        "    print(f\"   â”œâ”€â”€ train/class1/, class2/, ...\")\n",
        "    print(f\"   â””â”€â”€ test/class1/, class2/, ...\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataloader_section_kaggle"
      },
      "source": [
        "## 6ï¸âƒ£ Create Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataloaders_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"âœ… Train DataLoader: {len(train_loader)} batches\")\n",
        "print(f\"âœ… Test DataLoader: {len(test_loader)} batches\")\n",
        "\n",
        "print(\"\\nğŸ” Testing batch loading...\")\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f\"   Batch shape: {images.shape}\")\n",
        "print(f\"   Labels: {labels[:5].tolist()}\")\n",
        "print(\"âœ… Data loading successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_loading_section_kaggle"
      },
      "source": [
        "## 7ï¸âƒ£ Load MedSigLIP Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_medsiglip_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# CELL 7: Load MedSigLIP Model & Create Text Prompts\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸ–¥ï¸  Using device: {device}\")\n",
        "\n",
        "print(\"\\nğŸ“¥ Loading MedSigLIP model...\")\n",
        "model_id = \"google/medsiglip-448\"\n",
        "\n",
        "try:\n",
        "    model = AutoModel.from_pretrained(\"google/medsiglip-448\")\n",
        "    processor = AutoProcessor.from_pretrained(\"google/medsiglip-448\")\n",
        "\n",
        "    print(\"âœ… MedSigLIP model loaded successfully!\")\n",
        "    print(f\"\\nğŸ“Š Model info:\")\n",
        "    print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    print(f\"\\nğŸ“‹ Troubleshooting:\")\n",
        "    print(f\"   1. Make sure you logged in with Hugging Face token\")\n",
        "    print(f\"   2. Request access: https://huggingface.co/google/medsiglip-448\")\n",
        "    print(f\"   3. Wait a few minutes for access grant\")\n",
        "    raise\n",
        "\n",
        "# Create text prompts for each class\n",
        "class_prompts = {\n",
        "    0: \"A medical image of acral lentiginous melanoma with black lines under the nail.\",\n",
        "    1: \"A medical image showing blue discoloration of the fingernail bed.\",\n",
        "    2: \"A medical image of nail clubbing with bulging and rounded nail appearance.\",\n",
        "    3: \"A medical image of a healthy normal nail.\",\n",
        "    4: \"A medical image of onychogryphosis with thickened and curved nails.\",\n",
        "    5: \"A medical image of nail pitting with small depressions in the nail plate.\",\n",
        "    6: \"A medical image of psoriatic nails with pitting and discoloration.\"\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ“ Generated text prompts for classes:\")\n",
        "for class_idx, prompt in class_prompts.items():\n",
        "    print(f\"   {class_idx}. {prompt[:60]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "classifier_head_section_kaggle"
      },
      "source": [
        "## 8ï¸âƒ£ Add Classification Head (ADVANCED - Last 8 Layers Unfrozen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "add_classifier_head_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class MedSigLIPClassifier(nn.Module):\n",
        "    def __init__(self, medsiglip_model, num_classes, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.medsiglip = medsiglip_model\n",
        "        self.device = device\n",
        "\n",
        "        embed_dim = 1152\n",
        "\n",
        "        # ğŸ†• LARGER CLASSIFIER for better feature learning\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.35),\n",
        "            \n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            \n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        # ğŸ†• IMPORTANT: Unfreeze LAST 8 LAYERS of vision model encoder (was 5)\n",
        "        for param in self.medsiglip.vision_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze last 8 transformer blocks for better domain adaptation\n",
        "        if hasattr(self.medsiglip.vision_model, 'encoder'):\n",
        "            for param in self.medsiglip.vision_model.encoder.layers[-8:].parameters():\n",
        "                param.requires_grad = True\n",
        "                \n",
        "        # Also unfreeze layer norm\n",
        "        if hasattr(self.medsiglip.vision_model, 'layer_norm'):\n",
        "            for param in self.medsiglip.vision_model.layer_norm.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Allow gradients to flow through vision model\n",
        "        outputs = self.medsiglip.vision_model(pixel_values=images)\n",
        "        embeddings = outputs.pooler_output\n",
        "\n",
        "        logits = self.classifier(embeddings)\n",
        "        return logits\n",
        "\n",
        "\n",
        "num_classes = len(train_dataset.classes)\n",
        "classifier = MedSigLIPClassifier(\n",
        "    medsiglip_model=model,\n",
        "    num_classes=num_classes,\n",
        "    device=device\n",
        ").to(device)\n",
        "\n",
        "print(f\"âœ… Classifier ready! Classes: {num_classes}\")\n",
        "print(f\"   ğŸ†• Enhanced architecture with last 8 vision layers unfrozen\")\n",
        "print(f\"   ğŸ†• Larger classifier: 1152â†’1024â†’512â†’256â†’128â†’{num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_setup_section_kaggle"
      },
      "source": [
        "## 9ï¸âƒ£ Setup ADVANCED Training Configuration (10 Epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_setup_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# ğŸ†• ADVANCED: Optimized hyperparameters for 0.9+ accuracy in 10 epochs\n",
        "NUM_EPOCHS = 10  # âœ… KEPT AS REQUESTED\n",
        "LEARNING_RATE = 1e-3  # Increased for better convergence\n",
        "WEIGHT_DECAY = 5e-4  # Stronger regularization\n",
        "WARMUP_EPOCHS = 2\n",
        "GRADIENT_ACCUMULATION_STEPS = 2\n",
        "\n",
        "# ğŸ†• Label smoothing for better regularization\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
        "\n",
        "# ğŸ†• Optimize both classifier and unfrozen vision model layers\n",
        "unfrozen_params = [p for p in classifier.medsiglip.vision_model.encoder.layers[-8:].parameters() if p.requires_grad]\n",
        "if hasattr(classifier.medsiglip.vision_model, 'layer_norm'):\n",
        "    unfrozen_params.extend([p for p in classifier.medsiglip.vision_model.layer_norm.parameters() if p.requires_grad])\n",
        "\n",
        "classifier_params = list(classifier.classifier.parameters())\n",
        "\n",
        "# Separate learning rates for different layer groups\n",
        "param_groups = [\n",
        "    {'params': classifier_params, 'lr': LEARNING_RATE},\n",
        "    {'params': unfrozen_params, 'lr': LEARNING_RATE * 0.1}  # Lower LR for vision model\n",
        "]\n",
        "\n",
        "optimizer = optim.AdamW(param_groups, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# ğŸ†• Cosine annealing with warm-up for better convergence\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS - WARMUP_EPOCHS, eta_min=1e-5)\n",
        "\n",
        "print(\"âœ… Advanced training configuration (10-Epoch Strategy):\")\n",
        "print(f\"   ğŸ“ Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"   ğŸ“ Classifier Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"   ğŸ“ Vision Model Learning Rate: {LEARNING_RATE * 0.1}\")\n",
        "print(f\"   ğŸ“ Batch Size: 32\")\n",
        "print(f\"   ğŸ“ Warm-up Epochs: {WARMUP_EPOCHS}\")\n",
        "print(f\"   ğŸ“ Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"   ğŸ“ Optimizer: AdamW with separate learning rates\")\n",
        "print(f\"   ğŸ“ Scheduler: CosineAnnealingLR\")\n",
        "print(f\"   ğŸ“ Weight Decay: {WEIGHT_DECAY}\")\n",
        "print(f\"   ğŸ“ Label Smoothing: 0.15\")\n",
        "print(f\"   ğŸ“ Unfrozen Vision Layers: Last 8 (+ layer_norm)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_loop_section_kaggle"
      },
      "source": [
        "## 1ï¸âƒ£0ï¸âƒ£ Training Functions (IMPROVED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_loop_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, accumulation_steps=1, warmup_epoch=False):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "    for step, (images, labels) in enumerate(pbar):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Gradient accumulation\n",
        "        loss = loss / accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.classifier.parameters(), 1.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * accumulation_steps\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        all_preds.extend(preds.cpu().detach().numpy())\n",
        "        all_labels.extend(labels.cpu().detach().numpy())\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{loss.item()*accumulation_steps:.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(test_loader, desc=\"Evaluating\")\n",
        "        for images, labels in pbar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    return avg_loss, accuracy, precision, recall, f1, all_preds, all_labels\n",
        "\n",
        "print(\"âœ… Advanced training functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main_training_section_kaggle"
      },
      "source": [
        "## 1ï¸âƒ£1ï¸âƒ£ Run ADVANCED Training (10 Epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main_training_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "history = {\n",
        "    'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [],\n",
        "    'test_precision': [], 'test_recall': [], 'test_f1': [], 'learning_rate': []\n",
        "}\n",
        "\n",
        "best_accuracy = 0\n",
        "best_epoch = 0\n",
        "patience_counter = 0\n",
        "max_patience = 5\n",
        "best_model_path = os.path.join(OUTPUT_PATH, 'best_model.pt')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸš€ STARTING 10-EPOCH ADVANCED TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nğŸ“Š Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    # ğŸ†• Warm-up phase\n",
        "    warmup = epoch < WARMUP_EPOCHS\n",
        "    if warmup:\n",
        "        warmup_lr = LEARNING_RATE * (epoch + 1) / WARMUP_EPOCHS\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = warmup_lr * (1 if param_group['lr'] >= LEARNING_RATE * 0.5 else 0.1)\n",
        "        print(f\"   ğŸ”¥ Warmup Phase (LR: {warmup_lr:.6f})\")\n",
        "\n",
        "    train_loss, train_acc = train_epoch(classifier, train_loader, criterion, optimizer, device, GRADIENT_ACCUMULATION_STEPS, warmup)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    test_loss, test_acc, test_prec, test_rec, test_f1, preds, labels = evaluate(classifier, test_loader, criterion, device)\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['test_acc'].append(test_acc)\n",
        "    history['test_precision'].append(test_prec)\n",
        "    history['test_recall'].append(test_rec)\n",
        "    history['test_f1'].append(test_f1)\n",
        "\n",
        "    print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"   Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "    print(f\"   Precision: {test_prec:.4f} | Recall: {test_rec:.4f} | F1: {test_f1:.4f}\")\n",
        "    print(f\"   LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # Step scheduler (after warmup)\n",
        "    if not warmup:\n",
        "        scheduler.step()\n",
        "\n",
        "    if test_acc > best_accuracy:\n",
        "        best_accuracy = test_acc\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save(classifier.state_dict(), best_model_path)\n",
        "        patience_counter = 0\n",
        "        print(f\"   â­ BEST model saved! (Accuracy: {best_accuracy:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= max_patience:\n",
        "            print(f\"   âš ï¸  No improvement for {max_patience} epochs. Consider stopping early.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… TRAINING COMPLETED\")\n",
        "print(f\"   Best Accuracy: {best_accuracy:.4f} at Epoch {best_epoch}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "history_path = os.path.join(OUTPUT_PATH, 'training_history.json')\n",
        "with open(history_path, 'w') as f:\n",
        "    json.dump(history, f, indent=4)\n",
        "print(f\"\\nğŸ’¾ Training history saved to: {history_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_section_kaggle"
      },
      "source": [
        "## 1ï¸âƒ£2ï¸âƒ£ Results & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results_visualization_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "classifier.load_state_dict(torch.load(best_model_path))\n",
        "classifier.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = classifier(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('MedSigLIP Nail Disease Classification - Advanced Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0, 0].plot(history['test_loss'], label='Test Loss', marker='s')\n",
        "axes[0, 0].axvline(x=best_epoch-1, color='red', linestyle='--', label=f'Best Epoch {best_epoch}')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Loss over Epochs')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
        "axes[0, 1].plot(history['test_acc'], label='Test Accuracy', marker='s')\n",
        "axes[0, 1].axvline(x=best_epoch-1, color='red', linestyle='--', label=f'Best Epoch {best_epoch}')\n",
        "axes[0, 1].axhline(y=0.9, color='green', linestyle=':', alpha=0.5, label='90% Target')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].set_title('Accuracy over Epochs')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].plot(history['test_precision'], label='Precision', marker='o')\n",
        "axes[1, 0].plot(history['test_recall'], label='Recall', marker='s')\n",
        "axes[1, 0].plot(history['test_f1'], label='F1 Score', marker='^')\n",
        "axes[1, 0].axhline(y=0.9, color='green', linestyle=':', alpha=0.5, label='90% Target')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Score')\n",
        "axes[1, 0].set_title('Precision, Recall, F1 Score')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1],\n",
        "            xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n",
        "axes[1, 1].set_title('Confusion Matrix')\n",
        "axes[1, 1].set_ylabel('True Label')\n",
        "axes[1, 1].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_PATH, 'training_results.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Training results visualization saved!\")\n",
        "print(f\"ğŸ“ Saved to: {os.path.join(OUTPUT_PATH, 'training_results.png')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overfitting_analysis_section"
      },
      "source": [
        "## 1ï¸âƒ£2ï¸âƒ£A - ğŸ” Advanced Overfitting Detection & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "overfitting_analysis_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Calculate overfitting metrics\n",
        "train_losses = np.array(history['train_loss'])\n",
        "test_losses = np.array(history['test_loss'])\n",
        "train_accs = np.array(history['train_acc'])\n",
        "test_accs = np.array(history['test_acc'])\n",
        "\n",
        "# 1. Loss Gap (most important overfitting indicator)\n",
        "loss_gap = test_losses - train_losses\n",
        "\n",
        "# 2. Accuracy Gap\n",
        "acc_gap = train_accs - test_accs\n",
        "\n",
        "# 3. Overfitting Coefficient (per epoch)\n",
        "overfitting_coeff = acc_gap / (train_accs + 1e-6)\n",
        "\n",
        "# 4. Create detailed metrics DataFrame\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Epoch': np.arange(1, NUM_EPOCHS + 1),\n",
        "    'Train_Loss': train_losses,\n",
        "    'Test_Loss': test_losses,\n",
        "    'Loss_Gap': loss_gap,\n",
        "    'Train_Accuracy': train_accs,\n",
        "    'Test_Accuracy': test_accs,\n",
        "    'Accuracy_Gap': acc_gap,\n",
        "    'Overfitting_Coefficient': overfitting_coeff,\n",
        "    'Test_Precision': np.array(history['test_precision']),\n",
        "    'Test_Recall': np.array(history['test_recall']),\n",
        "    'Test_F1': np.array(history['test_f1']),\n",
        "    'Learning_Rate': np.array(history['learning_rate'])\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“Š DETAILED OVERFITTING ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nğŸ” Per-Epoch Metrics:\")\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\\nğŸ“ˆ OVERFITTING SUMMARY STATISTICS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\n1ï¸âƒ£ Loss Gap Analysis:\")\n",
        "print(f\"   â€¢ Average Loss Gap: {loss_gap.mean():.4f}\")\n",
        "print(f\"   â€¢ Max Loss Gap: {loss_gap.max():.4f} (Epoch {loss_gap.argmax() + 1})\")\n",
        "print(f\"   â€¢ Min Loss Gap: {loss_gap.min():.4f} (Epoch {loss_gap.argmin() + 1})\")\n",
        "print(f\"   â€¢ Loss Gap Trend: {'ğŸŸ¢ DECREASING (Improving)' if np.polyfit(range(len(loss_gap)), loss_gap, 1)[0] < 0 else 'ğŸ”´ INCREASING (Worsening)'}\")\n",
        "\n",
        "print(f\"\\n2ï¸âƒ£ Accuracy Gap Analysis:\")\n",
        "print(f\"   â€¢ Average Acc Gap: {acc_gap.mean():.4f}\")\n",
        "print(f\"   â€¢ Max Acc Gap: {acc_gap.max():.4f} (Epoch {acc_gap.argmax() + 1})\")\n",
        "print(f\"   â€¢ Min Acc Gap: {acc_gap.min():.4f} (Epoch {acc_gap.argmin() + 1})\")\n",
        "print(f\"   â€¢ Final Acc Gap: {acc_gap[-1]:.4f}\")\n",
        "\n",
        "print(f\"\\n3ï¸âƒ£ Overfitting Coefficient:\")\n",
        "print(f\"   â€¢ Average Coefficient: {overfitting_coeff.mean():.4f}\")\n",
        "print(f\"   â€¢ Max Coefficient: {overfitting_coeff.max():.4f} (Epoch {overfitting_coeff.argmax() + 1})\")\n",
        "print(f\"   â€¢ Overfitting Level: \", end=\"\")\n",
        "if overfitting_coeff.mean() < 0.05:\n",
        "    print(\"ğŸŸ¢ MINIMAL (Excellent)\")\n",
        "elif overfitting_coeff.mean() < 0.15:\n",
        "    print(\"ğŸŸ¡ MILD (Good)\")\n",
        "elif overfitting_coeff.mean() < 0.30:\n",
        "    print(\"ğŸŸ  MODERATE (Fair)\")\n",
        "else:\n",
        "    print(\"ğŸ”´ SEVERE (Poor)\")\n",
        "\n",
        "print(f\"\\n4ï¸âƒ£ Final Performance:\")\n",
        "print(f\"   â€¢ Final Train Acc: {train_accs[-1]:.4f}\")\n",
        "print(f\"   â€¢ Final Test Acc: {test_accs[-1]:.4f}\")\n",
        "print(f\"   â€¢ Best Test Acc: {test_accs.max():.4f} (Epoch {test_accs.argmax() + 1})\")\n",
        "print(f\"   â€¢ Model Status: \", end=\"\")\n",
        "if test_accs.max() >= 0.90:\n",
        "    print(\"âœ…âœ… EXCELLENT PERFORMANCE (>=90%)\")\n",
        "elif test_accs.max() >= 0.85:\n",
        "    print(\"âœ… VERY GOOD PERFORMANCE (>=85%)\")\n",
        "elif test_accs.max() >= 0.80:\n",
        "    print(\"âœ… GOOD PERFORMANCE (>=80%)\")\n",
        "elif test_accs.max() >= 0.70:\n",
        "    print(\"âš ï¸ ACCEPTABLE PERFORMANCE (>=70%)\")\n",
        "else:\n",
        "    print(\"âŒ POOR PERFORMANCE (<70%)\")\n",
        "\n",
        "# Save metrics to CSV\n",
        "csv_path = os.path.join(OUTPUT_PATH, 'overfitting_metrics.csv')\n",
        "metrics_df.to_csv(csv_path, index=False)\n",
        "print(f\"\\nğŸ’¾ Detailed metrics saved to: {csv_path}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overfitting_viz_section"
      },
      "source": [
        "## 1ï¸âƒ£2ï¸âƒ£B - ğŸ“Š Comprehensive Overfitting Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "overfitting_viz_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create comprehensive overfitting visualization\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
        "\n",
        "fig.suptitle('ğŸ” Advanced Overfitting Detection & Analysis', fontsize=18, fontweight='bold', y=0.995)\n",
        "\n",
        "# 1. Loss Gap (most important)\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "epochs = np.arange(1, NUM_EPOCHS + 1)\n",
        "ax1.bar(epochs, loss_gap, color=['ğŸ”´' if gap > loss_gap.mean() else 'ğŸŸ¢' for gap in loss_gap], alpha=0.7)\n",
        "ax1.axhline(y=loss_gap.mean(), color='red', linestyle='--', linewidth=2, label=f'Avg: {loss_gap.mean():.4f}')\n",
        "ax1.set_xlabel('Epoch', fontweight='bold')\n",
        "ax1.set_ylabel('Loss Gap (Test - Train)', fontweight='bold')\n",
        "ax1.set_title('Loss Gap Per Epoch\\\\n(Larger = More Overfitting)', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Accuracy Gap\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.bar(epochs, acc_gap, color='coral', alpha=0.7)\n",
        "ax2.axhline(y=acc_gap.mean(), color='darkred', linestyle='--', linewidth=2, label=f'Avg: {acc_gap.mean():.4f}')\n",
        "ax2.set_xlabel('Epoch', fontweight='bold')\n",
        "ax2.set_ylabel('Accuracy Gap (Train - Test)', fontweight='bold')\n",
        "ax2.set_title('Accuracy Gap Per Epoch\\\\n(Smaller = Better)', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Overfitting Coefficient\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "colors = ['ğŸ”´' if coeff > 0.15 else 'ğŸŸ¡' if coeff > 0.05 else 'ğŸŸ¢' for coeff in overfitting_coeff]\n",
        "ax3.plot(epochs, overfitting_coeff, marker='o', linewidth=2, markersize=8, color='purple')\n",
        "ax3.axhline(y=0.05, color='green', linestyle=':', linewidth=2, alpha=0.5, label='Minimal (0.05)')\n",
        "ax3.axhline(y=0.15, color='orange', linestyle=':', linewidth=2, alpha=0.5, label='Moderate (0.15)')\n",
        "ax3.set_xlabel('Epoch', fontweight='bold')\n",
        "ax3.set_ylabel('Overfitting Coefficient', fontweight='bold')\n",
        "ax3.set_title('Overfitting Coefficient Trend', fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Train vs Test Loss with Gap\n",
        "ax4 = fig.add_subplot(gs[1, 0])\n",
        "ax4.plot(epochs, train_losses, marker='o', label='Train Loss', linewidth=2.5, markersize=6)\n",
        "ax4.plot(epochs, test_losses, marker='s', label='Test Loss', linewidth=2.5, markersize=6)\n",
        "ax4.fill_between(epochs, train_losses, test_losses, alpha=0.2, color='red', label='Overfitting Gap')\n",
        "ax4.set_xlabel('Epoch', fontweight='bold')\n",
        "ax4.set_ylabel('Loss', fontweight='bold')\n",
        "ax4.set_title('Train vs Test Loss with Gap', fontweight='bold')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Train vs Test Accuracy\n",
        "ax5 = fig.add_subplot(gs[1, 1])\n",
        "ax5.plot(epochs, train_accs, marker='o', label='Train Accuracy', linewidth=2.5, markersize=6, color='green')\n",
        "ax5.plot(epochs, test_accs, marker='s', label='Test Accuracy', linewidth=2.5, markersize=6, color='blue')\n",
        "ax5.fill_between(epochs, train_accs, test_accs, alpha=0.2, color='red')\n",
        "ax5.axhline(y=0.9, color='green', linestyle=':', alpha=0.5, label='90% Target')\n",
        "ax5.set_xlabel('Epoch', fontweight='bold')\n",
        "ax5.set_ylabel('Accuracy', fontweight='bold')\n",
        "ax5.set_title('Train vs Test Accuracy', fontweight='bold')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Learning Rate Schedule\n",
        "ax6 = fig.add_subplot(gs[1, 2])\n",
        "ax6.plot(epochs, history['learning_rate'], marker='o', linewidth=2.5, markersize=6, color='purple')\n",
        "ax6.set_xlabel('Epoch', fontweight='bold')\n",
        "ax6.set_ylabel('Learning Rate', fontweight='bold')\n",
        "ax6.set_title('Learning Rate Schedule', fontweight='bold')\n",
        "ax6.grid(True, alpha=0.3)\n",
        "ax6.set_yscale('log')\n",
        "\n",
        "# 7. Per-Epoch Metrics Heatmap\n",
        "ax7 = fig.add_subplot(gs[2, :])\n",
        "heatmap_data = np.array([\n",
        "    train_losses / train_losses.max(),\n",
        "    test_losses / test_losses.max(),\n",
        "    train_accs,\n",
        "    test_accs,\n",
        "    history['test_precision'],\n",
        "    history['test_recall'],\n",
        "    history['test_f1']\n",
        "])\n",
        "im = ax7.imshow(heatmap_data, cmap='RdYlGn', aspect='auto')\n",
        "ax7.set_yticks(range(7))\n",
        "ax7.set_yticklabels(['Train Loss (norm)', 'Test Loss (norm)', 'Train Acc', 'Test Acc', 'Precision', 'Recall', 'F1 Score'])\n",
        "ax7.set_xticks(range(NUM_EPOCHS))\n",
        "ax7.set_xticklabels(epochs)\n",
        "ax7.set_xlabel('Epoch', fontweight='bold')\n",
        "ax7.set_title('All Metrics Heatmap (Green=Better, Red=Worse)', fontweight='bold')\n",
        "plt.colorbar(im, ax=ax7, label='Normalized Value')\n",
        "\n",
        "plt.savefig(os.path.join(OUTPUT_PATH, 'overfitting_analysis.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Overfitting analysis visualization saved!\")\n",
        "print(f\"ğŸ“ Saved to: {os.path.join(OUTPUT_PATH, 'overfitting_analysis.png')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_section_kaggle"
      },
      "source": [
        "## 1ï¸âƒ£3ï¸âƒ£ Summary & Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "summary_kaggle",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "final_accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… FINE-TUNING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nğŸ“Š Final Results:\")\n",
        "print(f\"   â€¢ Final Test Accuracy: {final_accuracy*100:.2f}%\")\n",
        "print(f\"   â€¢ Best Accuracy: {best_accuracy*100:.2f}% (Epoch {best_epoch})\")\n",
        "print(f\"   â€¢ Number of Classes: {num_classes}\")\n",
        "print(f\"   â€¢ Training Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"   â€¢ Target Achieved: {'âœ… YES! (>=90%)' if best_accuracy >= 0.9 else 'âš ï¸ CLOSE (Try longer training)' if best_accuracy >= 0.85 else 'âŒ Continue training'}\")\n",
        "\n",
        "print(f\"\\nğŸ“‹ Per-Class Performance:\")\n",
        "print(classification_report(all_labels, all_preds,\n",
        "                          target_names=train_dataset.classes,\n",
        "                          digits=4))\n",
        "\n",
        "print(f\"\\nğŸ“ Output Files (in /kaggle/working/output/):\")\n",
        "output_files = os.listdir(OUTPUT_PATH)\n",
        "for file in sorted(output_files):\n",
        "    file_path = os.path.join(OUTPUT_PATH, file)\n",
        "    file_size = os.path.getsize(file_path) / (1024*1024)\n",
        "    print(f\"   â€¢ {file} ({file_size:.2f} MB)\")\n",
        "\n",
        "print(f\"\\nğŸš€ Next Steps:\")\n",
        "print(f\"   1. âœ… Model is saved in /kaggle/working/output/\")\n",
        "print(f\"   2. ğŸ“¥ Download files via 'Output' tab\")\n",
        "print(f\"   3. ğŸ” Review overfitting_metrics.csv for detailed analysis\")\n",
        "print(f\"   4. ğŸ“Š Check overfitting_analysis.png for visual insights\")\n",
        "print(f\"   5. ğŸ§ª Test on new images\")\n",
        "print(f\"   6. ğŸš€ Deploy to production\")\n",
        "\n",
        "if best_accuracy < 0.90:\n",
        "    print(f\"\\nğŸ’¡ TIPS TO IMPROVE ACCURACY:\")\n",
        "    print(f\"   â€¢ Unfreeze even more vision model layers (current: last 8)\")\n",
        "    print(f\"   â€¢ Try increase epochs from {NUM_EPOCHS}\")\n",
        "    print(f\"   â€¢ Reduce batch size to 16 for more updates\")\n",
        "    print(f\"   â€¢ Use stronger augmentation patterns\")\n",
        "    print(f\"   â€¢ Check for class imbalance in your dataset\")\n",
        "    print(f\"   â€¢ Ensure high-quality training data\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ‰ Thank you for using MedSigLIP Fine-tuning on Kaggle!\")\n",
        "print(\"Version 3.1 - 10-Epoch Advanced Strategy for 0.9+ Accuracy\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "notebookc4efb879ae",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 9371763,
          "sourceId": 14669652,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31260,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
