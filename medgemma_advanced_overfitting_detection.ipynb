{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# MedGemma Fine-Tuning Advanced - With Overfitting Detection\n","\n","Features: Loss graphs | Overfitting detection | Metrics | Loss gap analysis"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["import os, torch, pandas as pd, numpy as np, json\n","IS_KAGGLE = os.path.exists('/kaggle')\n","print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"}')","print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')","print(f'Environment: Kaggle' if IS_KAGGLE else 'print(f\"Local\")')","print(f'PyTorch: {torch.__version__}')","print('âœ… Environment initialized')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["!pip install -q transformers datasets torch bitsandbytes peft trl scikit-learn matplotlib seaborn\n","print('âœ… Packages installed')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["from datasets import Dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","from trl import SFTTrainer, SFTConfig\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","print('âœ… Libraries imported')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["csv_path = '/kaggle/input/nail-disease-classification/nail_diseases.csv'\n","df = pd.read_csv(csv_path)\n","print(f'âœ… Loaded {len(df)} samples')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["def make_prompt(row):\n"," findings = str(row.get('clinical_findings', ''))[:200]\n"," diagnosis = str(row.get('confirmed_diagnosis', ''))\n"," return f'Findings: {findings}. Diagnosis: {diagnosis}'\n","\n","df['text'] = df.apply(make_prompt, axis=1)\n","train_df, temp = train_test_split(df, test_size=0.3, random_state=42)\n","val_df, test_df = train_test_split(temp, test_size=0.5, random_state=42)\n","print(f'Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.bfloat16)\n","model = AutoModelForCausalLM.from_pretrained('google/medgemma-4b', quantization_config=bnb_config, device_map='auto', token=True)\n","tokenizer = AutoTokenizer.from_pretrained('google/medgemma-4b')\n","tokenizer.pad_token = tokenizer.eos_token\n","print('âœ… Model loaded')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["model = prepare_model_for_kbit_training(model)\n","lora = LoraConfig(r=8, lora_alpha=16, target_modules=['q_proj', 'v_proj'], task_type='CAUSAL_LM')\n","model = get_peft_model(model, lora)\n","print(f'âœ… LoRA configured')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["train_ds = Dataset.from_pandas(train_df[['text']])\n","val_ds = Dataset.from_pandas(val_df[['text']])\n","test_ds = Dataset.from_pandas(test_df[['text']])\n","print(f'Datasets ready: {len(train_ds)} {len(val_ds)} {len(test_ds)}')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["config = SFTConfig(output_dir='./medgemma_nails', num_train_epochs=3, per_device_train_batch_size=4, per_device_eval_batch_size=4, gradient_accumulation_steps=2, learning_rate=2e-4, warmup_steps=50, max_seq_length=512, logging_steps=20, eval_steps=50, save_steps=50, evaluation_strategy='steps', load_best_model_at_end=True, metric_for_best_model='eval_loss', logging_dir='./logs')\n","trainer = SFTTrainer(model=model, args=config, train_dataset=train_ds, eval_dataset=val_ds, tokenizer=tokenizer, dataset_text_field='text')\n","print('âœ… Trainer ready')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["print('Starting training...')\n","result = trainer.train()\n","print(f'âœ… Training done: {result.training_loss:.4f}')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["eval_res = trainer.evaluate(test_ds)\n","print(f'Eval Loss: {eval_res.get(\"eval_loss\", 0):.4f}')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["model.save_pretrained('./medgemma_nails')\n","tokenizer.save_pretrained('./medgemma_nails')\n","print('âœ… Model saved')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Extract & Visualize Training Metrics"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["import matplotlib.pyplot as plt\n","history = {'train_loss': [], 'eval_loss': []}\n","try:\n"," from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n"," ea = EventAccumulator('./logs')\n"," ea.Reload()\n"," for tag in ea.Tags()['scalars']:\n"," events = ea.Scalars(tag)\n"," for e in events:\n"," if 'eval' in tag and 'loss' in tag:\n"," history['eval_loss'].append(e.value)\n"," elif 'loss' in tag:\n"," history['train_loss'].append(e.value)\n","except:\n"," print('Could not read tensorboard logs')\n","print(f'Extracted: {len(history[\"train_loss\"])} train, {len(history[\"eval_loss\"])} eval')","#print(history)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n","fig.suptitle('MedGemma Training: Overfitting Detection & Metrics', fontsize=14, fontweight='bold')\n","\n","train = np.array(history['train_loss'])\n","val = np.array(history['eval_loss']) if history['eval_loss'] else train\n","\n","if len(val) > 0:\n"," gap = val - train[-len(val):]\n"," axes[0, 0].plot(train, label='Train Loss', marker='o', markersize=3)\n"," axes[0, 0].set_title('Training Loss Progression')\n"," axes[0, 0].set_xlabel('Step')\n"," axes[0, 0].set_ylabel('Loss')\n"," axes[0, 0].legend()\n"," axes[0, 0].grid(True, alpha=0.3)\n","\n"," axes[0, 1].plot(val, label='Eval Loss', marker='s', markersize=3, color='orange')\n"," axes[0, 1].set_title('Validation Loss')\n"," axes[0, 1].set_xlabel('Eval Step')\n"," axes[0, 1].set_ylabel('Loss')\n"," axes[0, 1].legend()\n"," axes[0, 1].grid(True, alpha=0.3)\n","\n"," train_aligned = train[-len(val):]\n"," axes[1, 0].plot(train_aligned, marker='o', label='Train', linewidth=2)\n"," axes[1, 0].plot(val, marker='s', label='Eval', linewidth=2)\n"," axes[1, 0].fill_between(range(len(val)), train_aligned, val, alpha=0.2, color='red')\n"," axes[1, 0].set_title('Loss Gap (Train vs Eval)')\n"," axes[1, 0].set_xlabel('Step')\n"," axes[1, 0].set_ylabel('Loss')\n"," axes[1, 0].legend()\n"," axes[1, 0].grid(True, alpha=0.3)\n","\n"," gap_avg = np.mean(gap)\n"," gap_max = np.max(gap)\n"," if gap_avg < 0.01:\n"," status = 'MINIMAL OVERFITTING'\n"," elif gap_avg < 0.05:\n"," status = 'MILD OVERFITTING'\n"," else:\n"," status = 'MODERATE-SEVERE'\n","\n"," text = f'OVERFITTING ANALYSIS\\n\\nAvg Gap: {gap_avg:.6f}\\nMax Gap: {gap_max:.6f}\\n\\nStatus: {status}\\n\\nTrain Loss: {train_aligned[-1]:.6f}\\nEval Loss: {val[-1]:.6f}\\n\\nImprovement: {(1-val[-1]/val[0])*100:.1f}%'\n"," axes[1, 1].text(0.5, 0.5, text, ha='center', va='center', fontsize=11, family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n"," axes[1, 1].axis('off')\n","\n","plt.tight_layout()\n","plt.savefig('overfitting_analysis.png', dpi=150, bbox_inches='tight')\n","plt.show()\n","print('âœ… Overfitting analysis saved to overfitting_analysis.png')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["summary = {\n","'train_loss': float(train[-1]) if len(train) > 0 else 0,\n","'eval_loss': float(val[-1]) if len(val) > 0 else 0,\n","'avg_loss_gap': float(np.mean(gap)) if len(val) > 0 else 0,\n","'max_loss_gap': float(np.max(gap)) if len(val) > 0 else 0,\n","'train_samples': len(train_df),\n","'overfitting_status': status if len(val) > 0 else 'unknown'\n","}\n","\n","with open('training_summary.json', 'w') as f:\n"," json.dump(summary, f, indent=2)\n","\n","print('Training Summary:')\n","print(json.dumps(summary, indent=2))\n","print('âœ… Summary saved to training_summary.json')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["print('='*60)\n","print('âœ… TRAINING COMPLETE!')\n","print('='*60)\n","print('\\nOutput Files:')\n","print('  - medgemma_nails/ (fine-tuned model)')\n","print('  - overfitting_analysis.png (loss curves & overfitting detection)')\n","print('  - training_summary.json (metrics)') \n","print('  - logs/ (tensorboard data)')\n","print('\\nðŸš€ Download from Output tab on Kaggle')","print('='*60)"]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}