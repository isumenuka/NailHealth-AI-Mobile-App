{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGemma Fine-Tuning for Nail Disease Classification\n",
    "## Kaggle Optimized Version\n",
    "This notebook fine-tunes Google's MedGemma model on nail disease classification.\n",
    "- Better GPU: P100 (40GB) is 2-3x faster than Colab T4\n",
    "- Free Training with no runtime limits\n",
    "- Expected Time: 30 mins - 1 hour on P100 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 1: Detect Environment & GPU"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "IS_KAGGLE = os.path.exists('/kaggle')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print('Running on Kaggle')\n",
    "    ENVIRONMENT = 'kaggle'\n",
    "elif IS_COLAB:\n",
    "    print('Running on Google Colab')\n",
    "    ENVIRONMENT = 'colab'\n",
    "else:\n",
    "    print('Running on Local Machine')\n",
    "    ENVIRONMENT = 'local'\n",
    "\n",
    "print(f'GPU Available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 2: Install Dependencies"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch bitsandbytes peft trl tensorboard scikit-learn pandas numpy\n",
    "print('Dependencies installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 3: Import Libraries"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'Transformers: {transformers.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 4: Configuration"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'model_name': 'google/medgemma-4b',\n",
    "    'batch_size': 4,\n",
    "    'learning_rate': 2e-4,\n",
    "    'num_epochs': 3,\n",
    "    'max_seq_length': 512,\n",
    "    'lora_r': 8,\n",
    "    'lora_alpha': 16,\n",
    "    'output_dir': './medgemma_nails_finetuned',\n",
    "}\n",
    "\n",
    "for key, value in CONFIG.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 5: Load CSV Data"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENVIRONMENT == 'kaggle':\n",
    "    csv_path = '/kaggle/input/nail-disease-classification/nail_diseases.csv'\n",
    "elif ENVIRONMENT == 'colab':\n",
    "    csv_path = '/content/drive/MyDrive/nail_diseases.csv'\n",
    "else:\n",
    "    csv_path = './nail_diseases.csv'\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f'Loaded {len(df)} rows')\n",
    "print(f'Shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 6: Create Training Prompts"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(row):\n",
    "    findings = str(row.get('clinical_findings', 'N/A'))\n",
    "    diagnosis = str(row.get('confirmed_diagnosis', 'N/A'))\n",
    "    treatment = str(row.get('treatment_protocol', 'N/A'))\n",
    "    prognosis = str(row.get('prognosis', 'N/A'))\n",
    "    text = f'Clinical Findings: {findings}\\n'\n",
    "    text += f'Diagnosis: {diagnosis}\\n'\n",
    "    text += f'Treatment: {treatment}\\n'\n",
    "    text += f'Prognosis: {prognosis}'\n",
    "    return text\n",
    "\n",
    "df['text'] = df.apply(create_prompt, axis=1)\n",
    "print(f'Created {len(df)} training samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 7: Split Data"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Train: {len(train_df)} samples')\n",
    "print(f'Val: {len(val_df)} samples')\n",
    "print(f'Test: {len(test_df)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 8: Setup 4-bit Quantization"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "print('4-bit quantization configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 9: Load MedGemma Model"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loading {CONFIG[\"model_name\"]}...')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    token=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 10: Setup LoRA"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG['lora_r'],\n",
    "    lora_alpha=CONFIG['lora_alpha'],\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'Trainable: {trainable:,} / {total:,}')\n",
    "print(f'Trainable %: {100 * trainable / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 11: Create Datasets"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df[['text']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text']])\n",
    "\n",
    "print(f'Train: {len(train_dataset)} samples')\n",
    "print(f'Val: {len(val_dataset)} samples')\n",
    "print(f'Test: {len(test_dataset)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 12: Configure Training"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = SFTConfig(\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    num_train_epochs=CONFIG['num_epochs'],\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    warmup_steps=100,\n",
    "    max_seq_length=CONFIG['max_seq_length'],\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    report_to=['tensorboard'],\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "print('Training config ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 13: Initialize Trainer"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field='text'\n",
    ")\n",
    "print('Trainer initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 14: START TRAINING"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting training...')\n",
    "train_result = trainer.train()\n",
    "print(f'Training loss: {train_result.training_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 15: Evaluate"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.evaluate(test_dataset)\n",
    "print('Test Results:')\n",
    "print(json.dumps(test_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 16: Save Model"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(CONFIG['output_dir'])\n",
    "tokenizer.save_pretrained(CONFIG['output_dir'])\n",
    "print(f'Model saved to {CONFIG[\"output_dir\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 17: Test Inference"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = 'Clinical Findings: White nails with pink distal end. Diagnosis: '\n",
    "inputs = tokenizer(test_input, return_tensors='pt')\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print('Generated:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 18: Save Summary"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'model': CONFIG['model_name'],\n",
    "    'train_samples': len(train_df),\n",
    "    'val_samples': len(val_df),\n",
    "    'test_samples': len(test_df),\n",
    "    'epochs': CONFIG['num_epochs'],\n",
    "    'batch_size': CONFIG['batch_size'],\n",
    "    'final_loss': float(train_result.training_loss)\n",
    "}\n",
    "\n",
    "with open('training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print('Summary saved')\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 19: Complete"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAINING COMPLETE!')\n",
    "print('Files saved:')\n",
    "print(f'  Model: {CONFIG[\"output_dir\"]}/')\n",
    "print('  Logs: ./logs/')\n",
    "print('  Summary: ./training_summary.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}