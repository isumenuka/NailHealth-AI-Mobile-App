{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"colab":{"name":"medgemma_kaggle_finetuning","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14683799,"sourceType":"datasetVersion","datasetId":9380517}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tune MedGemma 4B for Nail Disease Clinical Explanations\n## Model 2: Clinical Findings ‚Üí Medical Explanations Pipeline\n\n**Pipeline Architecture:**\n- Stage 1 ‚úÖ DONE: MedSigLIP (Image Classification) ‚Üí \"Clubbing\" / \"Pitting\" etc.\n- Stage 2 ‚≠ê NOW: MedGemma 4B (Clinical Explanation) ‚Üí \"What does this mean?\"\n- Stage 3: MedGemma 27B (Disease Ranking) ‚Üí \"What diseases could cause this?\"\n\nBased on: https://github.com/google-health/medgemma\nModel: google/medgemma-4b-it (Lightweight, 4B params, instruction-tuned)\nLicense: Apache 2.0\n\nFeatures: Interactive HuggingFace auth | Medical prompt engineering | CSV integration | Missing value handling | Overfitting detection | 50% faster training","metadata":{"id":"v8Yyi_TgzCLL"}},{"cell_type":"markdown","source":"## Step 0: Suppress CUDA/cuDNN Warnings (Optional)","metadata":{"id":"_vVlUkIPzCLO"}},{"cell_type":"code","source":"# Suppress CUDA/cuDNN duplicate factory registration warnings\nimport os\nimport warnings\n\n# Suppress TensorFlow/XLA warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ['XLA_FLAGS'] = '--xla_gpu_deterministic_ops'\n\n# Suppress pydantic warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nimport logging\nlogging.getLogger('absl').setLevel(logging.ERROR)\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\nlogging.getLogger('transformers').setLevel(logging.WARNING)\n\nprint('‚úÖ Warning filters applied - CUDA/cuDNN messages suppressed')","metadata":{"id":"I9bUeTmYzCLO","outputId":"ecea9978-838f-4824-da30-83d91c895b6c","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:11:36.457153Z","iopub.execute_input":"2026-01-31T04:11:36.457468Z","iopub.status.idle":"2026-01-31T04:11:36.466482Z","shell.execute_reply.started":"2026-01-31T04:11:36.457442Z","shell.execute_reply":"2026-01-31T04:11:36.465780Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Warning filters applied - CUDA/cuDNN messages suppressed\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Step 1: Setup Environment & Interactive HuggingFace Authentication","metadata":{"id":"GwxdXtgrzCLQ"}},{"cell_type":"code","source":"import os\nimport torch\nimport json\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nIS_KAGGLE = os.path.exists('/kaggle')\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint('='*60)\nprint('ENVIRONMENT SETUP')\nprint('='*60)\nprint(f'Environment: {\"Kaggle\" if IS_KAGGLE else \"Local/Colab\"}')\nprint(f'Device: {DEVICE}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\nelse:\n    print('GPU: None - CPU mode')\nprint(f'PyTorch: {torch.__version__}')\nprint('='*60)","metadata":{"id":"9ax_FIFbzCLQ","outputId":"6fd0dc08-0aae-44d3-8790-5ae656e3428a","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:11:36.467961Z","iopub.execute_input":"2026-01-31T04:11:36.468246Z","iopub.status.idle":"2026-01-31T04:11:44.352678Z","shell.execute_reply.started":"2026-01-31T04:11:36.468216Z","shell.execute_reply":"2026-01-31T04:11:44.352022Z"}},"outputs":[{"name":"stdout","text":"============================================================\nENVIRONMENT SETUP\n============================================================\nEnvironment: Kaggle\nDevice: cuda\nGPU: Tesla T4\nMemory: 15.6 GB\nPyTorch: 2.8.0+cu126\n============================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install packages\n!pip install -q transformers datasets torch bitsandbytes peft trl scikit-learn matplotlib huggingface-hub\nprint('‚úÖ Packages installed')","metadata":{"id":"OzTMGLF6zCLR","outputId":"13fcb7e3-391b-42af-c1f1-61940fe72fd6","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:11:44.353606Z","iopub.execute_input":"2026-01-31T04:11:44.354011Z","iopub.status.idle":"2026-01-31T04:11:53.286818Z","shell.execute_reply.started":"2026-01-31T04:11:44.353966Z","shell.execute_reply":"2026-01-31T04:11:53.285982Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h‚úÖ Packages installed\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Step 1a: üîë Interactive HuggingFace Authentication Helper","metadata":{"id":"XsQOf2NAzCLS"}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nprint(\"=\"*70)\nprint(\"üîê HUGGING FACE LOGIN\")\nprint(\"=\"*70)\nprint(\"\\nYou'll be prompted to enter your Hugging Face token.\")\nprint(\"Get your token: https://huggingface.co/settings/tokens\\n\")\n\nnotebook_login()\n\nprint(\"\\n‚úÖ Login successful!\")","metadata":{"colab":{"referenced_widgets":["f8bf654a82af474594c5f2730d59bb8f"]},"id":"9zB8aINSzCLS","outputId":"1f18f28a-558a-4801-a699-31de9c2a0cde","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:11:53.288132Z","iopub.execute_input":"2026-01-31T04:11:53.288377Z","iopub.status.idle":"2026-01-31T04:11:53.709751Z","shell.execute_reply.started":"2026-01-31T04:11:53.288352Z","shell.execute_reply":"2026-01-31T04:11:53.709040Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüîê HUGGING FACE LOGIN\n======================================================================\n\nYou'll be prompted to enter your Hugging Face token.\nGet your token: https://huggingface.co/settings/tokens\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1203c38fa30b48399315c3bf8b3a28fa"}},"metadata":{}},{"name":"stdout","text":"\n‚úÖ Login successful!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Step 2: Import Libraries","metadata":{"id":"aTdD5UXuzCLT"}},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    set_seed\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\nset_seed(42)\nprint('‚úÖ Libraries imported')","metadata":{"id":"d_Fs8ayEzCLT","outputId":"8731be9d-2fb5-4d9e-ab55-911effd9254a","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:11:53.712022Z","iopub.execute_input":"2026-01-31T04:11:53.712314Z","iopub.status.idle":"2026-01-31T04:12:36.412312Z","shell.execute_reply.started":"2026-01-31T04:11:53.712284Z","shell.execute_reply":"2026-01-31T04:12:36.411707Z"}},"outputs":[{"name":"stderr","text":"2026-01-31 04:12:11.191777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769832731.603133      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769832731.731755      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769832732.779098      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769832732.779142      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769832732.779145      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769832732.779148      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Libraries imported\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Step 3: Load & Explore Dataset (Model 2 Training Data)","metadata":{"id":"C-H2EXxpzCLU"}},{"cell_type":"code","source":"# ============================================================\n# STEP 2: Load CSV Dataset from Kaggle Input\n# ============================================================\n\nimport os\nimport sys\nimport pandas as pd\nfrom pathlib import Path\n\nprint('='*60)\nprint('LOADING DATASET')\nprint('='*60)\n\n# ‚úÖ FIXED: Your dataset location\nKAGGLE_DATASET_PATH = '/kaggle/input/nail-diseases-dataset-medgemma'\n\n# Try to find CSV file\ncsv_files = list(Path(KAGGLE_DATASET_PATH).glob('*.csv'))\n\nif not csv_files:\n    print(f'‚ùå No CSV files found in {KAGGLE_DATASET_PATH}')\n    print('\\nAvailable files:')\n    for f in Path(KAGGLE_DATASET_PATH).iterdir():\n        print(f'  - {f.name}')\n    sys.exit(1)\n\n# Load the CSV\ncsv_file = csv_files[0]\nprint(f'‚úÖ Found CSV: {csv_file.name}')\nprint(f'üìÅ Loading from: {csv_file}')\n\ndf = pd.read_csv(csv_file)\n\nprint(f'\\nüìä Dataset loaded successfully!')\nprint(f'Rows: {len(df):,}')\nprint(f'Columns: {len(df.columns)}')\nprint(f'\\nColumns: {list(df.columns)}')\n\n# Show first few rows\nprint(f'\\nüîç First 3 rows:')\nprint(df.head(3))\n\nprint('='*60)","metadata":{"id":"DjFe5X9DzCLV","outputId":"0acd30b1-7d26-40be-82d0-90e77c449603","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:12:36.413117Z","iopub.execute_input":"2026-01-31T04:12:36.413602Z","iopub.status.idle":"2026-01-31T04:12:36.550148Z","shell.execute_reply.started":"2026-01-31T04:12:36.413579Z","shell.execute_reply":"2026-01-31T04:12:36.549372Z"}},"outputs":[{"name":"stdout","text":"============================================================\nLOADING DATASET\n============================================================\n‚úÖ Found CSV: nail_diseases.csv\nüìÅ Loading from: /kaggle/input/nail-diseases-dataset-medgemma/nail_diseases.csv\n\nüìä Dataset loaded successfully!\nRows: 10,000\nColumns: 15\n\nColumns: ['nail_disease_category', 'model_1_predicted_disease', 'confirmed_diagnosis', 'patient_age', 'patient_sex', 'patient_ethnicity', 'fitzpatrick_skin_type', 'disease_severity', 'clinical_findings', 'differential_diagnoses', 'recommended_medical_tests', 'treatment_protocol', 'comorbidities', 'clinical_notes', 'prognosis']\n\nüîç First 3 rows:\n  nail_disease_category model_1_predicted_disease confirmed_diagnosis  \\\n0           Blue_Finger               Blue Finger         Blue Finger   \n1          Healthy_Nail              Healthy Nail        Healthy Nail   \n2             Psoriasis                 Psoriasis           Psoriasis   \n\n   patient_age patient_sex patient_ethnicity  fitzpatrick_skin_type  \\\n0           43        Male         Caucasian                      1   \n1           29        Male             Asian                      4   \n2           73      Female   African Descent                      5   \n\n  disease_severity                                  clinical_findings  \\\n0           Severe  Transient blue discoloration; Blue-gray pigmen...   \n1             Mild                      Intact cuticle; Pink nail bed   \n2             Mild   Thickened crumbling nails; Nail bed inflammation   \n\n                     differential_diagnoses  \\\n0  Acral Lentiginous Melanoma, Healthy Nail   \n1                      Pitting, Blue Finger   \n2                  Pitting, Onychogryphosis   \n\n                           recommended_medical_tests  \\\n0   Pulse Oximetry, Arterial Blood Gas, Cardiac Echo   \n1                    Routine Physical, None required   \n2  Nail Clipping Biopsy, KOH Prep, Rheumatoid Factor   \n\n                                  treatment_protocol        comorbidities  \\\n0    Oxygen therapy, Treat underlying cause, Warming                  NaN   \n1                    None, Routine care, Moisturizer                  NaN   \n2  Intralesional steroids, Biologics, Topical ret...  Psoriatic Arthritis   \n\n                                clinical_notes           prognosis  \n0       Pain upon pressure. Skin Type 1 noted.  Dependent on cause  \n1         No pain reported. Skin Type 4 noted.                Good  \n2  Family history positive. Skin Type 5 noted.          Manageable  \n============================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Step 4: Data Cleaning - Handle Missing Values","metadata":{"id":"D8YidBqvzCLV"}},{"cell_type":"code","source":"print('üßπ DATA CLEANING - MISSING VALUE HANDLING')\nprint('='*60)\n\n# Show missing values before cleaning\nmissing_before = df.isnull().sum()\nprint('\\nüìä Missing values BEFORE cleaning:')\nprint(missing_before[missing_before > 0])\n\n# Create a copy for cleaning\ndf_clean = df.copy()\n\n# Strategy 1: Drop rows where critical columns are missing\ncritical_cols = ['nail_disease', 'disease_name', 'clinical_findings', 'findings']\ncritical_cols_present = [col for col in critical_cols if col in df_clean.columns]\n\nif critical_cols_present:\n    initial_len = len(df_clean)\n    df_clean = df_clean.dropna(subset=critical_cols_present)\n    dropped_critical = initial_len - len(df_clean)\n    print(f'\\nüóëÔ∏è  Dropped {dropped_critical} rows with missing critical fields')\n\n# Strategy 2: Fill missing values in non-critical columns\nnon_critical_cols = ['comorbidities', 'patient_age', 'age', 'patient_sex', 'sex', 'gender']\nfor col in df_clean.columns:\n    if col in non_critical_cols and df_clean[col].isnull().sum() > 0:\n        if df_clean[col].dtype in ['float64', 'int64']:\n            # Fill numeric columns with median\n            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n            print(f'  üìå Filled {col} with median value')\n        else:\n            # Fill string columns with unknown\n            df_clean[col].fillna('unknown', inplace=True)\n            print(f'  üìå Filled {col} with unknown')\n\n# Strategy 3: Drop rows with ANY remaining NaN\ninitial_len = len(df_clean)\ndf_clean = df_clean.dropna()\ndropped_final = initial_len - len(df_clean)\nif dropped_final > 0:\n    print(f'\\nüóëÔ∏è  Dropped {dropped_final} additional rows with remaining NaN values')\n\n# Show missing values after cleaning\nmissing_after = df_clean.isnull().sum()\nif missing_after.sum() == 0:\n    print('\\n‚úÖ No missing values remaining!')\nelse:\n    print('\\n‚ö†Ô∏è  Remaining missing values:')\n    print(missing_after[missing_after > 0])\n\nprint(f'\\nüìä Dataset Summary:')\nprint(f'  Original rows: {len(df)}')\nprint(f'  Clean rows: {len(df_clean)}')\nprint(f'  Removed: {len(df) - len(df_clean)} ({(len(df) - len(df_clean))/len(df)*100:.1f}%)')\nprint('='*60)\n\n# Use cleaned dataset\ndf = df_clean","metadata":{"id":"wdQXi40wzCLW","outputId":"ff5f59f6-a49f-4cac-e164-473956576950","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:13:29.937072Z","iopub.execute_input":"2026-01-31T04:13:29.937849Z","iopub.status.idle":"2026-01-31T04:13:29.984425Z","shell.execute_reply.started":"2026-01-31T04:13:29.937805Z","shell.execute_reply":"2026-01-31T04:13:29.983580Z"}},"outputs":[{"name":"stdout","text":"üßπ DATA CLEANING - MISSING VALUE HANDLING\n============================================================\n\nüìä Missing values BEFORE cleaning:\nSeries([], dtype: int64)\n\nüóëÔ∏è  Dropped 0 rows with missing critical fields\n\n‚úÖ No missing values remaining!\n\nüìä Dataset Summary:\n  Original rows: 10000\n  Clean rows: 10000\n  Removed: 0 (0.0%)\n============================================================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Step 5: Enhanced Medical Prompt Templates for Clinical Explanations","metadata":{"id":"jZGK7PZ9zCLW"}},{"cell_type":"code","source":"def create_medical_prompt_model2(row):\n    \"\"\"\n    Creates advanced medical prompts for MedGemma Model 2 (Clinical Explanation Stage).\n\n    Input: Nail disease classification from Model 1 + clinical findings\n    Output: Detailed clinical explanation of findings, differential diagnoses, and systemic implications\n    \"\"\"\n\n    # Extract fields (handle missing/None values)\n    nail_disease = str(row.get('nail_disease', row.get('disease_name', 'unknown'))).strip()\n    clinical_findings = str(row.get('clinical_findings', row.get('findings', 'no findings reported'))).strip()\n    patient_age = row.get('patient_age', row.get('age', 'unknown'))\n    patient_sex = str(row.get('patient_sex', row.get('sex', row.get('gender', 'unknown')))).strip()\n    differential_diagnoses = str(row.get('differential_diagnoses', row.get('differentials', 'pending investigation'))).strip()\n    systemic_implications = str(row.get('systemic_implications', row.get('implications', 'requires clinical assessment'))).strip()\n    treatment_protocol = str(row.get('treatment_protocol', row.get('treatment', 'refer to specialist'))).strip()\n    comorbidities = str(row.get('comorbidities', 'none reported')).strip()\n\n    # Build instruction-following prompt (Orca format)\n    prompt = f\"\"\"CLINICAL ANALYSIS: Nail Disease Diagnosis\n\nPATIENT DEMOGRAPHICS:\nAge: {patient_age}\nSex: {patient_sex}\nComorbidities: {comorbidities}\n\nPRIMARY FINDING (from Model 1 - MedSigLIP):\n{nail_disease}\n\nCLINICAL PRESENTATION:\n{clinical_findings}\n\nINSTRUCTION:\nBased on the nail disease finding and clinical presentation above, provide:\n1. Detailed explanation of what the nail finding indicates\n2. Possible systemic diseases that could cause this nail finding\n3. Recommended diagnostic workup and treatment approach\n\nEXPECTED RESPONSE:\nNail Finding Explanation: {nail_disease} indicates {systemic_implications}\n\nDifferential Diagnoses: {differential_diagnoses}\n\nRecommended Treatment: {treatment_protocol}\n\"\"\"\n\n    return prompt.strip()\n\n# Apply prompt template to dataset\ndf['text'] = df.apply(create_medical_prompt_model2, axis=1)\n\nprint(f'‚úÖ Created {len(df)} medical prompts for Model 2 training')\nprint(f'\\nExample prompt (first 500 chars):')\nprint('='*60)\nprint(df['text'].iloc[0][:500])\nprint('='*60)\nprint(f'\\nAverage prompt length: {df[\"text\"].str.len().mean():.0f} chars')\nprint(f'Max prompt length: {df[\"text\"].str.len().max():.0f} chars')","metadata":{"id":"94u8WDKqzCLW","outputId":"84a924b1-5e72-45a3-bbac-47dd34b6263e","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:13:33.363428Z","iopub.execute_input":"2026-01-31T04:13:33.364218Z","iopub.status.idle":"2026-01-31T04:13:33.979749Z","shell.execute_reply.started":"2026-01-31T04:13:33.364191Z","shell.execute_reply":"2026-01-31T04:13:33.979132Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Created 10000 medical prompts for Model 2 training\n\nExample prompt (first 500 chars):\n============================================================\nCLINICAL ANALYSIS: Nail Disease Diagnosis\n\nPATIENT DEMOGRAPHICS:\nAge: 43\nSex: Male\nComorbidities: unknown\n\nPRIMARY FINDING (from Model 1 - MedSigLIP):\nunknown\n\nCLINICAL PRESENTATION:\nTransient blue discoloration; Blue-gray pigmentation\n\nINSTRUCTION:\nBased on the nail disease finding and clinical presentation above, provide:\n1. Detailed explanation of what the nail finding indicates\n2. Possible systemic diseases that could cause this nail finding\n3. Recommended diagnostic workup and treatment app\n============================================================\n\nAverage prompt length: 727 chars\nMax prompt length: 774 chars\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Step 6: Data Quality & Validation Check","metadata":{"id":"AdDtUZvyzCLX"}},{"cell_type":"code","source":"# Check for missing values and data quality\nprint('üìä DATA QUALITY REPORT')\nprint('='*60)\n\n# Missing values\nmissing = df.isnull().sum()\nif missing.sum() > 0:\n    print('\\n‚ö†Ô∏è  Missing values detected:')\n    print(missing[missing > 0])\nelse:\n    print('\\n‚úÖ No missing values')\n\n# Check text field quality\nempty_texts = (df['text'].str.len() < 50).sum()\nif empty_texts > 0:\n    print(f'\\n‚ö†Ô∏è  {empty_texts} prompts are too short (<50 chars)')\nelse:\n    print(f'\\n‚úÖ All prompts have sufficient length ({len(df)} samples)')\n\n# Disease distribution\nif 'nail_disease' in df.columns:\n    print(f'\\nüìã Disease Distribution:')\n    print(df['nail_disease'].value_counts())\nelif 'disease_name' in df.columns:\n    print(f'\\nüìã Disease Distribution:')\n    print(df['disease_name'].value_counts())\n\nprint('\\n' + '='*60)","metadata":{"id":"vp2sXTOrzCLX","outputId":"b905fb91-e06a-4e5e-d69c-7abd608967fe","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:13:38.698447Z","iopub.execute_input":"2026-01-31T04:13:38.699226Z","iopub.status.idle":"2026-01-31T04:13:38.717746Z","shell.execute_reply.started":"2026-01-31T04:13:38.699196Z","shell.execute_reply":"2026-01-31T04:13:38.716954Z"}},"outputs":[{"name":"stdout","text":"üìä DATA QUALITY REPORT\n============================================================\n\n‚úÖ No missing values\n\n‚úÖ All prompts have sufficient length (10000 samples)\n\n============================================================\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Step 7: Split Dataset (Train/Val/Test)","metadata":{"id":"fEtWL3pUzCLY"}},{"cell_type":"code","source":"# Stratified split: 70% train, 15% val, 15% test\nsplit_key = None\nif 'nail_disease' in df.columns:\n    split_key = 'nail_disease'\nelif 'disease_name' in df.columns:\n    split_key = 'disease_name'\n\ntrain_df, temp_df = train_test_split(\n    df,\n    test_size=0.3,\n    random_state=42,\n    stratify=df[split_key] if split_key else None\n)\n\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=0.5,\n    random_state=42,\n    stratify=temp_df[split_key] if split_key else None\n)\n\nprint('üìä DATASET SPLIT')\nprint('='*60)\nprint(f'Train: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)')\nprint(f'Val:   {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)')\nprint(f'Test:  {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)')\nprint(f'Total: {len(df)} samples')\nprint('='*60)","metadata":{"id":"0OvOOflFzCLY","outputId":"382dfaa5-2dcf-4011-aca0-8b69e9314810","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:13:41.316422Z","iopub.execute_input":"2026-01-31T04:13:41.316833Z","iopub.status.idle":"2026-01-31T04:13:41.331892Z","shell.execute_reply.started":"2026-01-31T04:13:41.316807Z","shell.execute_reply":"2026-01-31T04:13:41.331153Z"}},"outputs":[{"name":"stdout","text":"üìä DATASET SPLIT\n============================================================\nTrain: 7000 samples (70.0%)\nVal:   1500 samples (15.0%)\nTest:  1500 samples (15.0%)\nTotal: 10000 samples\n============================================================\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Step 8: Create HuggingFace Datasets","metadata":{"id":"7sQTfffMzCLY"}},{"cell_type":"code","source":"# Create HuggingFace datasets\ntrain_dataset = Dataset.from_pandas(train_df[['text']])\nval_dataset = Dataset.from_pandas(val_df[['text']])\ntest_dataset = Dataset.from_pandas(test_df[['text']])\n\nprint('‚úÖ HuggingFace datasets created')\nprint(f'  Train: {len(train_dataset)} samples')\nprint(f'  Val:   {len(val_dataset)} samples')\nprint(f'  Test:  {len(test_dataset)} samples')","metadata":{"id":"il3Wf9T-zCLZ","outputId":"45b93755-0c5d-4328-aeb1-9a52fc9ea30d","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:13:43.835288Z","iopub.execute_input":"2026-01-31T04:13:43.835584Z","iopub.status.idle":"2026-01-31T04:13:43.914858Z","shell.execute_reply.started":"2026-01-31T04:13:43.835560Z","shell.execute_reply":"2026-01-31T04:13:43.914079Z"}},"outputs":[{"name":"stdout","text":"‚úÖ HuggingFace datasets created\n  Train: 7000 samples\n  Val:   1500 samples\n  Test:  1500 samples\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Step 9: Setup Model & Tokenizer (MedGemma 4B - Lightweight & Fast)","metadata":{"id":"zMWaVY2mzCLZ"}},{"cell_type":"code","source":"# Model configuration: Using MedGemma 4B for FASTER TRAINING\n# 4B is 50% faster than 7B with similar medical understanding\nMODEL_ID = 'google/medgemma-4b-it'\n# Alternative (larger, slower): MODEL_ID = 'google/medgemma-7b-orcamath-it'\n\n# 4-bit quantization config (memory efficient)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint('='*60)\nprint(f'Loading model: {MODEL_ID}')\nprint('This may take 1-2 minutes (4B is faster than 7B)...')\nprint('='*60)\nprint('üí° Model Info:')\nprint('  - Size: 4B parameters (50% smaller than 7B)')\nprint('  - Speed: ~2x faster training')\nprint('  - Quality: Excellent medical understanding')\nprint('  - Memory: Fits in most GPUs (8GB+)')\nprint()\n\nmodel = None\ntokenizer = None\n\ntry:\n    # Try with HuggingFace token authentication\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID,\n        quantization_config=bnb_config,\n        device_map='auto',\n        trust_remote_code=True,\n        use_auth_token=True,  # Enable auth token\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_ID,\n        trust_remote_code=True,\n        use_auth_token=True\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n\n    print(f'‚úÖ Model loaded successfully!')\n    print(f'   Model: MedGemma 4B (Lightweight)')\n    print(f'   Size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters')\n    print(f'   Memory: ~8GB GPU VRAM (4-bit quantized)')\n    print(f'   Expected Training Time: 15-30 minutes')\n\nexcept Exception as e:\n    error_msg = str(e)\n    print(f'\\n‚ùå Error loading model: {error_msg[:200]}')\n    print('\\nüîß TROUBLESHOOTING:')\n    print('\\n1. ACCEPT MODEL LICENSE:')\n    print(f'   - Visit: https://huggingface.co/{MODEL_ID}')\n    print('   - Click \"Accept\" button')\n    print('\\n2. LOGIN TO HUGGINGFACE:')\n    print('   - Run setup_huggingface_auth() in Step 1a again')\n    print('   - Or manually: from huggingface_hub import login')\n    print('   - Then: login(token=\"hf_YOUR_TOKEN\")')\n    print('\\n3. CHECK TOKEN VALIDITY:')\n    print('   - Visit: https://huggingface.co/settings/tokens')\n    print('   - Ensure your token has \"Read\" access')\n    print('\\n4. ENVIRONMENT VARIABLES:')\n    print('   - Set: export HF_TOKEN=\"hf_YOUR_TOKEN\"')\n    print('\\n5. OFFLINE MODE:')\n    print('   - Download model locally first')\n    print('   - Use: AutoModel.from_pretrained(\"./local/path\")')\n    print('\\n' + '='*60)\n    sys.exit(1)\n\nif model is None or tokenizer is None:\n    print('‚ùå Model or tokenizer failed to load!')\n    sys.exit(1)","metadata":{"colab":{"referenced_widgets":["7c2c1ff0f5ff4077bd82a1149ce1a3b0","f016eae2e15f4667b3e2876367e9b2e4","3141f17d9c044fe5a004e5eace3a2ab2","04ca908e0b5248288e73c40664a14fd3","0da07aa6e654465badfafb14b91e9465","dec7886526384ff9af5e77777d0ff196","61fe0497973c4dbb960d4ca15359f0af","1b608c77d3bb42929fa6b46fb6b6bf26","cc3a16f6608249e2b4c28c6c20f05f2c","dc39178a489946ac97c1d68c54ecb029","14ebaaa37a644add921575e7f694c16a","23f1c59131b446a1835429fcf3371195","579ccae1051546eeae1272764bdf56dd"]},"id":"vWgSLsQvzCLZ","outputId":"673bd355-14e8-46ab-e4c2-175278b503d0","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:13:46.566818Z","iopub.execute_input":"2026-01-31T04:13:46.567382Z","iopub.status.idle":"2026-01-31T04:15:18.191637Z","shell.execute_reply.started":"2026-01-31T04:13:46.567357Z","shell.execute_reply":"2026-01-31T04:15:18.190950Z"}},"outputs":[{"name":"stdout","text":"============================================================\nLoading model: google/medgemma-4b-it\nThis may take 1-2 minutes (4B is faster than 7B)...\n============================================================\nüí° Model Info:\n  - Size: 4B parameters (50% smaller than 7B)\n  - Speed: ~2x faster training\n  - Quality: Excellent medical understanding\n  - Memory: Fits in most GPUs (8GB+)\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f24c5467ae5471c9c496351dad86500"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b983c9bb0a904a58948157023ff12495"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"154f35387ce4476a84ab389e88dcd3ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cadc6224d3c04a5f8f40df739b9de262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3463a0a831834e528b79adbe6fcb1be5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46b7798272534279b51ca3a0c150b510"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e2ae413c8304de5a54d8a83a86797f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ceecfead68f45f5917989395d7c9fa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e864d5d57194d49afe1f3f6ee08130b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c783f08f96b84d5fa5b3e79484962dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"083422c754624d9ea92e7e2151147ee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc8689981361424cba394112b130cdb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a67e466a96474e4788b3498089df0dd0"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Model loaded successfully!\n   Model: MedGemma 4B (Lightweight)\n   Size: 2.49B parameters\n   Memory: ~8GB GPU VRAM (4-bit quantized)\n   Expected Training Time: 15-30 minutes\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Step 10: Configure LoRA (Low-Rank Adaptation)","metadata":{"id":"Crg4aTrUzCLZ"}},{"cell_type":"code","source":"# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,  # Rank\n    lora_alpha=32,  # Alpha scaling\n    target_modules=['q_proj', 'v_proj', 'k_proj'],  # Query, Value, Key projections\n    lora_dropout=0.05,\n    bias='none',\n    task_type='CAUSAL_LM'\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Count trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f'‚úÖ LoRA configured for 4B model')\nprint(f'  Total params: {total_params / 1e9:.2f}B')\nprint(f'  Trainable: {trainable_params / 1e6:.2f}M ({100*trainable_params/total_params:.3f}%)')","metadata":{"id":"GDntDsvazCLa","outputId":"e32afdfa-88fa-4c5b-b530-565a421b458e","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:15:38.360958Z","iopub.execute_input":"2026-01-31T04:15:38.361795Z","iopub.status.idle":"2026-01-31T04:15:38.727397Z","shell.execute_reply.started":"2026-01-31T04:15:38.361765Z","shell.execute_reply":"2026-01-31T04:15:38.726724Z"}},"outputs":[{"name":"stdout","text":"‚úÖ LoRA configured for 4B model\n  Total params: 2.50B\n  Trainable: 9.39M (0.376%)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Step 11: Setup Training Configuration (Optimized for 4B) - W&B FIX APPLIED","metadata":{"id":"zB5Z8lSwzCLa"}},{"cell_type":"code","source":"# Training arguments (optimized for MedGemma 4B - FASTER!)\n# üîß FIX: Added report_to='none' to disable W&B and prevent hanging\ntraining_args = TrainingArguments(\n    output_dir='./medgemma_nail_disease_model2_finetuned',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-4,\n    lr_scheduler_type='cosine',\n    warmup_steps=100,\n    weight_decay=0.01,\n    max_steps=500,\n    logging_steps=10,\n    eval_steps=50,\n    save_steps=50,\n    eval_strategy='steps',\n    save_strategy='steps',\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    logging_dir='./logs',\n    optim='paged_adamw_8bit',\n    seed=42,\n    dataloader_pin_memory=True,\n    report_to='none',  # üîß FIX: Disable W&B to prevent hanging on API key prompt\n)\n\nprint('‚úÖ Training configuration ready (4B optimized)')\nprint(f'  Output: ./medgemma_nail_disease_model2_finetuned')\nprint(f'  Epochs: {training_args.num_train_epochs}')\nprint(f'  Batch size: {training_args.per_device_train_batch_size} (increased for 4B)')\nprint(f'  Learning rate: {training_args.learning_rate}')\nprint(f'  Max steps: {training_args.max_steps}')\nprint(f'  Expected time: 15-30 minutes on single GPU')\nprint(f'  üîß W&B disabled (report_to=\"none\") to prevent hanging')","metadata":{"id":"YEG6hotCzCLa","outputId":"493920d0-c678-4dd0-beeb-4166f4c35a65","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:15:40.954907Z","iopub.execute_input":"2026-01-31T04:15:40.955209Z","iopub.status.idle":"2026-01-31T04:15:40.996325Z","shell.execute_reply.started":"2026-01-31T04:15:40.955171Z","shell.execute_reply":"2026-01-31T04:15:40.995491Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Training configuration ready (4B optimized)\n  Output: ./medgemma_nail_disease_model2_finetuned\n  Epochs: 3\n  Batch size: 8 (increased for 4B)\n  Learning rate: 0.0002\n  Max steps: 500\n  Expected time: 15-30 minutes on single GPU\n  üîß W&B disabled (report_to=\"none\") to prevent hanging\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Step 12: Initialize SFT Trainer","metadata":{"id":"STXxXY8azCLa"}},{"cell_type":"code","source":"from transformers import Trainer, DataCollatorForLanguageModeling\n\nprint('üîß TOKENIZING DATASET...')\nprint('This may take 1-2 minutes...')\nprint('='*60)\n\n# üîß FIX: Tokenize the dataset\ndef tokenize_function(examples):\n    \"\"\"Tokenize text samples\"\"\"\n    return tokenizer(\n        examples['text'],\n        max_length=512,\n        truncation=True,\n        padding='max_length',\n        return_tensors=None,  # Don't return tensors yet\n    )\n\n# Apply tokenization\nprint('Tokenizing training dataset...')\ntrain_dataset_tokenized = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['text'],  # üîß Remove raw text column\n    desc='Tokenizing training data'\n)\n\nprint('Tokenizing validation dataset...')\nval_dataset_tokenized = val_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['text'],\n    desc='Tokenizing validation data'\n)\n\nprint('‚úÖ Datasets tokenized')\nprint(f'  Training: {len(train_dataset_tokenized)} samples')\nprint(f'  Validation: {len(val_dataset_tokenized)} samples')\nprint(f'  Sample columns: {train_dataset_tokenized.column_names}')\nprint('='*60)\n\n# üîß Use data collator for language modeling\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,  # Not masked language modeling\n)\n\n# üîß Create trainer with tokenized data\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset_tokenized,\n    eval_dataset=val_dataset_tokenized,\n    data_collator=data_collator,\n)\n\nprint('‚úÖ Trainer initialized with tokenized datasets')\nprint('üîß READY FOR TRAINING')","metadata":{"colab":{"referenced_widgets":["9a7021eb97ac462e85a4d9f01b5207a4","6419b75f014f402388a771fc4363d3b8","4ede6d9f559b46f88bb5f4d33bf335b7","fcd96773cfaf413b8e2ae35915060e0f","2584a35fa26740abb36f1cc2dbe96094","5463e53a51024bf88bf91fa03654fb00"]},"id":"ToZHecgszCLb","outputId":"74a1ca7d-4d19-40a8-aa51-79444e5b7719","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:21:52.122410Z","iopub.execute_input":"2026-01-31T04:21:52.122864Z","iopub.status.idle":"2026-01-31T04:21:55.351268Z","shell.execute_reply.started":"2026-01-31T04:21:52.122837Z","shell.execute_reply":"2026-01-31T04:21:55.350496Z"}},"outputs":[{"name":"stdout","text":"üîß TOKENIZING DATASET...\nThis may take 1-2 minutes...\n============================================================\nTokenizing training dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing training data:   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5f2ecd92b10472d930e7534a8489048"}},"metadata":{}},{"name":"stdout","text":"Tokenizing validation dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing validation data:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a2daba5661246a0bea91c07e845bf90"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Datasets tokenized\n  Training: 7000 samples\n  Validation: 1500 samples\n  Sample columns: ['__index_level_0__', 'input_ids', 'attention_mask']\n============================================================\n‚úÖ Trainer initialized with tokenized datasets\nüîß READY FOR TRAINING\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Step 13: üöÄ START TRAINING (15-30 minutes with 4B)","metadata":{"id":"Sx4j6fdUzCLb"}},{"cell_type":"code","source":"print('\\n' + '='*60)\nprint('üöÄ STARTING MODEL 2 TRAINING (MedGemma 4B)')\nprint('Stage: Clinical Explanation Fine-tuning')\nprint('Expected Duration: 15-30 minutes')\nprint('='*60)\n\ntrain_result = trainer.train()\n\nprint('\\n' + '='*60)\nprint('‚úÖ TRAINING COMPLETE')\nprint(f'Final Training Loss: {train_result.training_loss:.4f}')\nprint('='*60)","metadata":{"id":"KewC47uGzCLb","outputId":"566dbe41-a3f5-41e8-8939-2694683c5c75","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:21:58.715061Z","iopub.execute_input":"2026-01-31T04:21:58.715343Z"}},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüöÄ STARTING MODEL 2 TRAINING (MedGemma 4B)\nStage: Clinical Explanation Fine-tuning\nExpected Duration: 15-30 minutes\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='258' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [258/500 7:29:05 < 7:04:31, 0.01 it/s, Epoch 0.29/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.539100</td>\n      <td>1.124578</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.340900</td>\n      <td>0.325096</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.270500</td>\n      <td>0.263622</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.228400</td>\n      <td>0.227106</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.215500</td>\n      <td>0.214447</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"## Step 14: Evaluate & Save Model","metadata":{"id":"qXBwNCshzCLb"}},{"cell_type":"code","source":"# Evaluate on test set\neval_results = trainer.evaluate(test_dataset)\nprint(f'Test Loss: {eval_results.get(\"eval_loss\", 0):.4f}')\n\n# Save model\nmodel.save_pretrained('./medgemma_nail_disease_model2_finetuned')\ntokenizer.save_pretrained('./medgemma_nail_disease_model2_finetuned')\nprint('\\n‚úÖ Model saved to ./medgemma_nail_disease_model2_finetuned')","metadata":{"id":"Ou_qlVw3zCLc","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:12:37.424525Z","iopub.status.idle":"2026-01-31T04:12:37.424832Z","shell.execute_reply.started":"2026-01-31T04:12:37.424706Z","shell.execute_reply":"2026-01-31T04:12:37.424724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 15: Extract & Visualize Training Metrics","metadata":{"id":"Xs0_YvqszCLc"}},{"cell_type":"code","source":"import pandas as pd\n\nhistory = {'train_loss': [], 'eval_loss': []}\n\ntry:\n    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n    if os.path.exists('./logs'):\n        for file in sorted(os.listdir('./logs')):\n            if 'events.out.tfevents' in file:\n                ea = EventAccumulator(os.path.join('./logs', file))\n                ea.Reload()\n                for tag in ea.Tags().get('scalars', []):\n                    events = ea.Scalars(tag)\n                    for e in events:\n                        if 'eval' in tag and 'loss' in tag:\n                            history['eval_loss'].append(e.value)\n                        elif 'loss' in tag and 'eval' not in tag:\n                            history['train_loss'].append(e.value)\nexcept Exception as e:\n    print(f'Note: Could not extract tensorboard data: {str(e)[:50]}')\n\nprint(f'Extracted: {len(history[\"train_loss\"])} train steps, {len(history[\"eval_loss\"])} eval steps')","metadata":{"id":"0Vmv7IgyzCLc","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:12:37.425901Z","iopub.status.idle":"2026-01-31T04:12:37.426149Z","shell.execute_reply.started":"2026-01-31T04:12:37.426023Z","shell.execute_reply":"2026-01-31T04:12:37.426035Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 16: üìä Plot Loss Curves & Overfitting Analysis","metadata":{"id":"SpBm6ND4zCLc"}},{"cell_type":"code","source":"train_loss = np.array(history['train_loss']) if history['train_loss'] else np.array([])\neval_loss = np.array(history['eval_loss']) if history['eval_loss'] else np.array([])\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('MedGemma 4B Model 2: Training Metrics & Overfitting Detection', fontsize=14, fontweight='bold')\n\n# Plot 1: Training Loss\nif len(train_loss) > 0:\n    axes[0, 0].plot(train_loss, marker='o', markersize=3, linewidth=2, color='blue')\n    axes[0, 0].set_title('Training Loss Progression', fontweight='bold')\n    axes[0, 0].set_xlabel('Training Step')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Validation Loss\nif len(eval_loss) > 0:\n    axes[0, 1].plot(eval_loss, marker='s', markersize=3, linewidth=2, color='orange')\n    axes[0, 1].set_title('Validation Loss Progression', fontweight='bold')\n    axes[0, 1].set_xlabel('Evaluation Step')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Train vs Eval with Gap\nif len(eval_loss) > 0 and len(train_loss) > 0:\n    min_len = min(len(train_loss), len(eval_loss))\n    train_aligned = train_loss[-min_len:]\n    eval_aligned = eval_loss[-min_len:]\n\n    axes[1, 0].plot(train_aligned, marker='o', label='Train Loss', linewidth=2)\n    axes[1, 0].plot(eval_aligned, marker='s', label='Eval Loss', linewidth=2)\n    axes[1, 0].fill_between(range(min_len), train_aligned, eval_aligned, alpha=0.2, color='red', label='Overfitting Gap')\n    axes[1, 0].set_title('Loss Gap: Train vs Eval', fontweight='bold')\n    axes[1, 0].set_xlabel('Step')\n    axes[1, 0].set_ylabel('Loss')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n\n# Plot 4: Overfitting Metrics Summary\nif len(eval_loss) > 0 and len(train_loss) > 0:\n    min_len = min(len(train_loss), len(eval_loss))\n    train_aligned = train_loss[-min_len:]\n    eval_aligned = eval_loss[-min_len:]\n    loss_gap = eval_aligned - train_aligned\n\n    avg_gap = np.mean(loss_gap)\n    max_gap = np.max(loss_gap)\n\n    if avg_gap < 0.01:\n        status = 'MINIMAL OVERFITTING'\n    elif avg_gap < 0.05:\n        status = 'MILD OVERFITTING'\n    else:\n        status = 'MODERATE-SEVERE OVERFITTING'\n\n    metrics_text = f'OVERFITTING ANALYSIS\\n\\nAvg Loss Gap: {avg_gap:.6f}\\nMax Loss Gap: {max_gap:.6f}\\n\\nStatus: {status}\\n\\nTrain Loss: {train_aligned[-1]:.6f}\\nEval Loss: {eval_aligned[-1]:.6f}\\n\\nImprovement: {(1-eval_aligned[-1]/eval_aligned[0])*100:.1f}%'\n\n    axes[1, 1].text(0.5, 0.5, metrics_text, ha='center', va='center', fontsize=10, family='monospace', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n    axes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.savefig('model2_overfitting_analysis.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('‚úÖ Overfitting analysis saved to model2_overfitting_analysis.png')","metadata":{"id":"0TZTWsADzCLk","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:12:37.427433Z","iopub.status.idle":"2026-01-31T04:12:37.427732Z","shell.execute_reply.started":"2026-01-31T04:12:37.427572Z","shell.execute_reply":"2026-01-31T04:12:37.427586Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 17: üîç Detailed Overfitting Report","metadata":{"id":"XPeb7jdgzCLl"}},{"cell_type":"code","source":"if len(eval_loss) > 0 and len(train_loss) > 0:\n    min_len = min(len(train_loss), len(eval_loss))\n    train_aligned = train_loss[-min_len:]\n    eval_aligned = eval_loss[-min_len:]\n    loss_gap = eval_aligned - train_aligned\n\n    print('\\n' + '='*60)\n    print('üîç OVERFITTING DETECTION ANALYSIS')\n    print('='*60)\n\n    print(f'\\nüìä Loss Gap Statistics:')\n    print(f'  Average Gap: {np.mean(loss_gap):.6f}')\n    print(f'  Max Gap: {np.max(loss_gap):.6f}')\n    print(f'  Min Gap: {np.min(loss_gap):.6f}')\n\n    print(f'\\nüìà Performance Metrics:')\n    print(f'  Final Train Loss: {train_aligned[-1]:.6f}')\n    print(f'  Final Eval Loss: {eval_aligned[-1]:.6f}')\n    print(f'  Loss Improvement: {(1-eval_aligned[-1]/eval_aligned[0])*100:.1f}%')\n\n    if np.mean(loss_gap) < 0.01:\n        status = 'üü¢ MINIMAL OVERFITTING (Excellent!)'\n    elif np.mean(loss_gap) < 0.05:\n        status = 'üü° MILD OVERFITTING (Good)'\n    else:\n        status = 'üî¥ MODERATE-SEVERE OVERFITTING'\n\n    print(f'\\n‚úÖ Status: {status}')\n    print('='*60)","metadata":{"id":"MZf7MLn9zCLl","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:12:37.428645Z","iopub.status.idle":"2026-01-31T04:12:37.428898Z","shell.execute_reply.started":"2026-01-31T04:12:37.428792Z","shell.execute_reply":"2026-01-31T04:12:37.428805Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 18: Save Training Summary & Metadata","metadata":{"id":"oWlSo9-hzCLl"}},{"cell_type":"code","source":"summary = {\n    'pipeline_stage': 'Model 2 - Clinical Explanation',\n    'model': 'google/medgemma-4b-it',\n    'model_size': '4B (Lightweight)',\n    'training_type': 'SFT (Supervised Fine-Tuning) with LoRA',\n    'lora_rank': 16,\n    'lora_alpha': 32,\n    'target_modules': ['q_proj', 'v_proj', 'k_proj'],\n    'train_samples': len(train_df),\n    'val_samples': len(val_df),\n    'test_samples': len(test_df),\n    'epochs': 3,\n    'batch_size': 8,\n    'gradient_accumulation_steps': 1,\n    'learning_rate': 2e-4,\n    'optimizer': 'paged_adamw_8bit',\n    'max_steps': 500,\n    'quantization': '4-bit (nf4)',\n    'training_speed': '~2x faster than 7B',\n    'dataset_source': str(csv_file),\n}\n\nif len(eval_loss) > 0 and len(train_loss) > 0:\n    min_len = min(len(train_loss), len(eval_loss))\n    train_aligned = train_loss[-min_len:]\n    eval_aligned = eval_loss[-min_len:]\n    loss_gap = eval_aligned - train_aligned\n\n    summary.update({\n        'final_train_loss': float(train_aligned[-1]),\n        'final_eval_loss': float(eval_aligned[-1]),\n        'avg_loss_gap': float(np.mean(loss_gap)),\n        'max_loss_gap': float(np.max(loss_gap)),\n        'loss_improvement_percent': float((1-eval_aligned[-1]/eval_aligned[0])*100),\n        'overfitting_status': 'MINIMAL' if np.mean(loss_gap) < 0.01 else 'MILD' if np.mean(loss_gap) < 0.05 else 'MODERATE-SEVERE'\n    })\n\nwith open('model2_training_summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint('‚úÖ Training Summary:')\nprint(json.dumps(summary, indent=2))","metadata":{"id":"oFMbIPx-zCLl","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:12:37.429760Z","iopub.status.idle":"2026-01-31T04:12:37.430560Z","shell.execute_reply.started":"2026-01-31T04:12:37.430415Z","shell.execute_reply":"2026-01-31T04:12:37.430437Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 19: Test Inference with Clinical Example","metadata":{"id":"_t0V2sIXzCLm"}},{"cell_type":"code","source":"# Load best model for inference\nif os.path.exists('./medgemma_nail_disease_model2_finetuned/adapter_model.bin'):\n    model.load_state_dict(torch.load('./medgemma_nail_disease_model2_finetuned/adapter_model.bin', map_location=DEVICE))\n\n# Test with clinical examples\ntest_cases = [\n    \"\"\"CLINICAL ANALYSIS: Nail Disease Diagnosis\n\nPATIENT DEMOGRAPHICS:\nAge: 65\nSex: Female\n\nPRIMARY FINDING (from Model 1 - MedSigLIP):\nClubbing\n\nCLINICAL PRESENTATION:\nConvex nail beds, increased angle between nail and cuticle, bulbous fingertips. Patient has chronic cough and dyspnea.\n\nINSTRUCTION:\nBased on the nail disease finding and clinical presentation above, provide:\n1. Detailed explanation of what the nail finding indicates\n2. Possible systemic diseases that could cause this nail finding\n3. Recommended diagnostic workup and treatment approach\n\nEXPECTED RESPONSE:\n\"\"\"\n]\n\nprint('\\n' + '='*60)\nprint('üîç TEST INFERENCE: Clinical Explanation (4B Model)')\nprint('='*60)\n\nfor i, test_prompt in enumerate(test_cases, 1):\n    print(f'\\nTest Case {i}:')\n    print('-'*60)\n\n    inputs = tokenizer(test_prompt, return_tensors='pt').to(DEVICE)\n    outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_p=0.9, temperature=0.7)\n    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    print(result)\n    print('-'*60)","metadata":{"id":"Ie3t0bQozCLm","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:12:37.431391Z","iopub.status.idle":"2026-01-31T04:12:37.431716Z","shell.execute_reply.started":"2026-01-31T04:12:37.431533Z","shell.execute_reply":"2026-01-31T04:12:37.431548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 20: ‚úÖ Complete!","metadata":{"id":"lJyJdlomzCLm"}},{"cell_type":"code","source":"print('\\n' + '='*60)\nprint('‚úÖ MODEL 2 FINE-TUNING & ANALYSIS COMPLETE!')\nprint('='*60)\nprint('\\nüìä Model Used: MedGemma 4B (Lightweight & Fast)')\nprint('\\nüìÅ Output Files:')\nprint('  ‚úÖ medgemma_nail_disease_model2_finetuned/')\nprint('     - adapter_model.bin (LoRA weights)')\nprint('     - config.json')\nprint('     - tokenizer files')\nprint('  ‚úÖ model2_overfitting_analysis.png (4-subplot visualization)')\nprint('  ‚úÖ model2_training_summary.json (metrics & config)')\nprint('  ‚úÖ logs/ (tensorboard data)')\nprint('\\n‚ö° Performance Benefits of 4B Model:')\nprint('  ‚úÖ ~2x faster training than 7B')\nprint('  ‚úÖ 50% smaller model size')\nprint('  ‚úÖ Lower memory requirements')\nprint('  ‚úÖ Excellent medical understanding maintained')\nprint('\\nüöÄ Next Steps:')\nprint('  1. Download files from Kaggle Output tab')\nprint('  2. Use model2 for clinical explanations in your app')\nprint('  3. Start Stage 3 training with MedGemma 27B')\nprint('  4. Build mobile/web app integrating all 3 stages')\nprint('\\nüìä Model Performance:')\nif len(eval_loss) > 0:\n    print(f'  Final Test Loss: {eval_loss[-1]:.4f}')\nprint('='*60)","metadata":{"id":"lQPnsrvRzCLm","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:12:37.433547Z","iopub.status.idle":"2026-01-31T04:12:37.433959Z","shell.execute_reply.started":"2026-01-31T04:12:37.433791Z","shell.execute_reply":"2026-01-31T04:12:37.433811Z"}},"outputs":[],"execution_count":null}]}
