{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune MedGemma 4B for Nail Disease Clinical Explanations\n",
    "## Model 2: Clinical Findings ‚Üí Medical Explanations Pipeline\n",
    "\n",
    "**Pipeline Architecture:**\n",
    "- Stage 1 ‚úÖ DONE: MedSigLIP (Image Classification) ‚Üí \"Clubbing\" / \"Pitting\" etc.\n",
    "- Stage 2 ‚≠ê NOW: MedGemma 4B (Clinical Explanation) ‚Üí \"What does this mean?\"\n",
    "- Stage 3: MedGemma 27B (Disease Ranking) ‚Üí \"What diseases could cause this?\"\n",
    "\n",
    "Based on: https://github.com/google-health/medgemma\n",
    "Model: google/medgemma-7b-orcamath-it (Instruction-tuned, 7B params)\n",
    "License: Apache 2.0\n",
    "\n",
    "Features: Medical prompt engineering | CSV integration | Overfitting detection | Comprehensive metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "IS_KAGGLE = os.path.exists('/kaggle')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('='*60)\n",
    "print('ENVIRONMENT SETUP')\n",
    "print('='*60)\n",
    "print(f'Environment: {\"Kaggle\" if IS_KAGGLE else \"Local/Colab\"}')\n",
    "print(f'Device: {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('GPU: None - CPU mode')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch bitsandbytes peft trl scikit-learn matplotlib\n",
    "print('‚úÖ Packages installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "set_seed(42)\n",
    "print('‚úÖ Libraries imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load & Explore Dataset (Model 2 Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV dataset for Model 2: Clinical Explanation Stage\n",
    "csv_path = '/kaggle/input/nail-disease-medgemma/nail_diseases.csv'\n",
    "\n",
    "# Find CSV in various possible locations\n",
    "possible_paths = [\n",
    "    '/kaggle/input/nail-disease-medgemma/nail_diseases.csv',\n",
    "    '/kaggle/input/nail-disease-classification/nail_diseases.csv',\n",
    "    '/kaggle/input/nail-diseases/nail_diseases.csv',\n",
    "    './nail_diseases.csv'\n",
    "]\n",
    "\n",
    "df = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        csv_path = path\n",
    "        break\n",
    "\n",
    "if df is None:\n",
    "    print(f'‚ùå CSV file not found in standard locations')\n",
    "    print('\\nAvailable inputs:')\n",
    "    if IS_KAGGLE:\n",
    "        for item in os.listdir('/kaggle/input'):\n",
    "            print(f'  - {item}')\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f'‚úÖ Loaded {len(df)} samples from {csv_path}')\n",
    "print(f'\\nDataset Shape: {df.shape}')\n",
    "print(f'\\nColumns: {list(df.columns)}')\n",
    "print(f'\\nFirst row:')\n",
    "print(df.iloc[0])\n",
    "print(f'\\nData types:')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Enhanced Medical Prompt Templates for Clinical Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_medical_prompt_model2(row):\n",
    "    \"\"\"\n",
    "    Creates advanced medical prompts for MedGemma Model 2 (Clinical Explanation Stage).\n",
    "    \n",
    "    Input: Nail disease classification from Model 1 + clinical findings\n",
    "    Output: Detailed clinical explanation of findings, differential diagnoses, and systemic implications\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract fields (handle missing/None values)\n",
    "    nail_disease = str(row.get('nail_disease', row.get('disease_name', ''))).strip()\n",
    "    clinical_findings = str(row.get('clinical_findings', row.get('findings', ''))).strip()\n",
    "    patient_age = row.get('patient_age', row.get('age', 'unknown'))\n",
    "    patient_sex = str(row.get('patient_sex', row.get('sex', row.get('gender', 'unknown')))).strip()\n",
    "    differential_diagnoses = str(row.get('differential_diagnoses', row.get('differentials', ''))).strip()\n",
    "    systemic_implications = str(row.get('systemic_implications', row.get('implications', ''))).strip()\n",
    "    treatment_protocol = str(row.get('treatment_protocol', row.get('treatment', ''))).strip()\n",
    "    \n",
    "    # Build instruction-following prompt (Orca format)\n",
    "    prompt = f\"\"\"CLINICAL ANALYSIS: Nail Disease Diagnosis\n",
    "\n",
    "PATIENT DEMOGRAPHICS:\n",
    "Age: {patient_age}\n",
    "Sex: {patient_sex}\n",
    "\n",
    "PRIMARY FINDING (from Model 1 - MedSigLIP):\n",
    "{nail_disease}\n",
    "\n",
    "CLINICAL PRESENTATION:\n",
    "{clinical_findings}\n",
    "\n",
    "INSTRUCTION:\n",
    "Based on the nail disease finding and clinical presentation above, provide:\n",
    "1. Detailed explanation of what the nail finding indicates\n",
    "2. Possible systemic diseases that could cause this nail finding\n",
    "3. Recommended diagnostic workup and treatment approach\n",
    "\n",
    "EXPECTED RESPONSE:\n",
    "Nail Finding Explanation: {nail_disease} indicates {systemic_implications}\n",
    "\n",
    "Differential Diagnoses: {differential_diagnoses}\n",
    "\n",
    "Recommended Treatment: {treatment_protocol}\n\"\"\"\n",
    "    \n",
    "    return prompt.strip()\n",
    "\n",
    "# Apply prompt template to dataset\n",
    "df['text'] = df.apply(create_medical_prompt_model2, axis=1)\n",
    "\n",
    "print(f'‚úÖ Created {len(df)} medical prompts for Model 2 training')\n",
    "print(f'\\nExample prompt (first 500 chars):')\n",
    "print('='*60)\n",
    "print(df['text'].iloc[0][:500])\n",
    "print('='*60)\n",
    "print(f'\\nAverage prompt length: {df[\"text\"].str.len().mean():.0f} chars')\n",
    "print(f'Max prompt length: {df[\"text\"].str.len().max():.0f} chars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Quality & Validation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and data quality\n",
    "print('üìä DATA QUALITY REPORT')\n",
    "print('='*60)\n",
    "\n",
    "# Missing values\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print('\\n‚ö†Ô∏è  Missing values detected:')\n",
    "    print(missing[missing > 0])\nelse:\n",
    "    print('\\n‚úÖ No missing values')\n",
    "\n",
    "# Check text field quality\n",
    "empty_texts = (df['text'].str.len() < 50).sum()\n",
    "if empty_texts > 0:\n",
    "    print(f'\\n‚ö†Ô∏è  {empty_texts} prompts are too short (<50 chars)')\n",
    "else:\n",
    "    print(f'\\n‚úÖ All prompts have sufficient length')\n",
    "\n",
    "# Disease distribution\n",
    "if 'nail_disease' in df.columns:\n",
    "    print(f'\\nüìã Disease Distribution:')\n",
    "    print(df['nail_disease'].value_counts())\n",
    "\n",
    "print('\\n' + '='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Split Dataset (Train/Val/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split: 70% train, 15% val, 15% test\n",
    "split_key = 'nail_disease' if 'nail_disease' in df.columns else None\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=df[split_key] if split_key else None\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.5, \n",
    "    random_state=42,\n",
    "    stratify=temp_df[split_key] if split_key else None\n",
    ")\n",
    "\n",
    "print('üìä DATASET SPLIT')\n",
    "print('='*60)\n",
    "print(f'Train: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)')\n",
    "print(f'Val:   {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)')\n",
    "print(f'Test:  {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)')\n",
    "print(f'Total: {len(df)} samples')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create HuggingFace Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[['text']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text']])\n",
    "\n",
    "print('‚úÖ HuggingFace datasets created')\n",
    "print(f'  Train: {len(train_dataset)} samples')\n",
    "print(f'  Val:   {len(val_dataset)} samples')\n",
    "print(f'  Test:  {len(test_dataset)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Setup Model & Tokenizer (MedGemma 4B/7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (MedGemma 4B or 7B instruction-tuned)\n",
    "MODEL_ID = 'google/medgemma-7b-orcamath-it'\n",
    "# Alternative: MODEL_ID = 'google/medgemma-4b-orcamath-it' for lighter footprint\n",
    "\n",
    "# 4-bit quantization config (memory efficient)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f'Loading model: {MODEL_ID}')\n",
    "print('This may take 2-3 minutes...')\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f'‚úÖ Model loaded successfully')\n",
    "    print(f'   Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters')\nexcept Exception as e:\n",
    "    print(f'‚ùå Error loading model: {str(e)[:100]}')\n",
    "    print('Make sure you have HuggingFace token set up for gated models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Configure LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,  # Alpha scaling\n",
    "    target_modules=['q_proj', 'v_proj', 'k_proj'],  # Query, Value, Key projections\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f'‚úÖ LoRA configured')\n",
    "print(f'  Total params: {total_params / 1e9:.2f}B')\n",
    "print(f'  Trainable: {trainable_params / 1e6:.2f}M ({100*trainable_params/total_params:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Setup Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (optimized for Model 2: Clinical Explanation)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./medgemma_nail_disease_model2_finetuned',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    max_steps=500,  # Limit steps for faster training\n",
    "    max_seq_length=512,\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    logging_dir='./logs',\n",
    "    optim='paged_adamw_8bit',  # Memory-efficient optimizer\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=True,\n",
    ")\n",
    "\n",
    "print('‚úÖ Training configuration ready')\n",
    "print(f'  Output: ./medgemma_nail_disease_model2_finetuned')\n",
    "print(f'  Epochs: {training_args.num_train_epochs}')\n",
    "print(f'  Batch size: {training_args.per_device_train_batch_size}')\n",
    "print(f'  Learning rate: {training_args.learning_rate}')\n",
    "print(f'  Max steps: {training_args.max_steps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Initialize SFT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=512,\n",
    ")\n",
    "\n",
    "print('‚úÖ Trainer initialized and ready for training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: üöÄ START TRAINING (30-60 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('üöÄ STARTING MODEL 2 TRAINING')\n",
    "print('Stage: Clinical Explanation Fine-tuning')\n",
    "print('='*60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('‚úÖ TRAINING COMPLETE')\n",
    "print(f'Final Training Loss: {train_result.training_loss:.4f}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Evaluate & Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "print(f'Test Loss: {eval_results.get(\"eval_loss\", 0):.4f}')\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('./medgemma_nail_disease_model2_finetuned')\n",
    "tokenizer.save_pretrained('./medgemma_nail_disease_model2_finetuned')\n",
    "print('\\n‚úÖ Model saved to ./medgemma_nail_disease_model2_finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Extract & Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history = {'train_loss': [], 'eval_loss': []}\n",
    "\n",
    "try:\n",
    "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "    if os.path.exists('./logs'):\n",
    "        for file in sorted(os.listdir('./logs')):\n",
    "            if 'events.out.tfevents' in file:\n",
    "                ea = EventAccumulator(os.path.join('./logs', file))\n",
    "                ea.Reload()\n",
    "                for tag in ea.Tags().get('scalars', []):\n",
    "                    events = ea.Scalars(tag)\n",
    "                    for e in events:\n",
    "                        if 'eval' in tag and 'loss' in tag:\n",
    "                            history['eval_loss'].append(e.value)\n",
    "                        elif 'loss' in tag and 'eval' not in tag:\n",
    "                            history['train_loss'].append(e.value)\nexcept Exception as e:\n",
    "    print(f'Note: Could not extract tensorboard data: {str(e)[:50]}')\n",
    "\n",
    "print(f'Extracted: {len(history[\"train_loss\"])} train steps, {len(history[\"eval_loss\"])} eval steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: üìä Plot Loss Curves & Overfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.array(history['train_loss']) if history['train_loss'] else np.array([])\n",
    "eval_loss = np.array(history['eval_loss']) if history['eval_loss'] else np.array([])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('MedGemma Model 2: Training Metrics & Overfitting Detection', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "if len(train_loss) > 0:\n",
    "    axes[0, 0].plot(train_loss, marker='o', markersize=3, linewidth=2, color='blue')\n",
    "    axes[0, 0].set_title('Training Loss Progression', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Training Step')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "if len(eval_loss) > 0:\n",
    "    axes[0, 1].plot(eval_loss, marker='s', markersize=3, linewidth=2, color='orange')\n",
    "    axes[0, 1].set_title('Validation Loss Progression', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Evaluation Step')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Train vs Eval with Gap\n",
    "if len(eval_loss) > 0 and len(train_loss) > 0:\n",
    "    min_len = min(len(train_loss), len(eval_loss))\n",
    "    train_aligned = train_loss[-min_len:]\n",
    "    eval_aligned = eval_loss[-min_len:]\n",
    "    \n",
    "    axes[1, 0].plot(train_aligned, marker='o', label='Train Loss', linewidth=2)\n",
    "    axes[1, 0].plot(eval_aligned, marker='s', label='Eval Loss', linewidth=2)\n",
    "    axes[1, 0].fill_between(range(min_len), train_aligned, eval_aligned, alpha=0.2, color='red', label='Overfitting Gap')\n",
    "    axes[1, 0].set_title('Loss Gap: Train vs Eval', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Step')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Overfitting Metrics Summary\n",
    "if len(eval_loss) > 0 and len(train_loss) > 0:\n",
    "    min_len = min(len(train_loss), len(eval_loss))\n",
    "    train_aligned = train_loss[-min_len:]\n",
    "    eval_aligned = eval_loss[-min_len:]\n",
    "    loss_gap = eval_aligned - train_aligned\n",
    "    \n",
    "    avg_gap = np.mean(loss_gap)\n",
    "    max_gap = np.max(loss_gap)\n",
    "    \n",
    "    if avg_gap < 0.01:\n",
    "        status = 'MINIMAL OVERFITTING'\n",
    "    elif avg_gap < 0.05:\n",
    "        status = 'MILD OVERFITTING'\n",
    "    else:\n",
    "        status = 'MODERATE-SEVERE OVERFITTING'\n",
    "    \n",
    "    metrics_text = f'OVERFITTING ANALYSIS\\n\\nAvg Loss Gap: {avg_gap:.6f}\\nMax Loss Gap: {max_gap:.6f}\\n\\nStatus: {status}\\n\\nTrain Loss: {train_aligned[-1]:.6f}\\nEval Loss: {eval_aligned[-1]:.6f}\\n\\nImprovement: {(1-eval_aligned[-1]/eval_aligned[0])*100:.1f}%'\n",
    "    \n",
    "    axes[1, 1].text(0.5, 0.5, metrics_text, ha='center', va='center', fontsize=10, family='monospace', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model2_overfitting_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('‚úÖ Overfitting analysis saved to model2_overfitting_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: üîç Detailed Overfitting Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(eval_loss) > 0 and len(train_loss) > 0:\n",
    "    min_len = min(len(train_loss), len(eval_loss))\n",
    "    train_aligned = train_loss[-min_len:]\n",
    "    eval_aligned = eval_loss[-min_len:]\n",
    "    loss_gap = eval_aligned - train_aligned\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print('üîç OVERFITTING DETECTION ANALYSIS')\n",
    "    print('='*60)\n",
    "    \n",
    "    print(f'\\nüìä Loss Gap Statistics:')\n",
    "    print(f'  Average Gap: {np.mean(loss_gap):.6f}')\n",
    "    print(f'  Max Gap: {np.max(loss_gap):.6f}')\n",
    "    print(f'  Min Gap: {np.min(loss_gap):.6f}')\n",
    "    \n",
    "    print(f'\\nüìà Performance Metrics:')\n",
    "    print(f'  Final Train Loss: {train_aligned[-1]:.6f}')\n",
    "    print(f'  Final Eval Loss: {eval_aligned[-1]:.6f}')\n",
    "    print(f'  Loss Improvement: {(1-eval_aligned[-1]/eval_aligned[0])*100:.1f}%')\n",
    "    \n",
    "    if np.mean(loss_gap) < 0.01:\n",
    "        status = 'üü¢ MINIMAL OVERFITTING (Excellent!)'\n",
    "    elif np.mean(loss_gap) < 0.05:\n",
    "        status = 'üü° MILD OVERFITTING (Good)'\n",
    "    else:\n",
    "        status = 'üî¥ MODERATE-SEVERE OVERFITTING'\n",
    "    \n",
    "    print(f'\\n‚úÖ Status: {status}')\n",
    "    print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Save Training Summary & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'pipeline_stage': 'Model 2 - Clinical Explanation',\n",
    "    'model': 'google/medgemma-7b-orcamath-it',\n",
    "    'training_type': 'SFT (Supervised Fine-Tuning) with LoRA',\n",
    "    'lora_rank': 16,\n",
    "    'lora_alpha': 32,\n",
    "    'target_modules': ['q_proj', 'v_proj', 'k_proj'],\n",
    "    'train_samples': len(train_df),\n",
    "    'val_samples': len(val_df),\n",
    "    'test_samples': len(test_df),\n",
    "    'epochs': 3,\n",
    "    'batch_size': 4,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'learning_rate': 2e-4,\n",
    "    'optimizer': 'paged_adamw_8bit',\n",
    "    'max_steps': 500,\n",
    "    'quantization': '4-bit (nf4)',\n",
    "    'dataset_source': csv_path,\n",
    "}\n",
    "\n",
    "if len(eval_loss) > 0 and len(train_loss) > 0:\n",
    "    min_len = min(len(train_loss), len(eval_loss))\n",
    "    train_aligned = train_loss[-min_len:]\n",
    "    eval_aligned = eval_loss[-min_len:]\n",
    "    loss_gap = eval_aligned - train_aligned\n",
    "    \n",
    "    summary.update({\n",
    "        'final_train_loss': float(train_aligned[-1]),\n",
    "        'final_eval_loss': float(eval_aligned[-1]),\n",
    "        'avg_loss_gap': float(np.mean(loss_gap)),\n",
    "        'max_loss_gap': float(np.max(loss_gap)),\n",
    "        'loss_improvement_percent': float((1-eval_aligned[-1]/eval_aligned[0])*100),\n",
    "        'overfitting_status': 'MINIMAL' if np.mean(loss_gap) < 0.01 else 'MILD' if np.mean(loss_gap) < 0.05 else 'MODERATE-SEVERE'\n",
    "    })\n",
    "\n",
    "with open('model2_training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print('‚úÖ Training Summary:')\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Test Inference with Clinical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for inference\n",
    "if os.path.exists('./medgemma_nail_disease_model2_finetuned/adapter_model.bin'):\n",
    "    model.load_state_dict(torch.load('./medgemma_nail_disease_model2_finetuned/adapter_model.bin', map_location=DEVICE))\n",
    "\n",
    "# Test with clinical examples\n",
    "test_cases = [\n",
    "    \"\"\"CLINICAL ANALYSIS: Nail Disease Diagnosis\n",
    "\n",
    "PATIENT DEMOGRAPHICS:\n",
    "Age: 65\n",
    "Sex: Female\n",
    "\n",
    "PRIMARY FINDING (from Model 1 - MedSigLIP):\n",
    "Clubbing\n",
    "\n",
    "CLINICAL PRESENTATION:\n",
    "Convex nail beds, increased angle between nail and cuticle, bulbous fingertips. Patient has chronic cough and dyspnea.\n",
    "\n",
    "INSTRUCTION:\n",
    "Based on the nail disease finding and clinical presentation above, provide:\n",
    "1. Detailed explanation of what the nail finding indicates\n",
    "2. Possible systemic diseases that could cause this nail finding\n",
    "3. Recommended diagnostic workup and treatment approach\n",
    "\n",
    "EXPECTED RESPONSE:\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('üîç TEST INFERENCE: Clinical Explanation')\n",
    "print('='*60)\n",
    "\n",
    "for i, test_prompt in enumerate(test_cases, 1):\n",
    "    print(f'\\nTest Case {i}:')\n",
    "    print('-'*60)\n",
    "    \n",
    "    inputs = tokenizer(test_prompt, return_tensors='pt').to(DEVICE)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_p=0.9, temperature=0.7)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(result)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19: 3-Stage Pipeline Integration Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('üéØ 3-STAGE NAIL DISEASE ANALYSIS PIPELINE')\n",
    "print('='*70)\n",
    "\n",
    "print('''\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ STAGE 1: IMAGE CLASSIFICATION (MedSigLIP - COMPLETED)              ‚îÇ\n",
    "‚îÇ Input:  Nail photograph                                             ‚îÇ\n",
    "‚îÇ Output: Disease class (\"Clubbing\", \"Pitting\", \"Splinter\", etc.)    ‚îÇ\n",
    "‚îÇ Model:  MedSigLIP (Vision-Language Model)                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ STAGE 2: CLINICAL EXPLANATION (MedGemma 4B - NOW TRAINED!)        ‚îÇ\n",
    "‚îÇ Input:  Disease class + clinical findings + patient data           ‚îÇ\n",
    "‚îÇ Output: Detailed medical explanation & differential diagnoses      ‚îÇ\n",
    "‚îÇ Model:  MedGemma 4B/7B (Fine-tuned with your CSV data)             ‚îÇ\n",
    "‚îÇ                                                                     ‚îÇ\n",
    "‚îÇ Example Output:                                                     ‚îÇ\n",
    "‚îÇ \"Clubbing indicates increased tissue proliferation at nail bases    ‚îÇ\n",
    "‚îÇ  Often caused by chronic lung disease, cardiac disease, or IBD.    ‚îÇ\n",
    "‚îÇ  Recommend: CXR, ECG, pulmonary function tests...\"                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ STAGE 3: DISEASE RANKING (MedGemma 27B - NEXT)                     ‚îÇ\n",
    "‚îÇ Input:  Clinical explanation + patient history                     ‚îÇ\n",
    "‚îÇ Output: Ranked list of probable diseases + urgency levels          ‚îÇ\n",
    "‚îÇ Model:  MedGemma 27B (More capable for complex reasoning)          ‚îÇ\n",
    "‚îÇ                                                                     ‚îÇ\n",
    "‚îÇ Example Output:                                                     ‚îÇ\n",
    "‚îÇ 1. Lung Cancer (High urgency) - requires immediate CT scan         ‚îÇ\n",
    "‚îÇ 2. COPD (Medium) - needs pulmonary function tests                  ‚îÇ\n",
    "‚îÇ 3. Heart Disease (Medium) - needs echocardiogram                   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "''')\n",
    "\n",
    "print('\\n‚úÖ MODEL 2 TRAINING COMPLETE!')\n",
    "print('\\nüìÅ Output Files:')\n",
    "print('  ‚úÖ medgemma_nail_disease_model2_finetuned/')\n",
    "print('     - adapter_model.bin (LoRA weights)')\n",
    "print('     - config.json')\n",
    "print('     - tokenizer files')\n",
    "print('  ‚úÖ model2_overfitting_analysis.png')\n",
    "print('  ‚úÖ model2_training_summary.json')\n",
    "print('\\nüöÄ Next Steps:')\n",
    "print('  1. Download all files from Kaggle Output tab')\n",
    "print('  2. Integrate Model 2 with Stage 1 (MedSigLIP)')\n",
    "print('  3. Start Stage 3 training with MedGemma 27B')\n",
    "print('  4. Build 3-screen mobile/web app for complete pipeline')\n",
    "print('\\n' + '='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20: Save Pipeline Integration Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save integration code snippet for 3-stage pipeline\n",
    "integration_code = '''# INTEGRATION: 3-Stage Nail Disease Analysis Pipeline\n# Use this code to combine all three stages\n\nfrom PIL import Image\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\nclass NailDiseaseAnalyzer:\n    def __init__(self, model1_path, model2_path, model3_path):\n        # Stage 1: MedSigLIP (Image Classification)\n        self.model1 = load_medsiglip(model1_path)\n        \n        # Stage 2: MedGemma 4B (Clinical Explanation)\n        self.model2 = AutoModelForCausalLM.from_pretrained(\"google/medgemma-7b-orcamath-it\")\n        self.model2 = PeftModel.from_pretrained(self.model2, model2_path)\n        \n        # Stage 3: MedGemma 27B (Disease Ranking)\n        self.model3 = AutoModelForCausalLM.from_pretrained(\"google/medgemma-27b-orcamath-it\")\n        self.model3 = PeftModel.from_pretrained(self.model3, model3_path)\n        \n        self.tokenizer2 = AutoTokenizer.from_pretrained(\"google/medgemma-7b-orcamath-it\")\n        self.tokenizer3 = AutoTokenizer.from_pretrained(\"google/medgemma-27b-orcamath-it\")\n    \n    def analyze_nail_image(self, image_path, patient_age, patient_sex, clinical_findings):\n        # Stage 1: Classify nail disease from image\n        nail_disease = self.model1.predict(Image.open(image_path))\n        \n        # Stage 2: Generate clinical explanation\n        prompt2 = f\"\"\"CLINICAL ANALYSIS: Nail Disease Diagnosis\n        \n        PATIENT DEMOGRAPHICS:\n        Age: {patient_age}\n        Sex: {patient_sex}\n        \n        PRIMARY FINDING:\n        {nail_disease}\n        \n        CLINICAL PRESENTATION:\n        {clinical_findings}\n        \n        INSTRUCTION: Provide detailed explanation...\"\"\"\n        \n        explanation = self.model2.generate(self.tokenizer2(prompt2, return_tensors='pt'))\n        \n        # Stage 3: Rank possible diseases\n        prompt3 = f\"\"\"Based on the nail disease {nail_disease} and explanation: {explanation}\n        \n        Rank the most probable systemic diseases...\"\"\"\n        \n        diagnosis_ranking = self.model3.generate(self.tokenizer3(prompt3, return_tensors='pt'))\n        \n        return {\n            \"stage1_classification\": nail_disease,\n            \"stage2_explanation\": explanation,\n            \"stage3_diagnosis_ranking\": diagnosis_ranking\n        }\n\n# Usage:\nanalyzer = NailDiseaseAnalyzer(\n    model1_path=\"./medsiglib_nail_classifier\",\n    model2_path=\"./medgemma_nail_disease_model2_finetuned\",\n    model3_path=\"./medgemma_nail_disease_model3_finetuned\"\n)\n\nresult = analyzer.analyze_nail_image(\n    image_path=\"nail_photo.jpg\",\n    patient_age=65,\n    patient_sex=\"Female\",\n    clinical_findings=\"Convex nail beds with bulbous fingertips, chronic cough\"\n)\n\nprint(result)\n'''\n",
    "\n",
    "with open('pipeline_integration_code.py', 'w') as f:\n",
    "    f.write(integration_code)\n",
    "\n",
    "print('‚úÖ Pipeline integration code saved to pipeline_integration_code.py')\n",
    "print('\\nYou can use this code template to combine all 3 stages!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete! ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('‚úÖ MODEL 2 FINE-TUNING & ANALYSIS COMPLETE!')\n",
    "print('='*60)\n",
    "print('\\nüìÅ Output Files:')\n",
    "print('  ‚úÖ medgemma_nail_disease_model2_finetuned/')\n",
    "print('     - adapter_model.bin (LoRA weights)')\n",
    "print('     - config.json')\n",
    "print('     - tokenizer files')\n",
    "print('  ‚úÖ model2_overfitting_analysis.png (4-subplot visualization)')\n",
    "print('  ‚úÖ model2_training_summary.json (metrics & config)')\n",
    "print('  ‚úÖ pipeline_integration_code.py (3-stage template)')\n",
    "print('  ‚úÖ logs/ (tensorboard data)')\n",
    "print('\\nüöÄ Next Steps:')\n",
    "print('  1. Download files from Kaggle Output tab')\n",
    "print('  2. Use model2 for clinical explanations in your app')\n",
    "print('  3. Start Stage 3 training with MedGemma 27B')\n",
    "print('  4. Build mobile/web app integrating all 3 stages')\n",
    "print('\\nüìä Model Performance:')\nif len(eval_loss) > 0:\n",
    "    print(f'  Final Test Loss: {eval_loss[-1]:.4f}')\nprint('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}