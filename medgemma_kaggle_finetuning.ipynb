{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Fine-tune MedGemma for Nail Disease Classification\n","## Advanced Kaggle Notebook with Overfitting Detection\n","\n","Based on: https://github.com/google-health/medgemma\n","Model: google/medgemma-7b-orcamath-it (Instruction-tuned, 7B params)\n","License: Apache 2.0\n","\n","Features: Loss graphs | Overfitting detection | Comprehensive metrics"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 1: Setup Environment"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["import os\nimport torch\nimport json\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nIS_KAGGLE = os.path.exists('/kaggle')\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint('='*60)\nprint('ENVIRONMENT SETUP')\nprint('='*60)\nprint(f'Environment: {\"Kaggle\" if IS_KAGGLE else \"Local/Colab\"}')\nprint(f'Device: {DEVICE}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\nelse:\n    print('GPU: None - CPU mode')\nprint(f'PyTorch: {torch.__version__}')\nprint('='*60)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["!pip install -q transformers datasets torch bitsandbytes peft trl scikit-learn matplotlib\nprint('‚úÖ Packages installed')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 2: Import Libraries"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["from datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    BitsAndBytesConfig,\n    TrainingArguments,\n    set_seed\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\nset_seed(42)\nprint('‚úÖ Libraries imported')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 3: Load & Explore Dataset"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Load CSV dataset\ncsv_path = '/kaggle/input/nail-disease-medgemma/nail_diseases.csv'\n\ntry:\n    df = pd.read_csv(csv_path)\n    print(f'‚úÖ Loaded {len(df)} samples')\n    print(f'\\nColumns: {list(df.columns)}')\n    print(f'\\nFirst row:')\n    print(df.iloc[0])\nexcept FileNotFoundError:\n    print(f'‚ùå File not found: {csv_path}')\n    print('\\nAlternative paths to try:')\n    print('  - /kaggle/input/nail-disease-classification/nail_diseases.csv')\n    print('  - /kaggle/input/nail-diseases/nail_diseases.csv')\n    print('\\nAvailable inputs:')\n    for item in os.listdir('/kaggle/input')::\n        print(f'  - {item}')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 4: Create Training Prompts"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["def create_medical_prompt(row):\n    \"\"\"\n    Creates a medical text prompt following MedGemma format.\n    Structure: Clinical findings ‚Üí Diagnosis ‚Üí Treatment ‚Üí Prognosis\n    \"\"\"\n    findings = str(row.get('clinical_findings', '')).strip()\n    diagnosis = str(row.get('confirmed_diagnosis', '')).strip()\n    treatment = str(row.get('treatment_protocol', '')).strip()\n    prognosis = str(row.get('prognosis', '')).strip()\n    \n    # Build prompt in Orca format (instruction-following)\n    prompt = f\"\"\"Clinical Case Analysis:\n    \n    Clinical Findings:\n    {findings}\n    \n    Please provide the diagnosis and treatment plan.\n    \n    Expected Output:\n    Diagnosis: {diagnosis}\n    Treatment: {treatment}\n    Prognosis: {prognosis}\"\"\"\n    \n    return prompt.strip()\n\n# Apply to dataset\ndf['text'] = df.apply(create_medical_prompt, axis=1)\n\nprint(f'‚úÖ Created {len(df)} training prompts')\nprint(f'\\nExample prompt (first 300 chars):')\nprint(df['text'].iloc[0][:300])"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 5: Split Dataset"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Split: 70% train, 15% val, 15% test\ntrain_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\nprint(f'Train: {len(train_df)} samples')\nprint(f'Val:   {len(val_df)} samples')\nprint(f'Test:  {len(test_df)} samples')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 6: Create HuggingFace Datasets"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Create HuggingFace datasets\ntrain_dataset = Dataset.from_pandas(train_df[['text']])\nval_dataset = Dataset.from_pandas(val_df[['text']])\ntest_dataset = Dataset.from_pandas(test_df[['text']])\n\nprint(f'‚úÖ HuggingFace datasets created')\nprint(f'  Train: {len(train_dataset)} samples')\nprint(f'  Val:   {len(val_dataset)} samples')\nprint(f'  Test:  {len(test_dataset)} samples')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 7: Setup Model & Tokenizer"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Model configuration\nMODEL_ID = 'google/medgemma-7b-orcamath-it'\n\n# 4-bit quantization config (memory efficient)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(f'Loading model: {MODEL_ID}')\nprint('This may take 2-3 minutes...')\n\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID,\n        quantization_config=bnb_config,\n        device_map='auto',\n        trust_remote_code=True,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    print(f'‚úÖ Model loaded successfully')\n    print(f'   Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters')\nexcept Exception as e:\n    print(f'‚ùå Error loading model: {str(e)[:100]}')\n    print('Make sure you have HuggingFace token set up for gated models')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 8: Configure LoRA"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,  # Rank\n    lora_alpha=32,  # Alpha scaling\n    target_modules=['q_proj', 'v_proj'],  # Query and Value projections\n    lora_dropout=0.05,\n    bias='none',\n    task_type='CAUSAL_LM'\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Count trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f'‚úÖ LoRA configured')\nprint(f'  Total params: {total_params / 1e9:.2f}B')\nprint(f'  Trainable: {trainable_params / 1e6:.2f}M ({100*trainable_params/total_params:.3f}%)')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 9: Setup Training Configuration"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./medgemma_nail_disease_finetuned',\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-4,\n    lr_scheduler_type='cosine',\n    warmup_steps=100,\n    weight_decay=0.01,\n    max_steps=500,  # Limit steps for faster training\n    max_seq_length=512,\n    logging_steps=10,\n    eval_steps=50,\n    save_steps=50,\n    evaluation_strategy='steps',\n    save_strategy='steps',\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    logging_dir='./logs',\n    optim='paged_adamw_8bit',  # Memory-efficient optimizer\n    seed=42,\n)\n\nprint('‚úÖ Training configuration ready')\nprint(f'  Output: ./medgemma_nail_disease_finetuned')\nprint(f'  Epochs: {training_args.num_train_epochs}')\nprint(f'  Batch size: {training_args.per_device_train_batch_size}')\nprint(f'  Learning rate: {training_args.learning_rate}')\nprint(f'  Max steps: {training_args.max_steps}')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 10: Initialize SFT Trainer"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    dataset_text_field='text',\n    max_seq_length=512,\n)\n\nprint('‚úÖ Trainer initialized')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 11: üöÄ START TRAINING (30-60 minutes)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["print('\\n' + '='*60)\nprint('üöÄ STARTING TRAINING')\nprint('='*60)\n\ntrain_result = trainer.train()\n\nprint('\\n' + '='*60)\nprint('‚úÖ TRAINING COMPLETE')\nprint(f'Final Training Loss: {train_result.training_loss:.4f}')\nprint('='*60)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 12: Evaluate & Save"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Evaluate on test set\neval_results = trainer.evaluate(test_dataset)\nprint(f'\\nTest Loss: {eval_results.get(\"eval_loss\", 0):.4f}')\n\n# Save model\nmodel.save_pretrained('./medgemma_nail_disease_finetuned')\ntokenizer.save_pretrained('./medgemma_nail_disease_finetuned')\nprint('\\n‚úÖ Model saved to ./medgemma_nail_disease_finetuned')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 13: Extract & Visualize Training Metrics"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["import pandas as pd\n\nhistory = {'train_loss': [], 'eval_loss': []}\n\ntry:\n    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n    if os.path.exists('./logs'):\n        for file in sorted(os.listdir('./logs')):\n            if 'events.out.tfevents' in file:\n                ea = EventAccumulator(os.path.join('./logs', file))\n                ea.Reload()\n                for tag in ea.Tags().get('scalars', []):\n                    events = ea.Scalars(tag)\n                    for e in events:\n                        if 'eval' in tag and 'loss' in tag:\n                            history['eval_loss'].append(e.value)\n                        elif 'loss' in tag and 'eval' not in tag:\n                            history['train_loss'].append(e.value)\nexcept Exception as e:\n    print(f'Note: Could not extract tensorboard data: {str(e)[:50]}')\n\nprint(f'Extracted: {len(history[\"train_loss\"])} train steps, {len(history[\"eval_loss\"])} eval steps')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 14: üìä Plot Loss Curves & Overfitting Analysis"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["train_loss = np.array(history['train_loss']) if history['train_loss'] else np.array([])\neval_loss = np.array(history['eval_loss']) if history['eval_loss'] else np.array([])\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('MedGemma Training: Overfitting Detection & Metrics', fontsize=14, fontweight='bold')\n\n# Plot 1: Training Loss\nif len(train_loss) > 0:\n    axes[0, 0].plot(train_loss, marker='o', markersize=3, linewidth=2, color='blue')\n    axes[0, 0].set_title('Training Loss Progression', fontweight='bold')\n    axes[0, 0].set_xlabel('Training Step')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Validation Loss\nif len(eval_loss) > 0:\n    axes[0, 1].plot(eval_loss, marker='s', markersize=3, linewidth=2, color='orange')\n    axes[0, 1].set_title('Validation Loss Progression', fontweight='bold')\n    axes[0, 1].set_xlabel('Evaluation Step')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Train vs Eval with Gap\nif len(eval_loss) > 0 and len(train_loss) > 0:\n    min_len = min(len(train_loss), len(eval_loss))\n    train_aligned = train_loss[-min_len:]\n    eval_aligned = eval_loss[-min_len:]\n    \n    axes[1, 0].plot(train_aligned, marker='o', label='Train Loss', linewidth=2)\n    axes[1, 0].plot(eval_aligned, marker='s', label='Eval Loss', linewidth=2)\n    axes[1, 0].fill_between(range(min_len), train_aligned, eval_aligned, alpha=0.2, color='red', label='Overfitting Gap')\n    axes[1, 0].set_title('Loss Gap: Train vs Eval', fontweight='bold')\n    axes[1, 0].set_xlabel('Step')\n    axes[1, 0].set_ylabel('Loss')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n\n# Plot 4: Overfitting Metrics Summary\nif len(eval_loss) > 0 and len(train_loss) > 0:\n    min_len = min(len(train_loss), len(eval_loss))\n    train_aligned = train_loss[-min_len:]\n    eval_aligned = eval_loss[-min_len:]\n    loss_gap = eval_aligned - train_aligned\n    \n    avg_gap = np.mean(loss_gap)\n    max_gap = np.max(loss_gap)\n    \n    if avg_gap < 0.01:\n        status = 'MINIMAL OVERFITTING'\n    elif avg_gap < 0.05:\n        status = 'MILD OVERFITTING'\n    else:\n        status = 'MODERATE-SEVERE OVERFITTING'\n    \n    metrics_text = f'OVERFITTING ANALYSIS\\n\\nAvg Loss Gap: {avg_gap:.6f}\\nMax Loss Gap: {max_gap:.6f}\\n\\nStatus: {status}\\n\\nTrain Loss: {train_aligned[-1]:.6f}\\nEval Loss: {eval_aligned[-1]:.6f}\\n\\nImprovement: {(1-eval_aligned[-1]/eval_aligned[0])*100:.1f}%'\n    \n    axes[1, 1].text(0.5, 0.5, metrics_text, ha='center', va='center', fontsize=10, family='monospace', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n    axes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.savefig('overfitting_analysis.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('‚úÖ Overfitting analysis saved to overfitting_analysis.png')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 15: üîç Detailed Overfitting Report"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["if len(eval_loss) > 0 and len(train_loss) > 0:\n    min_len = min(len(train_loss), len(eval_loss))\n    train_aligned = train_loss[-min_len:]\n    eval_aligned = eval_loss[-min_len:]\n    loss_gap = eval_aligned - train_aligned\n    \n    print('\\n' + '='*60)\n    print('üîç OVERFITTING DETECTION ANALYSIS')\n    print('='*60)\n    \n    print(f'\\nüìä Loss Gap Statistics:')\n    print(f'  Average Gap: {np.mean(loss_gap):.6f}')\n    print(f'  Max Gap: {np.max(loss_gap):.6f}')\n    print(f'  Min Gap: {np.min(loss_gap):.6f}')\n    \n    print(f'\\nüìà Performance Metrics:')\n    print(f'  Final Train Loss: {train_aligned[-1]:.6f}')\n    print(f'  Final Eval Loss: {eval_aligned[-1]:.6f}')\n    print(f'  Loss Improvement: {(1-eval_aligned[-1]/eval_aligned[0])*100:.1f}%')\n    \n    if np.mean(loss_gap) < 0.01:\n        status = 'üü¢ MINIMAL OVERFITTING (Excellent!)'\n    elif np.mean(loss_gap) < 0.05:\n        status = 'üü° MILD OVERFITTING (Good)'\n    else:\n        status = 'üî¥ MODERATE-SEVERE OVERFITTING'\n    \n    print(f'\\n‚úÖ Status: {status}')\n    print('='*60)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 16: Save Training Summary"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["summary = {\n    'model': 'google/medgemma-7b-orcamath-it',\n    'training_type': 'SFT (Supervised Fine-Tuning) with LoRA',\n    'lora_rank': 16,\n    'lora_alpha': 32,\n    'train_samples': len(train_df),\n    'val_samples': len(val_df),\n    'test_samples': len(test_df),\n    'epochs': 3,\n    'batch_size': 4,\n    'gradient_accumulation_steps': 2,\n    'learning_rate': 2e-4,\n    'optimizer': 'paged_adamw_8bit',\n    'max_steps': 500,\n    'quantization': '4-bit (nf4)',\n}\n\nif len(eval_loss) > 0 and len(train_loss) > 0:\n    min_len = min(len(train_loss), len(eval_loss))\n    train_aligned = train_loss[-min_len:]\n    eval_aligned = eval_loss[-min_len:]\n    loss_gap = eval_aligned - train_aligned\n    \n    summary.update({\n        'final_train_loss': float(train_aligned[-1]),\n        'final_eval_loss': float(eval_aligned[-1]),\n        'avg_loss_gap': float(np.mean(loss_gap)),\n        'max_loss_gap': float(np.max(loss_gap)),\n        'loss_improvement_percent': float((1-eval_aligned[-1]/eval_aligned[0])*100),\n        'overfitting_status': 'MINIMAL' if np.mean(loss_gap) < 0.01 else 'MILD' if np.mean(loss_gap) < 0.05 else 'MODERATE-SEVERE'\n    })\n\nwith open('training_summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint('‚úÖ Training Summary:')\nprint(json.dumps(summary, indent=2))"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 17: Test Inference"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Load best model for inference\nmodel.load_state_dict(torch.load('./medgemma_nail_disease_finetuned/adapter_model.bin', map_location=DEVICE))\n\ntest_prompt = \"\"\"Clinical Case Analysis:\n\nClinical Findings:\nWhite nail beds with dark edges, slight clubbing.\n\nPlease provide the diagnosis and treatment plan.\n\nExpected Output:\n\"\"\"\n\ninputs = tokenizer(test_prompt, return_tensors='pt').to(DEVICE)\noutputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, top_p=0.9)\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint('Test Inference:')\nprint('='*60)\nprint(result)\nprint('='*60)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 18: Complete! ‚úÖ"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["print('\\n' + '='*60)\nprint('‚úÖ FINE-TUNING & ANALYSIS COMPLETE!')\nprint('='*60)\nprint('\\nüìÅ Output Files:')\nprint('  ‚úÖ medgemma_nail_disease_finetuned/')\nprint('     - adapter_model.bin (LoRA weights)')\nprint('     - config.json')\nprint('     - tokenizer files')\nprint('  ‚úÖ overfitting_analysis.png (4-subplot visualization)')\nprint('  ‚úÖ training_summary.json (metrics & config)')\nprint('  ‚úÖ logs/ (tensorboard data)')\nprint('\\nüöÄ Next Steps:')\nprint('  1. Download files from Kaggle Output tab')\nprint('  2. Use for inference on new nail disease cases')\nprint('  3. Evaluate on real medical images')\nprint('\\nüìä Model Performance:')\nif len(eval_loss) > 0:\n    print(f'  Final Test Loss: {eval_loss[-1]:.4f}')\nprint('='*60)"]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}