{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8Yyi_TgzCLL"
      },
      "source": [
        "# Fine-tune MedGemma 4B for Nail Disease Clinical Explanations\n",
        "## Model 2: Clinical Findings ‚Üí Medical Explanations Pipeline\n",
        "\n",
        "**Pipeline Architecture:**\n",
        "- Stage 1 ‚úÖ DONE: MedSigLIP (Image Classification) ‚Üí \"Clubbing\" / \"Pitting\" etc.\n",
        "- Stage 2 ‚≠ê NOW: MedGemma 4B (Clinical Explanation) ‚Üí \"What does this mean?\"\n",
        "- Stage 3: MedGemma 27B (Disease Ranking) ‚Üí \"What diseases could cause this?\"\n",
        "\n",
        "Based on: https://github.com/google-health/medgemma\n",
        "Model: google/medgemma-4b-it (Lightweight, 4B params, instruction-tuned)\n",
        "License: Apache 2.0\n",
        "\n",
        "Features: Interactive HuggingFace auth | Medical prompt engineering | CSV integration | Missing value handling | Overfitting detection | 50% faster training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vVlUkIPzCLO"
      },
      "source": [
        "## Step 0: Suppress CUDA/cuDNN Warnings (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:47:52.516472Z",
          "iopub.status.busy": "2026-01-30T17:47:52.515757Z",
          "iopub.status.idle": "2026-01-30T17:47:52.525001Z",
          "shell.execute_reply": "2026-01-30T17:47:52.524343Z",
          "shell.execute_reply.started": "2026-01-30T17:47:52.51644Z"
        },
        "id": "I9bUeTmYzCLO",
        "outputId": "ecea9978-838f-4824-da30-83d91c895b6c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Warning filters applied - CUDA/cuDNN messages suppressed\n"
          ]
        }
      ],
      "source": [
        "# Suppress CUDA/cuDNN duplicate factory registration warnings\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress TensorFlow/XLA warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "os.environ['XLA_FLAGS'] = '--xla_gpu_deterministic_ops'\n",
        "\n",
        "# Suppress pydantic warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "import logging\n",
        "logging.getLogger('absl').setLevel(logging.ERROR)\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "logging.getLogger('transformers').setLevel(logging.WARNING)\n",
        "\n",
        "print('‚úÖ Warning filters applied - CUDA/cuDNN messages suppressed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwxdXtgrzCLQ"
      },
      "source": [
        "## Step 1: Setup Environment & Interactive HuggingFace Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:47:52.526396Z",
          "iopub.status.busy": "2026-01-30T17:47:52.52613Z",
          "iopub.status.idle": "2026-01-30T17:48:00.385334Z",
          "shell.execute_reply": "2026-01-30T17:48:00.384521Z",
          "shell.execute_reply.started": "2026-01-30T17:47:52.526374Z"
        },
        "id": "9ax_FIFbzCLQ",
        "outputId": "6fd0dc08-0aae-44d3-8790-5ae656e3428a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ENVIRONMENT SETUP\n",
            "============================================================\n",
            "Environment: Kaggle\n",
            "Device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory: 15.6 GB\n",
            "PyTorch: 2.8.0+cu126\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "IS_KAGGLE = os.path.exists('/kaggle')\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('='*60)\n",
        "print('ENVIRONMENT SETUP')\n",
        "print('='*60)\n",
        "print(f'Environment: {\"Kaggle\" if IS_KAGGLE else \"Local/Colab\"}')\n",
        "print(f'Device: {DEVICE}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
        "else:\n",
        "    print('GPU: None - CPU mode')\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:48:00.386877Z",
          "iopub.status.busy": "2026-01-30T17:48:00.386393Z",
          "iopub.status.idle": "2026-01-30T17:48:09.409342Z",
          "shell.execute_reply": "2026-01-30T17:48:09.408553Z",
          "shell.execute_reply.started": "2026-01-30T17:48:00.386855Z"
        },
        "id": "OzTMGLF6zCLR",
        "outputId": "13fcb7e3-391b-42af-c1f1-61940fe72fd6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Packages installed\n"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip install -q transformers datasets torch bitsandbytes peft trl scikit-learn matplotlib huggingface-hub\n",
        "print('‚úÖ Packages installed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsQOf2NAzCLS"
      },
      "source": [
        "## Step 1a: üîë Interactive HuggingFace Authentication Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f8bf654a82af474594c5f2730d59bb8f"
          ]
        },
        "execution": {
          "iopub.execute_input": "2026-01-30T17:50:02.066949Z",
          "iopub.status.busy": "2026-01-30T17:50:02.065927Z",
          "iopub.status.idle": "2026-01-30T17:50:02.502982Z",
          "shell.execute_reply": "2026-01-30T17:50:02.502184Z",
          "shell.execute_reply.started": "2026-01-30T17:50:02.066907Z"
        },
        "id": "9zB8aINSzCLS",
        "outputId": "1f18f28a-558a-4801-a699-31de9c2a0cde",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üîê HUGGING FACE LOGIN\n",
            "======================================================================\n",
            "\n",
            "You'll be prompted to enter your Hugging Face token.\n",
            "Get your token: https://huggingface.co/settings/tokens\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8bf654a82af474594c5f2730d59bb8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Login successful!\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üîê HUGGING FACE LOGIN\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nYou'll be prompted to enter your Hugging Face token.\")\n",
        "print(\"Get your token: https://huggingface.co/settings/tokens\\n\")\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "print(\"\\n‚úÖ Login successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTdD5UXuzCLT"
      },
      "source": [
        "## Step 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:50:22.010434Z",
          "iopub.status.busy": "2026-01-30T17:50:22.009711Z",
          "iopub.status.idle": "2026-01-30T17:51:04.272776Z",
          "shell.execute_reply": "2026-01-30T17:51:04.272019Z",
          "shell.execute_reply.started": "2026-01-30T17:50:22.010404Z"
        },
        "id": "d_Fs8ayEzCLT",
        "outputId": "8731be9d-2fb5-4d9e-ab55-911effd9254a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-30 17:50:39.207418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769795439.679879      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769795439.822274      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769795440.962512      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769795440.962550      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769795440.962552      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769795440.962555      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    set_seed\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "set_seed(42)\n",
        "print('‚úÖ Libraries imported')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-H2EXxpzCLU"
      },
      "source": [
        "## Step 3: Load & Explore Dataset (Model 2 Training Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:51:10.90108Z",
          "iopub.status.busy": "2026-01-30T17:51:10.900516Z",
          "iopub.status.idle": "2026-01-30T17:51:10.952649Z",
          "shell.execute_reply": "2026-01-30T17:51:10.952068Z",
          "shell.execute_reply.started": "2026-01-30T17:51:10.901054Z"
        },
        "id": "DjFe5X9DzCLV",
        "outputId": "0acd30b1-7d26-40be-82d0-90e77c449603",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found CSV at: /kaggle/input/nail-diseases/nail_diseases.csv\n",
            "\n",
            "‚úÖ Loaded 10000 samples from /kaggle/input/nail-diseases/nail_diseases.csv\n",
            "\n",
            "Dataset Shape: (10000, 15)\n",
            "\n",
            "Columns: ['nail_disease_category', 'model_1_predicted_disease', 'confirmed_diagnosis', 'patient_age', 'patient_sex', 'patient_ethnicity', 'fitzpatrick_skin_type', 'disease_severity', 'clinical_findings', 'differential_diagnoses', 'recommended_medical_tests', 'treatment_protocol', 'comorbidities', 'clinical_notes', 'prognosis']\n",
            "\n",
            "First row:\n",
            "nail_disease_category                                              Blue_Finger\n",
            "model_1_predicted_disease                                          Blue Finger\n",
            "confirmed_diagnosis                                                Blue Finger\n",
            "patient_age                                                                 43\n",
            "patient_sex                                                               Male\n",
            "patient_ethnicity                                                    Caucasian\n",
            "fitzpatrick_skin_type                                                        1\n",
            "disease_severity                                                        Severe\n",
            "clinical_findings            Transient blue discoloration; Blue-gray pigmen...\n",
            "differential_diagnoses                Acral Lentiginous Melanoma, Healthy Nail\n",
            "recommended_medical_tests     Pulse Oximetry, Arterial Blood Gas, Cardiac Echo\n",
            "treatment_protocol             Oxygen therapy, Treat underlying cause, Warming\n",
            "comorbidities                                                              NaN\n",
            "clinical_notes                          Pain upon pressure. Skin Type 1 noted.\n",
            "prognosis                                                   Dependent on cause\n",
            "Name: 0, dtype: object\n",
            "\n",
            "Data types:\n",
            "nail_disease_category        object\n",
            "model_1_predicted_disease    object\n",
            "confirmed_diagnosis          object\n",
            "patient_age                   int64\n",
            "patient_sex                  object\n",
            "patient_ethnicity            object\n",
            "fitzpatrick_skin_type         int64\n",
            "disease_severity             object\n",
            "clinical_findings            object\n",
            "differential_diagnoses       object\n",
            "recommended_medical_tests    object\n",
            "treatment_protocol           object\n",
            "comorbidities                object\n",
            "clinical_notes               object\n",
            "prognosis                    object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Load CSV dataset for Model 2: Clinical Explanation Stage\n",
        "csv_path = '/kaggle/input/nail-disease-medgemma/nail_diseases.csv'\n",
        "\n",
        "# Find CSV in various possible locations\n",
        "possible_paths = [\n",
        "    '/kaggle/input/nail-disease-medgemma/nail_diseases.csv',\n",
        "    '/kaggle/input/nail-disease-classification/nail_diseases.csv',\n",
        "    '/kaggle/input/nail-diseases/nail_diseases.csv',\n",
        "    './nail_diseases.csv'\n",
        "]\n",
        "\n",
        "df = None\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        csv_path = path\n",
        "        print(f'‚úÖ Found CSV at: {path}')\n",
        "        break\n",
        "\n",
        "if df is None:\n",
        "    print(f'‚ùå CSV file not found in standard locations')\n",
        "    print('\\nAvailable inputs:')\n",
        "    if IS_KAGGLE and os.path.exists('/kaggle/input'):\n",
        "        for item in os.listdir('/kaggle/input'):\n",
        "            print(f'  - {item}')\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f'\\n‚úÖ Loaded {len(df)} samples from {csv_path}')\n",
        "print(f'\\nDataset Shape: {df.shape}')\n",
        "print(f'\\nColumns: {list(df.columns)}')\n",
        "print(f'\\nFirst row:')\n",
        "print(df.iloc[0])\n",
        "print(f'\\nData types:')\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8YidBqvzCLV"
      },
      "source": [
        "## Step 4: Data Cleaning - Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:51:18.521145Z",
          "iopub.status.busy": "2026-01-30T17:51:18.520855Z",
          "iopub.status.idle": "2026-01-30T17:51:18.57607Z",
          "shell.execute_reply": "2026-01-30T17:51:18.575438Z",
          "shell.execute_reply.started": "2026-01-30T17:51:18.521124Z"
        },
        "id": "wdQXi40wzCLW",
        "outputId": "ff5f59f6-a49f-4cac-e164-473956576950",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ DATA CLEANING - MISSING VALUE HANDLING\n",
            "============================================================\n",
            "\n",
            "üìä Missing values BEFORE cleaning:\n",
            "comorbidities    3671\n",
            "dtype: int64\n",
            "\n",
            "üóëÔ∏è  Dropped 0 rows with missing critical fields\n",
            "  üìå Filled comorbidities with unknown\n",
            "\n",
            "‚úÖ No missing values remaining!\n",
            "\n",
            "üìä Dataset Summary:\n",
            "  Original rows: 10000\n",
            "  Clean rows: 10000\n",
            "  Removed: 0 (0.0%)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print('üßπ DATA CLEANING - MISSING VALUE HANDLING')\n",
        "print('='*60)\n",
        "\n",
        "# Show missing values before cleaning\n",
        "missing_before = df.isnull().sum()\n",
        "print('\\nüìä Missing values BEFORE cleaning:')\n",
        "print(missing_before[missing_before > 0])\n",
        "\n",
        "# Create a copy for cleaning\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Strategy 1: Drop rows where critical columns are missing\n",
        "critical_cols = ['nail_disease', 'disease_name', 'clinical_findings', 'findings']\n",
        "critical_cols_present = [col for col in critical_cols if col in df_clean.columns]\n",
        "\n",
        "if critical_cols_present:\n",
        "    initial_len = len(df_clean)\n",
        "    df_clean = df_clean.dropna(subset=critical_cols_present)\n",
        "    dropped_critical = initial_len - len(df_clean)\n",
        "    print(f'\\nüóëÔ∏è  Dropped {dropped_critical} rows with missing critical fields')\n",
        "\n",
        "# Strategy 2: Fill missing values in non-critical columns\n",
        "non_critical_cols = ['comorbidities', 'patient_age', 'age', 'patient_sex', 'sex', 'gender']\n",
        "for col in df_clean.columns:\n",
        "    if col in non_critical_cols and df_clean[col].isnull().sum() > 0:\n",
        "        if df_clean[col].dtype in ['float64', 'int64']:\n",
        "            # Fill numeric columns with median\n",
        "            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
        "            print(f'  üìå Filled {col} with median value')\n",
        "        else:\n",
        "            # Fill string columns with unknown\n",
        "            df_clean[col].fillna('unknown', inplace=True)\n",
        "            print(f'  üìå Filled {col} with unknown')\n",
        "\n",
        "# Strategy 3: Drop rows with ANY remaining NaN\n",
        "initial_len = len(df_clean)\n",
        "df_clean = df_clean.dropna()\n",
        "dropped_final = initial_len - len(df_clean)\n",
        "if dropped_final > 0:\n",
        "    print(f'\\nüóëÔ∏è  Dropped {dropped_final} additional rows with remaining NaN values')\n",
        "\n",
        "# Show missing values after cleaning\n",
        "missing_after = df_clean.isnull().sum()\n",
        "if missing_after.sum() == 0:\n",
        "    print('\\n‚úÖ No missing values remaining!')\n",
        "else:\n",
        "    print('\\n‚ö†Ô∏è  Remaining missing values:')\n",
        "    print(missing_after[missing_after > 0])\n",
        "\n",
        "print(f'\\nüìä Dataset Summary:')\n",
        "print(f'  Original rows: {len(df)}')\n",
        "print(f'  Clean rows: {len(df_clean)}')\n",
        "print(f'  Removed: {len(df) - len(df_clean)} ({(len(df) - len(df_clean))/len(df)*100:.1f}%)')\n",
        "print('='*60)\n",
        "\n",
        "# Use cleaned dataset\n",
        "df = df_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZGK7PZ9zCLW"
      },
      "source": [
        "## Step 5: Enhanced Medical Prompt Templates for Clinical Explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:51:24.680486Z",
          "iopub.status.busy": "2026-01-30T17:51:24.680109Z",
          "iopub.status.idle": "2026-01-30T17:51:25.281421Z",
          "shell.execute_reply": "2026-01-30T17:51:25.280761Z",
          "shell.execute_reply.started": "2026-01-30T17:51:24.68046Z"
        },
        "id": "94u8WDKqzCLW",
        "outputId": "84a924b1-5e72-45a3-bbac-47dd34b6263e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created 10000 medical prompts for Model 2 training\n",
            "\n",
            "Example prompt (first 500 chars):\n",
            "============================================================\n",
            "CLINICAL ANALYSIS: Nail Disease Diagnosis\n",
            "\n",
            "PATIENT DEMOGRAPHICS:\n",
            "Age: 43\n",
            "Sex: Male\n",
            "Comorbidities: unknown\n",
            "\n",
            "PRIMARY FINDING (from Model 1 - MedSigLIP):\n",
            "unknown\n",
            "\n",
            "CLINICAL PRESENTATION:\n",
            "Transient blue discoloration; Blue-gray pigmentation\n",
            "\n",
            "INSTRUCTION:\n",
            "Based on the nail disease finding and clinical presentation above, provide:\n",
            "1. Detailed explanation of what the nail finding indicates\n",
            "2. Possible systemic diseases that could cause this nail finding\n",
            "3. Recommended diagnostic workup and treatment app\n",
            "============================================================\n",
            "\n",
            "Average prompt length: 727 chars\n",
            "Max prompt length: 774 chars\n"
          ]
        }
      ],
      "source": [
        "def create_medical_prompt_model2(row):\n",
        "    \"\"\"\n",
        "    Creates advanced medical prompts for MedGemma Model 2 (Clinical Explanation Stage).\n",
        "\n",
        "    Input: Nail disease classification from Model 1 + clinical findings\n",
        "    Output: Detailed clinical explanation of findings, differential diagnoses, and systemic implications\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract fields (handle missing/None values)\n",
        "    nail_disease = str(row.get('nail_disease', row.get('disease_name', 'unknown'))).strip()\n",
        "    clinical_findings = str(row.get('clinical_findings', row.get('findings', 'no findings reported'))).strip()\n",
        "    patient_age = row.get('patient_age', row.get('age', 'unknown'))\n",
        "    patient_sex = str(row.get('patient_sex', row.get('sex', row.get('gender', 'unknown')))).strip()\n",
        "    differential_diagnoses = str(row.get('differential_diagnoses', row.get('differentials', 'pending investigation'))).strip()\n",
        "    systemic_implications = str(row.get('systemic_implications', row.get('implications', 'requires clinical assessment'))).strip()\n",
        "    treatment_protocol = str(row.get('treatment_protocol', row.get('treatment', 'refer to specialist'))).strip()\n",
        "    comorbidities = str(row.get('comorbidities', 'none reported')).strip()\n",
        "\n",
        "    # Build instruction-following prompt (Orca format)\n",
        "    prompt = f\"\"\"CLINICAL ANALYSIS: Nail Disease Diagnosis\n",
        "\n",
        "PATIENT DEMOGRAPHICS:\n",
        "Age: {patient_age}\n",
        "Sex: {patient_sex}\n",
        "Comorbidities: {comorbidities}\n",
        "\n",
        "PRIMARY FINDING (from Model 1 - MedSigLIP):\n",
        "{nail_disease}\n",
        "\n",
        "CLINICAL PRESENTATION:\n",
        "{clinical_findings}\n",
        "\n",
        "INSTRUCTION:\n",
        "Based on the nail disease finding and clinical presentation above, provide:\n",
        "1. Detailed explanation of what the nail finding indicates\n",
        "2. Possible systemic diseases that could cause this nail finding\n",
        "3. Recommended diagnostic workup and treatment approach\n",
        "\n",
        "EXPECTED RESPONSE:\n",
        "Nail Finding Explanation: {nail_disease} indicates {systemic_implications}\n",
        "\n",
        "Differential Diagnoses: {differential_diagnoses}\n",
        "\n",
        "Recommended Treatment: {treatment_protocol}\n",
        "\"\"\"\n",
        "\n",
        "    return prompt.strip()\n",
        "\n",
        "# Apply prompt template to dataset\n",
        "df['text'] = df.apply(create_medical_prompt_model2, axis=1)\n",
        "\n",
        "print(f'‚úÖ Created {len(df)} medical prompts for Model 2 training')\n",
        "print(f'\\nExample prompt (first 500 chars):')\n",
        "print('='*60)\n",
        "print(df['text'].iloc[0][:500])\n",
        "print('='*60)\n",
        "print(f'\\nAverage prompt length: {df[\"text\"].str.len().mean():.0f} chars')\n",
        "print(f'Max prompt length: {df[\"text\"].str.len().max():.0f} chars')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdDtUZvyzCLX"
      },
      "source": [
        "## Step 6: Data Quality & Validation Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:51:28.528008Z",
          "iopub.status.busy": "2026-01-30T17:51:28.527712Z",
          "iopub.status.idle": "2026-01-30T17:51:28.546413Z",
          "shell.execute_reply": "2026-01-30T17:51:28.545671Z",
          "shell.execute_reply.started": "2026-01-30T17:51:28.527984Z"
        },
        "id": "vp2sXTOrzCLX",
        "outputId": "b905fb91-e06a-4e5e-d69c-7abd608967fe",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä DATA QUALITY REPORT\n",
            "============================================================\n",
            "\n",
            "‚úÖ No missing values\n",
            "\n",
            "‚úÖ All prompts have sufficient length (10000 samples)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values and data quality\n",
        "print('üìä DATA QUALITY REPORT')\n",
        "print('='*60)\n",
        "\n",
        "# Missing values\n",
        "missing = df.isnull().sum()\n",
        "if missing.sum() > 0:\n",
        "    print('\\n‚ö†Ô∏è  Missing values detected:')\n",
        "    print(missing[missing > 0])\n",
        "else:\n",
        "    print('\\n‚úÖ No missing values')\n",
        "\n",
        "# Check text field quality\n",
        "empty_texts = (df['text'].str.len() < 50).sum()\n",
        "if empty_texts > 0:\n",
        "    print(f'\\n‚ö†Ô∏è  {empty_texts} prompts are too short (<50 chars)')\n",
        "else:\n",
        "    print(f'\\n‚úÖ All prompts have sufficient length ({len(df)} samples)')\n",
        "\n",
        "# Disease distribution\n",
        "if 'nail_disease' in df.columns:\n",
        "    print(f'\\nüìã Disease Distribution:')\n",
        "    print(df['nail_disease'].value_counts())\n",
        "elif 'disease_name' in df.columns:\n",
        "    print(f'\\nüìã Disease Distribution:')\n",
        "    print(df['disease_name'].value_counts())\n",
        "\n",
        "print('\\n' + '='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEtWL3pUzCLY"
      },
      "source": [
        "## Step 7: Split Dataset (Train/Val/Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:51:31.691905Z",
          "iopub.status.busy": "2026-01-30T17:51:31.6916Z",
          "iopub.status.idle": "2026-01-30T17:51:31.705566Z",
          "shell.execute_reply": "2026-01-30T17:51:31.704963Z",
          "shell.execute_reply.started": "2026-01-30T17:51:31.691882Z"
        },
        "id": "0OvOOflFzCLY",
        "outputId": "382dfaa5-2dcf-4011-aca0-8b69e9314810",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä DATASET SPLIT\n",
            "============================================================\n",
            "Train: 7000 samples (70.0%)\n",
            "Val:   1500 samples (15.0%)\n",
            "Test:  1500 samples (15.0%)\n",
            "Total: 10000 samples\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Stratified split: 70% train, 15% val, 15% test\n",
        "split_key = None\n",
        "if 'nail_disease' in df.columns:\n",
        "    split_key = 'nail_disease'\n",
        "elif 'disease_name' in df.columns:\n",
        "    split_key = 'disease_name'\n",
        "\n",
        "train_df, temp_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=df[split_key] if split_key else None\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=temp_df[split_key] if split_key else None\n",
        ")\n",
        "\n",
        "print('üìä DATASET SPLIT')\n",
        "print('='*60)\n",
        "print(f'Train: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)')\n",
        "print(f'Val:   {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)')\n",
        "print(f'Test:  {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)')\n",
        "print(f'Total: {len(df)} samples')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sQTfffMzCLY"
      },
      "source": [
        "## Step 8: Create HuggingFace Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:51:34.653827Z",
          "iopub.status.busy": "2026-01-30T17:51:34.653494Z",
          "iopub.status.idle": "2026-01-30T17:51:34.715448Z",
          "shell.execute_reply": "2026-01-30T17:51:34.714661Z",
          "shell.execute_reply.started": "2026-01-30T17:51:34.653803Z"
        },
        "id": "il3Wf9T-zCLZ",
        "outputId": "45b93755-0c5d-4328-aeb1-9a52fc9ea30d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ HuggingFace datasets created\n",
            "  Train: 7000 samples\n",
            "  Val:   1500 samples\n",
            "  Test:  1500 samples\n"
          ]
        }
      ],
      "source": [
        "# Create HuggingFace datasets\n",
        "train_dataset = Dataset.from_pandas(train_df[['text']])\n",
        "val_dataset = Dataset.from_pandas(val_df[['text']])\n",
        "test_dataset = Dataset.from_pandas(test_df[['text']])\n",
        "\n",
        "print('‚úÖ HuggingFace datasets created')\n",
        "print(f'  Train: {len(train_dataset)} samples')\n",
        "print(f'  Val:   {len(val_dataset)} samples')\n",
        "print(f'  Test:  {len(test_dataset)} samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMWaVY2mzCLZ"
      },
      "source": [
        "## Step 9: Setup Model & Tokenizer (MedGemma 4B - Lightweight & Fast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7c2c1ff0f5ff4077bd82a1149ce1a3b0",
            "f016eae2e15f4667b3e2876367e9b2e4",
            "3141f17d9c044fe5a004e5eace3a2ab2",
            "04ca908e0b5248288e73c40664a14fd3",
            "0da07aa6e654465badfafb14b91e9465",
            "dec7886526384ff9af5e77777d0ff196",
            "61fe0497973c4dbb960d4ca15359f0af",
            "1b608c77d3bb42929fa6b46fb6b6bf26",
            "cc3a16f6608249e2b4c28c6c20f05f2c",
            "dc39178a489946ac97c1d68c54ecb029",
            "14ebaaa37a644add921575e7f694c16a",
            "23f1c59131b446a1835429fcf3371195",
            "579ccae1051546eeae1272764bdf56dd"
          ]
        },
        "execution": {
          "iopub.execute_input": "2026-01-30T17:51:37.877397Z",
          "iopub.status.busy": "2026-01-30T17:51:37.877054Z",
          "iopub.status.idle": "2026-01-30T17:52:56.40202Z",
          "shell.execute_reply": "2026-01-30T17:52:56.401342Z",
          "shell.execute_reply.started": "2026-01-30T17:51:37.877372Z"
        },
        "id": "vWgSLsQvzCLZ",
        "outputId": "673bd355-14e8-46ab-e4c2-175278b503d0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Loading model: google/medgemma-4b-it\n",
            "This may take 1-2 minutes (4B is faster than 7B)...\n",
            "============================================================\n",
            "üí° Model Info:\n",
            "  - Size: 4B parameters (50% smaller than 7B)\n",
            "  - Speed: ~2x faster training\n",
            "  - Quality: Excellent medical understanding\n",
            "  - Memory: Fits in most GPUs (8GB+)\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c2c1ff0f5ff4077bd82a1149ce1a3b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f016eae2e15f4667b3e2876367e9b2e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3141f17d9c044fe5a004e5eace3a2ab2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04ca908e0b5248288e73c40664a14fd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0da07aa6e654465badfafb14b91e9465",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dec7886526384ff9af5e77777d0ff196",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61fe0497973c4dbb960d4ca15359f0af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b608c77d3bb42929fa6b46fb6b6bf26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc3a16f6608249e2b4c28c6c20f05f2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc39178a489946ac97c1d68c54ecb029",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14ebaaa37a644add921575e7f694c16a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23f1c59131b446a1835429fcf3371195",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "579ccae1051546eeae1272764bdf56dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model loaded successfully!\n",
            "   Model: MedGemma 4B (Lightweight)\n",
            "   Size: 2.49B parameters\n",
            "   Memory: ~8GB GPU VRAM (4-bit quantized)\n",
            "   Expected Training Time: 15-30 minutes\n"
          ]
        }
      ],
      "source": [
        "# Model configuration: Using MedGemma 4B for FASTER TRAINING\n",
        "# 4B is 50% faster than 7B with similar medical understanding\n",
        "MODEL_ID = 'google/medgemma-4b-it'\n",
        "# Alternative (larger, slower): MODEL_ID = 'google/medgemma-7b-orcamath-it'\n",
        "\n",
        "# 4-bit quantization config (memory efficient)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print('='*60)\n",
        "print(f'Loading model: {MODEL_ID}')\n",
        "print('This may take 1-2 minutes (4B is faster than 7B)...')\n",
        "print('='*60)\n",
        "print('üí° Model Info:')\n",
        "print('  - Size: 4B parameters (50% smaller than 7B)')\n",
        "print('  - Speed: ~2x faster training')\n",
        "print('  - Quality: Excellent medical understanding')\n",
        "print('  - Memory: Fits in most GPUs (8GB+)')\n",
        "print()\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "\n",
        "try:\n",
        "    # Try with HuggingFace token authentication\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map='auto',\n",
        "        trust_remote_code=True,\n",
        "        use_auth_token=True,  # Enable auth token\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        trust_remote_code=True,\n",
        "        use_auth_token=True\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(f'‚úÖ Model loaded successfully!')\n",
        "    print(f'   Model: MedGemma 4B (Lightweight)')\n",
        "    print(f'   Size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters')\n",
        "    print(f'   Memory: ~8GB GPU VRAM (4-bit quantized)')\n",
        "    print(f'   Expected Training Time: 15-30 minutes')\n",
        "\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    print(f'\\n‚ùå Error loading model: {error_msg[:200]}')\n",
        "    print('\\nüîß TROUBLESHOOTING:')\n",
        "    print('\\n1. ACCEPT MODEL LICENSE:')\n",
        "    print(f'   - Visit: https://huggingface.co/{MODEL_ID}')\n",
        "    print('   - Click \"Accept\" button')\n",
        "    print('\\n2. LOGIN TO HUGGINGFACE:')\n",
        "    print('   - Run setup_huggingface_auth() in Step 1a again')\n",
        "    print('   - Or manually: from huggingface_hub import login')\n",
        "    print('   - Then: login(token=\"hf_YOUR_TOKEN\")')\n",
        "    print('\\n3. CHECK TOKEN VALIDITY:')\n",
        "    print('   - Visit: https://huggingface.co/settings/tokens')\n",
        "    print('   - Ensure your token has \"Read\" access')\n",
        "    print('\\n4. ENVIRONMENT VARIABLES:')\n",
        "    print('   - Set: export HF_TOKEN=\"hf_YOUR_TOKEN\"')\n",
        "    print('\\n5. OFFLINE MODE:')\n",
        "    print('   - Download model locally first')\n",
        "    print('   - Use: AutoModel.from_pretrained(\"./local/path\")')\n",
        "    print('\\n' + '='*60)\n",
        "    sys.exit(1)\n",
        "\n",
        "if model is None or tokenizer is None:\n",
        "    print('‚ùå Model or tokenizer failed to load!')\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crg4aTrUzCLZ"
      },
      "source": [
        "## Step 10: Configure LoRA (Low-Rank Adaptation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:53:01.415477Z",
          "iopub.status.busy": "2026-01-30T17:53:01.414676Z",
          "iopub.status.idle": "2026-01-30T17:53:01.774831Z",
          "shell.execute_reply": "2026-01-30T17:53:01.774044Z",
          "shell.execute_reply.started": "2026-01-30T17:53:01.415447Z"
        },
        "id": "GDntDsvazCLa",
        "outputId": "e32afdfa-88fa-4c5b-b530-565a421b458e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LoRA configured for 4B model\n",
            "  Total params: 2.50B\n",
            "  Trainable: 9.39M (0.376%)\n"
          ]
        }
      ],
      "source": [
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank\n",
        "    lora_alpha=32,  # Alpha scaling\n",
        "    target_modules=['q_proj', 'v_proj', 'k_proj'],  # Query, Value, Key projections\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM'\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Count trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f'‚úÖ LoRA configured for 4B model')\n",
        "print(f'  Total params: {total_params / 1e9:.2f}B')\n",
        "print(f'  Trainable: {trainable_params / 1e6:.2f}M ({100*trainable_params/total_params:.3f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB5Z8lSwzCLa"
      },
      "source": [
        "## Step 11: Setup Training Configuration (Optimized for 4B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-30T17:57:15.338316Z",
          "iopub.status.busy": "2026-01-30T17:57:15.337751Z",
          "iopub.status.idle": "2026-01-30T17:57:15.378753Z",
          "shell.execute_reply": "2026-01-30T17:57:15.378165Z",
          "shell.execute_reply.started": "2026-01-30T17:57:15.338268Z"
        },
        "id": "YEG6hotCzCLa",
        "outputId": "493920d0-c678-4dd0-beeb-4166f4c35a65",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training configuration ready (4B optimized)\n",
            "  Output: ./medgemma_nail_disease_model2_finetuned\n",
            "  Epochs: 3\n",
            "  Batch size: 8 (increased for 4B)\n",
            "  Learning rate: 0.0002\n",
            "  Max steps: 500\n",
            "  Expected time: 15-30 minutes on single GPU\n"
          ]
        }
      ],
      "source": [
        "# Training arguments (optimized for MedGemma 4B - FASTER!)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./medgemma_nail_disease_model2_finetuned',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    max_steps=500,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=50,\n",
        "    eval_strategy='steps',\n",
        "    save_strategy='steps',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    greater_is_better=False,\n",
        "    logging_dir='./logs',\n",
        "    optim='paged_adamw_8bit',\n",
        "    seed=42,\n",
        "    dataloader_pin_memory=True,\n",
        ")\n",
        "\n",
        "print('‚úÖ Training configuration ready (4B optimized)')\n",
        "print(f'  Output: ./medgemma_nail_disease_model2_finetuned')\n",
        "print(f'  Epochs: {training_args.num_train_epochs}')\n",
        "print(f'  Batch size: {training_args.per_device_train_batch_size} (increased for 4B)')\n",
        "print(f'  Learning rate: {training_args.learning_rate}')\n",
        "print(f'  Max steps: {training_args.max_steps}')\n",
        "print(f'  Expected time: 15-30 minutes on single GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STXxXY8azCLa"
      },
      "source": [
        "## Step 12: Initialize SFT Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9a7021eb97ac462e85a4d9f01b5207a4",
            "6419b75f014f402388a771fc4363d3b8",
            "4ede6d9f559b46f88bb5f4d33bf335b7",
            "fcd96773cfaf413b8e2ae35915060e0f",
            "2584a35fa26740abb36f1cc2dbe96094",
            "5463e53a51024bf88bf91fa03654fb00"
          ]
        },
        "execution": {
          "iopub.execute_input": "2026-01-30T17:58:39.017626Z",
          "iopub.status.busy": "2026-01-30T17:58:39.016897Z",
          "iopub.status.idle": "2026-01-30T17:58:49.988358Z",
          "shell.execute_reply": "2026-01-30T17:58:49.987755Z",
          "shell.execute_reply.started": "2026-01-30T17:58:39.017595Z"
        },
        "id": "ToZHecgszCLb",
        "outputId": "74a1ca7d-4d19-40a8-aa51-79444e5b7719",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a7021eb97ac462e85a4d9f01b5207a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/7000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6419b75f014f402388a771fc4363d3b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/7000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ede6d9f559b46f88bb5f4d33bf335b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/7000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcd96773cfaf413b8e2ae35915060e0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to eval dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2584a35fa26740abb36f1cc2dbe96094",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5463e53a51024bf88bf91fa03654fb00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Trainer initialized (4B model)\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "print('‚úÖ Trainer initialized (4B model)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx4j6fdUzCLb"
      },
      "source": [
        "## Step 13: üöÄ START TRAINING (15-30 minutes with 4B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KewC47uGzCLb",
        "outputId": "566dbe41-a3f5-41e8-8939-2694683c5c75",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üöÄ STARTING MODEL 2 TRAINING (MedGemma 4B)\n",
            "Stage: Clinical Explanation Fine-tuning\n",
            "Expected Duration: 15-30 minutes\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print('\\n' + '='*60)\n",
        "print('üöÄ STARTING MODEL 2 TRAINING (MedGemma 4B)')\n",
        "print('Stage: Clinical Explanation Fine-tuning')\n",
        "print('Expected Duration: 15-30 minutes')\n",
        "print('='*60)\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('‚úÖ TRAINING COMPLETE')\n",
        "print(f'Final Training Loss: {train_result.training_loss:.4f}')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXBwNCshzCLb"
      },
      "source": [
        "## Step 14: Evaluate & Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "execution_failed": "2026-01-31T02:04:50.897Z"
        },
        "id": "Ou_qlVw3zCLc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "eval_results = trainer.evaluate(test_dataset)\n",
        "print(f'Test Loss: {eval_results.get(\"eval_loss\", 0):.4f}')\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained('./medgemma_nail_disease_model2_finetuned')\n",
        "tokenizer.save_pretrained('./medgemma_nail_disease_model2_finetuned')\n",
        "print('\\n‚úÖ Model saved to ./medgemma_nail_disease_model2_finetuned')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs0_YvqszCLc"
      },
      "source": [
        "## Step 15: Extract & Visualize Training Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "execution_failed": "2026-01-31T02:04:50.898Z"
        },
        "id": "0Vmv7IgyzCLc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "history = {'train_loss': [], 'eval_loss': []}\n",
        "\n",
        "try:\n",
        "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "    if os.path.exists('./logs'):\n",
        "        for file in sorted(os.listdir('./logs')):\n",
        "            if 'events.out.tfevents' in file:\n",
        "                ea = EventAccumulator(os.path.join('./logs', file))\n",
        "                ea.Reload()\n",
        "                for tag in ea.Tags().get('scalars', []):\n",
        "                    events = ea.Scalars(tag)\n",
        "                    for e in events:\n",
        "                        if 'eval' in tag and 'loss' in tag:\n",
        "                            history['eval_loss'].append(e.value)\n",
        "                        elif 'loss' in tag and 'eval' not in tag:\n",
        "                            history['train_loss'].append(e.value)\n",
        "except Exception as e:\n",
        "    print(f'Note: Could not extract tensorboard data: {str(e)[:50]}')\n",
        "\n",
        "print(f'Extracted: {len(history[\"train_loss\"])} train steps, {len(history[\"eval_loss\"])} eval steps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpBm6ND4zCLc"
      },
      "source": [
        "## Step 16: üìä Plot Loss Curves & Overfitting Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "execution_failed": "2026-01-31T02:04:50.898Z"
        },
        "id": "0TZTWsADzCLk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_loss = np.array(history['train_loss']) if history['train_loss'] else np.array([])\n",
        "eval_loss = np.array(history['eval_loss']) if history['eval_loss'] else np.array([])\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('MedGemma 4B Model 2: Training Metrics & Overfitting Detection', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Plot 1: Training Loss\n",
        "if len(train_loss) > 0:\n",
        "    axes[0, 0].plot(train_loss, marker='o', markersize=3, linewidth=2, color='blue')\n",
        "    axes[0, 0].set_title('Training Loss Progression', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Training Step')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Validation Loss\n",
        "if len(eval_loss) > 0:\n",
        "    axes[0, 1].plot(eval_loss, marker='s', markersize=3, linewidth=2, color='orange')\n",
        "    axes[0, 1].set_title('Validation Loss Progression', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Evaluation Step')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Train vs Eval with Gap\n",
        "if len(eval_loss) > 0 and len(train_loss) > 0:\n",
        "    min_len = min(len(train_loss), len(eval_loss))\n",
        "    train_aligned = train_loss[-min_len:]\n",
        "    eval_aligned = eval_loss[-min_len:]\n",
        "\n",
        "    axes[1, 0].plot(train_aligned, marker='o', label='Train Loss', linewidth=2)\n",
        "    axes[1, 0].plot(eval_aligned, marker='s', label='Eval Loss', linewidth=2)\n",
        "    axes[1, 0].fill_between(range(min_len), train_aligned, eval_aligned, alpha=0.2, color='red', label='Overfitting Gap')\n",
        "    axes[1, 0].set_title('Loss Gap: Train vs Eval', fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Step')\n",
        "    axes[1, 0].set_ylabel('Loss')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Overfitting Metrics Summary\n",
        "if len(eval_loss) > 0 and len(train_loss) > 0:\n",
        "    min_len = min(len(train_loss), len(eval_loss))\n",
        "    train_aligned = train_loss[-min_len:]\n",
        "    eval_aligned = eval_loss[-min_len:]\n",
        "    loss_gap = eval_aligned - train_aligned\n",
        "\n",
        "    avg_gap = np.mean(loss_gap)\n",
        "    max_gap = np.max(loss_gap)\n",
        "\n",
        "    if avg_gap < 0.01:\n",
        "        status = 'MINIMAL OVERFITTING'\n",
        "    elif avg_gap < 0.05:\n",
        "        status = 'MILD OVERFITTING'\n",
        "    else:\n",
        "        status = 'MODERATE-SEVERE OVERFITTING'\n",
        "\n",
        "    metrics_text = f'OVERFITTING ANALYSIS\\n\\nAvg Loss Gap: {avg_gap:.6f}\\nMax Loss Gap: {max_gap:.6f}\\n\\nStatus: {status}\\n\\nTrain Loss: {train_aligned[-1]:.6f}\\nEval Loss: {eval_aligned[-1]:.6f}\\n\\nImprovement: {(1-eval_aligned[-1]/eval_aligned[0])*100:.1f}%'\n",
        "\n",
        "    axes[1, 1].text(0.5, 0.5, metrics_text, ha='center', va='center', fontsize=10, family='monospace', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model2_overfitting_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('‚úÖ Overfitting analysis saved to model2_overfitting_analysis.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPeb7jdgzCLl"
      },
      "source": [
        "## Step 17: üîç Detailed Overfitting Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "execution_failed": "2026-01-31T02:04:50.898Z"
        },
        "id": "MZf7MLn9zCLl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if len(eval_loss) > 0 and len(train_loss) > 0:\n",
        "    min_len = min(len(train_loss), len(eval_loss))\n",
        "    train_aligned = train_loss[-min_len:]\n",
        "    eval_aligned = eval_loss[-min_len:]\n",
        "    loss_gap = eval_aligned - train_aligned\n",
        "\n",
        "    print('\\n' + '='*60)\n",
        "    print('üîç OVERFITTING DETECTION ANALYSIS')\n",
        "    print('='*60)\n",
        "\n",
        "    print(f'\\nüìä Loss Gap Statistics:')\n",
        "    print(f'  Average Gap: {np.mean(loss_gap):.6f}')\n",
        "    print(f'  Max Gap: {np.max(loss_gap):.6f}')\n",
        "    print(f'  Min Gap: {np.min(loss_gap):.6f}')\n",
        "\n",
        "    print(f'\\nüìà Performance Metrics:')\n",
        "    print(f'  Final Train Loss: {train_aligned[-1]:.6f}')\n",
        "    print(f'  Final Eval Loss: {eval_aligned[-1]:.6f}')\n",
        "    print(f'  Loss Improvement: {(1-eval_aligned[-1]/eval_aligned[0])*100:.1f}%')\n",
        "\n",
        "    if np.mean(loss_gap) < 0.01:\n",
        "        status = 'üü¢ MINIMAL OVERFITTING (Excellent!)'\n",
        "    elif np.mean(loss_gap) < 0.05:\n",
        "        status = 'üü° MILD OVERFITTING (Good)'\n",
        "    else:\n",
        "        status = 'üî¥ MODERATE-SEVERE OVERFITTING'\n",
        "\n",
        "    print(f'\\n‚úÖ Status: {status}')\n",
        "    print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWlSo9-hzCLl"
      },
      "source": [
        "## Step 18: Save Training Summary & Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "execution_failed": "2026-01-31T02:04:50.898Z"
        },
        "id": "oFMbIPx-zCLl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "summary = {\n",
        "    'pipeline_stage': 'Model 2 - Clinical Explanation',\n",
        "    'model': 'google/medgemma-4b-it',\n",
        "    'model_size': '4B (Lightweight)',\n",
        "    'training_type': 'SFT (Supervised Fine-Tuning) with LoRA',\n",
        "    'lora_rank': 16,\n",
        "    'lora_alpha': 32,\n",
        "    'target_modules': ['q_proj', 'v_proj', 'k_proj'],\n",
        "    'train_samples': len(train_df),\n",
        "    'val_samples': len(val_df),\n",
        "    'test_samples': len(test_df),\n",
        "    'epochs': 3,\n",
        "    'batch_size': 8,\n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'learning_rate': 2e-4,\n",
        "    'optimizer': 'paged_adamw_8bit',\n",
        "    'max_steps': 500,\n",
        "    'quantization': '4-bit (nf4)',\n",
        "    'training_speed': '~2x faster than 7B',\n",
        "    'dataset_source': csv_path,\n",
        "}\n",
        "\n",
        "if len(eval_loss) > 0 and len(train_loss) > 0:\n",
        "    min_len = min(len(train_loss), len(eval_loss))\n",
        "    train_aligned = train_loss[-min_len:]\n",
        "    eval_aligned = eval_loss[-min_len:]\n",
        "    loss_gap = eval_aligned - train_aligned\n",
        "\n",
        "    summary.update({\n",
        "        'final_train_loss': float(train_aligned[-1]),\n",
        "        'final_eval_loss': float(eval_aligned[-1]),\n",
        "        'avg_loss_gap': float(np.mean(loss_gap)),\n",
        "        'max_loss_gap': float(np.max(loss_gap)),\n",
        "        'loss_improvement_percent': float((1-eval_aligned[-1]/eval_aligned[0])*100),\n",
        "        'overfitting_status': 'MINIMAL' if np.mean(loss_gap) < 0.01 else 'MILD' if np.mean(loss_gap) < 0.05 else 'MODERATE-SEVERE'\n",
        "    })\n",
        "\n",
        "with open('model2_training_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print('‚úÖ Training Summary:')\n",
        "print(json.dumps(summary, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t0V2sIXzCLm"
      },
      "source": [
        "## Step 19: Test Inference with Clinical Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "execution_failed": "2026-01-31T02:04:50.898Z"
        },
        "id": "Ie3t0bQozCLm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load best model for inference\n",
        "if os.path.exists('./medgemma_nail_disease_model2_finetuned/adapter_model.bin'):\n",
        "    model.load_state_dict(torch.load('./medgemma_nail_disease_model2_finetuned/adapter_model.bin', map_location=DEVICE))\n",
        "\n",
        "# Test with clinical examples\n",
        "test_cases = [\n",
        "    \"\"\"CLINICAL ANALYSIS: Nail Disease Diagnosis\n",
        "\n",
        "PATIENT DEMOGRAPHICS:\n",
        "Age: 65\n",
        "Sex: Female\n",
        "\n",
        "PRIMARY FINDING (from Model 1 - MedSigLIP):\n",
        "Clubbing\n",
        "\n",
        "CLINICAL PRESENTATION:\n",
        "Convex nail beds, increased angle between nail and cuticle, bulbous fingertips. Patient has chronic cough and dyspnea.\n",
        "\n",
        "INSTRUCTION:\n",
        "Based on the nail disease finding and clinical presentation above, provide:\n",
        "1. Detailed explanation of what the nail finding indicates\n",
        "2. Possible systemic diseases that could cause this nail finding\n",
        "3. Recommended diagnostic workup and treatment approach\n",
        "\n",
        "EXPECTED RESPONSE:\n",
        "\"\"\"\n",
        "]\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('üîç TEST INFERENCE: Clinical Explanation (4B Model)')\n",
        "print('='*60)\n",
        "\n",
        "for i, test_prompt in enumerate(test_cases, 1):\n",
        "    print(f'\\nTest Case {i}:')\n",
        "    print('-'*60)\n",
        "\n",
        "    inputs = tokenizer(test_prompt, return_tensors='pt').to(DEVICE)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_p=0.9, temperature=0.7)\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(result)\n",
        "    print('-'*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJyJdlomzCLm"
      },
      "source": [
        "## Step 20: ‚úÖ Complete!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "execution_failed": "2026-01-31T02:04:50.899Z"
        },
        "id": "lQPnsrvRzCLm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print('\\n' + '='*60)\n",
        "print('‚úÖ MODEL 2 FINE-TUNING & ANALYSIS COMPLETE!')\n",
        "print('='*60)\n",
        "print('\\nüìä Model Used: MedGemma 4B (Lightweight & Fast)')\n",
        "print('\\nüìÅ Output Files:')\n",
        "print('  ‚úÖ medgemma_nail_disease_model2_finetuned/')\n",
        "print('     - adapter_model.bin (LoRA weights)')\n",
        "print('     - config.json')\n",
        "print('     - tokenizer files')\n",
        "print('  ‚úÖ model2_overfitting_analysis.png (4-subplot visualization)')\n",
        "print('  ‚úÖ model2_training_summary.json (metrics & config)')\n",
        "print('  ‚úÖ logs/ (tensorboard data)')\n",
        "print('\\n‚ö° Performance Benefits of 4B Model:')\n",
        "print('  ‚úÖ ~2x faster training than 7B')\n",
        "print('  ‚úÖ 50% smaller model size')\n",
        "print('  ‚úÖ Lower memory requirements')\n",
        "print('  ‚úÖ Excellent medical understanding maintained')\n",
        "print('\\nüöÄ Next Steps:')\n",
        "print('  1. Download files from Kaggle Output tab')\n",
        "print('  2. Use model2 for clinical explanations in your app')\n",
        "print('  3. Start Stage 3 training with MedGemma 27B')\n",
        "print('  4. Build mobile/web app integrating all 3 stages')\n",
        "print('\\nüìä Model Performance:')\n",
        "if len(eval_loss) > 0:\n",
        "    print(f'  Final Test Loss: {eval_loss[-1]:.4f}')\n",
        "print('='*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "medgemma_kaggle_finetuning",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 9378095,
          "sourceId": 14679389,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31260,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
