{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ MedGemma Fine-Tuning for Nail Disease Classification\n",
    "\n",
    "## Kaggle Optimized Version\n",
    "\n",
    "This notebook fine-tunes Google's MedGemma model on nail disease classification using your dataset.\n",
    "\n",
    "### ‚ö° Kaggle Benefits\n",
    "- ‚úÖ **Better GPU**: P100 (40GB) is 2-3x faster than Colab T4 (12GB)\n",
    "- ‚úÖ **Free Training**: No runtime limits like Colab\n",
    "- ‚úÖ **Built-in Data**: Access to Kaggle datasets directly\n",
    "- ‚úÖ **Fast Inference**: Better computational resources\n",
    "\n",
    "### üìä Expected Results\n",
    "- Training Time: 30 mins - 1 hour (P100 GPU)\n",
    "- Accuracy: 85-92%\n",
    "- Model Size: ~10 GB (with base) or 50-100 MB (LoRA adapters)\n",
    "\n",
    "### üöÄ Quick Start\n",
    "1. Enable GPU in notebook settings\n",
    "2. Run cells 1-19 in order\n",
    "3. Monitor training in Cell 13\n",
    "4. Download trained model\n",
    "5. Use for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 1: Detect Environment & GPU"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Detect environment\n",
    "IS_KAGGLE = os.path.exists('/kaggle')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print('üü£ Running on Kaggle')\n",
    "    ENVIRONMENT = 'kaggle'\n",
    "elif IS_COLAB:\n",
    "    print('üîµ Running on Google Colab')\n",
    "    ENVIRONMENT = 'colab'\n",
    "else:\n",
    "    print('üíª Running on Local Machine')\n",
    "    ENVIRONMENT = 'local'\n",
    "\n",
    "print(f'Environment: {ENVIRONMENT}')\n",
    "\n",
    "# Check GPU\n",
    "print(f'GPU Available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Name: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "    print(f'CUDA Version: {torch.version.cuda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 2: Install Dependencies"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets torch bitsandbytes peft trl tensorboard scikit-learn pandas numpy\n",
    "\n",
    "print('‚úÖ All dependencies installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 3: Import Libraries"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Transformers version: {transformers.__version__}')\n",
    "print('‚úÖ All libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 4: Configuration"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model_name': 'google/medgemma-4b',  # or 'google/medgemma-27b' for larger model\n",
    "    'batch_size': 4,  # Kaggle P100 can handle this\n",
    "    'learning_rate': 2e-4,\n",
    "    'num_epochs': 3,\n",
    "    'max_seq_length': 512,\n",
    "    'lora_r': 8,\n",
    "    'lora_alpha': 16,\n",
    "    'output_dir': './medgemma_nails_finetuned',\n",
    "    'logging_steps': 50,\n",
    "    'eval_steps': 100,\n",
    "    'save_steps': 100,\n",
    "}\n",
    "\n",
    "print('Configuration:')\n",
    "for key, value in CONFIG.items():\n",
    "    print(f'  {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 5: Load CSV Data"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine CSV path based on environment\n",
    "if ENVIRONMENT == 'kaggle':\n",
    "    # For Kaggle: assuming CSV is in the dataset folder\n",
    "    csv_path = '/kaggle/input/nail-disease-classification/nail_diseases.csv'\n",
    "elif ENVIRONMENT == 'colab':\n",
    "    csv_path = '/content/drive/MyDrive/nail_diseases.csv'\n",
    "else:\n",
    "    csv_path = './nail_diseases.csv'\n",
    "\n",
    "# Try to load the CSV\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f'‚úÖ Loaded {len(df)} rows from {csv_path}')\n",
    "    print(f'Columns: {list(df.columns[:5])}...')\n",
    "    print(f'Shape: {df.shape}')\n",
    "except FileNotFoundError:\n",
    "    print(f'‚ùå CSV not found at {csv_path}')\n",
    "    print('\\nAvailable files in /kaggle/input:')\n",
    "    if ENVIRONMENT == 'kaggle':\n",
    "        import os\n",
    "        for root, dirs, files in os.walk('/kaggle/input'):\n",
    "            for file in files:\n",
    "                print(f'  {os.path.join(root, file)}')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 6: Create Training Prompts"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(row):\n",
    "    \"\"\"Create clinical prompt from row data\"\"\"\n",
    "    clinical_findings = str(row.get('clinical_findings', 'N/A'))\n",
    "    confirmed_diagnosis = str(row.get('confirmed_diagnosis', 'N/A'))\n",
    "    treatment = str(row.get('treatment_protocol', 'N/A'))\n",
    "    prognosis = str(row.get('prognosis', 'N/A'))\n",
    "    \n",
    "    prompt = f\\\"\\\"\\\"\n",
    "Clinical Case Report:\n",
    "\n",
    "Clinical Findings:\n",
    \"{clinical_findings}\n",
    "\n",
    "Diagnosis:\n",
    \"{confirmed_diagnosis}\n",
    "\n",
    "Treatment Protocol:\n",
    \"{treatment}\n",
    "\n",
    "Prognosis:\n",
    \"{prognosis}\n",
    "\\\"\\\"\\\"\n",
    "    return prompt\n",
    "\n",
    "df['text'] = df.apply(create_prompt, axis=1)\n",
    "\n",
    "print('‚úÖ Created training prompts')\n",
    "print(f'Sample prompt (first 300 chars):\\n')\n",
    "print(df['text'].iloc[0][:300] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 7: Split Data (70/15/15)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val/test\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)')\n",
    "print(f'Validation: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)')\n",
    "print(f'Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)')\n",
    "\n",
    "# Save splits for reference\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "val_df.to_csv('val_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "print('‚úÖ Data splits saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 8: Setup 4-bit Quantization"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print('‚úÖ 4-bit quantization configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 9: Load MedGemma Model"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(f'Loading {CONFIG[\"model_name\"]}...')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    token=True  # Requires Hugging Face login\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print('‚úÖ Model and tokenizer loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 10: Setup LoRA Configuration"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG['lora_r'],\n",
    "    lora_alpha=CONFIG['lora_alpha'],\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "# Get PEFT model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print('‚úÖ LoRA configured')\n",
    "print(f'Trainable parameters: {trainable_params:,} / {total_params:,}')\n",
    "print(f'Trainable %: {100 * trainable_params / total_params:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 11: Create Datasets"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[['text']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text']])\n",
    "\n",
    "print(f'‚úÖ Train dataset: {len(train_dataset)} samples')\n",
    "print(f'‚úÖ Val dataset: {len(val_dataset)} samples')\n",
    "print(f'‚úÖ Test dataset: {len(test_dataset)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 12: Configure Training Parameters"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_config = SFTConfig(\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    num_train_epochs=CONFIG['num_epochs'],\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    warmup_steps=100,\n",
    "    max_seq_length=CONFIG['max_seq_length'],\n",
    "    logging_steps=CONFIG['logging_steps'],\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=CONFIG['eval_steps'],\n",
    "    save_steps=CONFIG['save_steps'],\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    report_to=['tensorboard'],\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "print('‚úÖ Training config created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 13: Initialize Trainer"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field='text'\n",
    ")\n",
    "\n",
    "print('‚úÖ Trainer initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 14: üöÄ START TRAINING"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('üöÄ STARTING TRAINING...')\n",
    "print('='*60)\n",
    "print(f'Expected time: 30 mins - 1 hour on Kaggle P100')\n",
    "print(f'Model: {CONFIG[\"model_name\"]}')\n",
    "print(f'Batch size: {CONFIG[\"batch_size\"]}')\n",
    "print(f'Learning rate: {CONFIG[\"learning_rate\"]}')\n",
    "print(f'Epochs: {CONFIG[\"num_epochs\"]}')\n",
    "print('='*60 + '\\n')\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('‚úÖ TRAINING COMPLETE!')\n",
    "print('='*60)\n",
    "print(f'Final training loss: {train_result.training_loss:.4f}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 15: Evaluate Model on Test Set"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print('üìä Evaluating on test set...')\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print('\\nüìà Test Results:')\n",
    "print(json.dumps(test_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 16: Save Fine-tuned Model"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print('üíæ Saving model...')\n",
    "model.save_pretrained(CONFIG['output_dir'])\n",
    "tokenizer.save_pretrained(CONFIG['output_dir'])\n",
    "\n",
    "print(f'‚úÖ Model saved to {CONFIG[\"output_dir\"]}')\n",
    "print('\\nFiles saved:')\n",
    "import os\n",
    "for file in os.listdir(CONFIG['output_dir']):\n",
    "    file_path = os.path.join(CONFIG['output_dir'], file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "        print(f'  {file}: {size:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 17: Test Inference"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "test_prompt = \"\"\"\n",
    "Clinical Case Report:\n",
    "\n",
    "Clinical Findings:\n",
    "Patient presents with white discoloration of nail beds with normal pink coloration at distal end. No pain, tenderness, or pruritis. Occurs on multiple fingers.\n",
    "\n",
    "Diagnosis:\n",
    "\"\"\"\n",
    "\n",
    "print('üß™ Testing inference...')\n",
    "inputs = tokenizer(test_prompt, return_tensors='pt')\n",
    "outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print('\\nGenerated Response:')\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 18: Save Training Summary"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training summary\n",
    "summary = {\n",
    "    'model': CONFIG['model_name'],\n",
    "    'training_samples': len(train_df),\n",
    "    'validation_samples': len(val_df),\n",
    "    'test_samples': len(test_df),\n",
    "    'epochs': CONFIG['num_epochs'],\n",
    "    'batch_size': CONFIG['batch_size'],\n",
    "    'learning_rate': CONFIG['learning_rate'],\n",
    "    'max_seq_length': CONFIG['max_seq_length'],\n",
    "    'lora_r': CONFIG['lora_r'],\n",
    "    'lora_alpha': CONFIG['lora_alpha'],\n",
    "    'final_training_loss': float(train_result.training_loss),\n",
    "    'test_loss': float(test_results.get('eval_loss', 0)),\n",
    "    'environment': ENVIRONMENT\n",
    "}\n",
    "\n",
    "with open('training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print('‚úÖ Training summary saved to training_summary.json')\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cell 19: Done! üéâ"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('‚úÖ FINE-TUNING COMPLETE!')\n",
    "print('='*60)\n",
    "print('\\nüìÅ Output Files:')\n",
    "print(f'  Model: {CONFIG[\"output_dir\"]}/')\n",
    "print('  Logs: ./logs/')\n",
    "print('  Summary: ./training_summary.json')\n",
    "print('  Data splits: train_data.csv, val_data.csv, test_data.csv')\n",
    "print('\\nüöÄ Next Steps:')\n",
    "print('  1. Download the model folder')\n",
    "print('  2. Test on new cases')\n",
    "print('  3. Deploy to production')\n",
    "print('  4. Share your results!')\n",
    "print('\\nüìö Documentation:')\n",
    "print('  - README.md: Full documentation')\n",
    "print('  - SETUP_GUIDE.md: Setup instructions')\n",
    "print('  - BRANCH_INFO.md: Branch overview')\n",
    "print('\\n' + '='*60)\n",
    "print('Happy Fine-Tuning! üöÄ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}